<!DOCTYPE html>
<html>
    <head>
        <title>Seq2Seq 모델을 이용한 기계 번역</title>
        <meta name="description" content="GRU와 Bahdanau attention 메커니즘, scheduled sampling을 바탕으로 기계 번역 모델을 제작합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/RNN4.html" />
        <meta property="og:title" content="Seq2Seq 모델을 이용한 기계 번역" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="GRU와 Bahdanau attention 메커니즘, scheduled sampling을 바탕으로 기계 번역 모델을 제작합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/ALs6j_Hf5FLH8qLBxVPIqBlO2I-hwG8YMPqPWXJG1LnfaR-MJFlxWPZTYkeFyFoJLt6O6aY9arhUJrjgXDq6p4aGXbU8mjpAKtyOGMNdJbzQztgOzbG_1dAUSu0VQ3pju2oxjtcFgjUJAVytwkdDgiK1IO6y3wlM9Bnt6_DjAPpIlLqxvOvjd_MQOhwde7n2REIXgOLodROIjBVEyVFaJ79iVi02Ddyzwx3tPoW3H6jpHfLoCFugW4cUPI8Ga3EXesyt4ELRb54jNZNgJm743D-f2xLzrP3n6OPYZVIzn4kyT-Aafr9j3YCQ0vCds-fkfmoUx3DuA3RthuzNTdEQTIsjx2R3KP9-QGdmtDaqYnMREvs4OwF1chwLgf0lIMpb3RqD_WPNOlPa3-edpWsLrkZXNmmsq2cRLZSrh_EImzRynLPpKFT3PvA17TrxX_i2RDEh_itk1rIQkruQV4oSrsKh2RTzRbTOcj-Hj_QzVbKCIUGSduiKrZE9vYsu3DMcLm3ylijFTB58NL5tmAmN9AJZZF10h_huao4u5EaZmaWxJthTBVWSDarCexnIww4_1yTlLLzyrIgLjIqgqfWQjIPbJJE3O3Nf3683fvQFeLGpmFHj4NYReZDVDl5eGmk-4qZ3e4CIh84zP3fn6BwyghaDUhoks6yPKi2H2pbTEUrJbzktv7e8ISoAw7v9G0JsSSXB0BVgVuL4xTSn1XYsAZXuz3WB4OwgXkLircNR0BN8ZUGRU6noD2PxBYO8YTi2-957K0BUznlB9bK9KxdmWK0uBjQ5oq6Cl9SQnE7GMnDpGKM58ZPNALEg3Yho_yAgZMC7767gRKdhNJNyiRHou89g3xpaYprCgJQhG_WtlxTZYzd5fQHqXYRICrlq5AyoK20lc5_YRUtFMnNfrDPAIKGQL6VQYfCwU-csb0sZ2IutbTuzvQgVUSfe7Pc4OKPjTrCI9NvGqKk_WqDt38T278Ft2cvMY6Wr_gfoI8IH9XLp8Z4ZMSPKCEnJE0MH0l0pPUTExLJ00RIVZcOw3Q4_r-1JdnzOgDX2Y1O7U80X-bVHzQY4BUgnJoLKIdpqdlVcNoGvqdmg3BReYdOo3N2caOakQsM-tKdbfiM5w48947fU2W37g9iGe2TeiuwRqOTNr7dhFRhgE2wwKNBYNYBbEU4onys0L55SBrR3zMlj6lDIbD9AEljDn03Zy4G3sdcT02a1eknTWKncgGNTulTS_eW0H-gMVXIUd7Y_GlF9YNMGEVHxLq8Lc3G5C-iA7Bpchwom_GuIQt2kqyfbPv17hgn9bF4NdMBAUzdYvkbUYAZ4jSnDQBYpE6JpAbxrJhKYn1I1FrWKbr3uNUKOAKwjKr0aAgs-ChQ5zYiGjl3KGYVgqaQZ-sSgSZ4FonEvXqbOMyKWEsB9Oj8T1HQwdzJptpsZkWaLZN6d6_YQ7rCFFi5ACl0OGmkTQLZVjK8aWa_Gtq8MN3tVte6tByA81m2AuvZpV-MP6g7sh-n_wOy3hYCKqWcUZkgpzuFcb9ChW4PyrufUmgzKJRGD0roO" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Recurrent Neural Network (RNN) / 4. Seq2Seq 모델을 이용한 기계 번역</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/ALs6j_Hf5FLH8qLBxVPIqBlO2I-hwG8YMPqPWXJG1LnfaR-MJFlxWPZTYkeFyFoJLt6O6aY9arhUJrjgXDq6p4aGXbU8mjpAKtyOGMNdJbzQztgOzbG_1dAUSu0VQ3pju2oxjtcFgjUJAVytwkdDgiK1IO6y3wlM9Bnt6_DjAPpIlLqxvOvjd_MQOhwde7n2REIXgOLodROIjBVEyVFaJ79iVi02Ddyzwx3tPoW3H6jpHfLoCFugW4cUPI8Ga3EXesyt4ELRb54jNZNgJm743D-f2xLzrP3n6OPYZVIzn4kyT-Aafr9j3YCQ0vCds-fkfmoUx3DuA3RthuzNTdEQTIsjx2R3KP9-QGdmtDaqYnMREvs4OwF1chwLgf0lIMpb3RqD_WPNOlPa3-edpWsLrkZXNmmsq2cRLZSrh_EImzRynLPpKFT3PvA17TrxX_i2RDEh_itk1rIQkruQV4oSrsKh2RTzRbTOcj-Hj_QzVbKCIUGSduiKrZE9vYsu3DMcLm3ylijFTB58NL5tmAmN9AJZZF10h_huao4u5EaZmaWxJthTBVWSDarCexnIww4_1yTlLLzyrIgLjIqgqfWQjIPbJJE3O3Nf3683fvQFeLGpmFHj4NYReZDVDl5eGmk-4qZ3e4CIh84zP3fn6BwyghaDUhoks6yPKi2H2pbTEUrJbzktv7e8ISoAw7v9G0JsSSXB0BVgVuL4xTSn1XYsAZXuz3WB4OwgXkLircNR0BN8ZUGRU6noD2PxBYO8YTi2-957K0BUznlB9bK9KxdmWK0uBjQ5oq6Cl9SQnE7GMnDpGKM58ZPNALEg3Yho_yAgZMC7767gRKdhNJNyiRHou89g3xpaYprCgJQhG_WtlxTZYzd5fQHqXYRICrlq5AyoK20lc5_YRUtFMnNfrDPAIKGQL6VQYfCwU-csb0sZ2IutbTuzvQgVUSfe7Pc4OKPjTrCI9NvGqKk_WqDt38T278Ft2cvMY6Wr_gfoI8IH9XLp8Z4ZMSPKCEnJE0MH0l0pPUTExLJ00RIVZcOw3Q4_r-1JdnzOgDX2Y1O7U80X-bVHzQY4BUgnJoLKIdpqdlVcNoGvqdmg3BReYdOo3N2caOakQsM-tKdbfiM5w48947fU2W37g9iGe2TeiuwRqOTNr7dhFRhgE2wwKNBYNYBbEU4onys0L55SBrR3zMlj6lDIbD9AEljDn03Zy4G3sdcT02a1eknTWKncgGNTulTS_eW0H-gMVXIUd7Y_GlF9YNMGEVHxLq8Lc3G5C-iA7Bpchwom_GuIQt2kqyfbPv17hgn9bF4NdMBAUzdYvkbUYAZ4jSnDQBYpE6JpAbxrJhKYn1I1FrWKbr3uNUKOAKwjKr0aAgs-ChQ5zYiGjl3KGYVgqaQZ-sSgSZ4FonEvXqbOMyKWEsB9Oj8T1HQwdzJptpsZkWaLZN6d6_YQ7rCFFi5ACl0OGmkTQLZVjK8aWa_Gtq8MN3tVte6tByA81m2AuvZpV-MP6g7sh-n_wOy3hYCKqWcUZkgpzuFcb9ChW4PyrufUmgzKJRGD0roO);">
                    <div>
                        <span class="mainTitle">Seq2Seq 모델을 이용한 기계 번역</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2022.09.12</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이전글에서는 sequence-to-sequence (seq2seq) 모델과 attention 메커니즘 예시와 scheduled sampling에 대해 살펴보았습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이번글에서는 GRU 모델을 이용하여 Tatoeba Project의 English-French 문장 쌍 데이터를 가지고 기계 번역 모델을 학습하고, Bahdanau attention mechanism (바다나우 어텐션)과 scheduled sampling을 적용 해보겠습니다.
                        본 코드는 영어 문장을 프랑스어로 번역하는 모델이며, 구현은 python의 PyTorch를 이용하였습니다. 그리고 모델을 학습하면서 training set과 test set의 loss의 변화와 더불어, 각종 지표 (PPL, BLEU, NIST), attention score, 기계 번역 결과 샘플도 살펴보겠습니다.</span>

                        <br><br>그리고 seq2seq 모델, attention, scheduled sampling에 대한 설명은 <a onclick="pjaxPage('RNN2.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.
                        그리고 학습을 위한 코드는 GitHub에 올려놓았으니 아래 링크를 참고하시기 바랍니다(본 글에서는 모델의 구현에 초점을 맞추고 있기 때문에, 데이터 전처리 및 학습을 위한 전체 코드는 아래 GitHub 링크를 참고하시기 바랍니다).

                        <br><br>그리고 텍스트를 토큰화 하기 위해 사용한 토크나이저는 word tokenizer를 구현하여 사용하였습니다.
                        물론 현재는 unknown 토큰 문제를 해결하기 위해 <a onclick="pjaxPage('word2vec1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">Word2Vec 글</span></a>에서 설명한 byte-pair-encoding (BPE) 같이 subword 기반의 토크나이저가 많이 사용되지만, <span class="highlight" style="color: rgb(0, 3, 206);">본 글에서는 attention 모델이 결과를 예측하기 위해 어떠한 단어에 집중을 했는지 그 score를 보기 위해서 단어 기반의 토크나이저를 선택하였습니다.</span>
                        
                        <br><br>여담으로 PyTorch의 유명한 seq2seq 기계 번역 모델 튜토리얼이 있습니다. 이 튜토리얼에서는 Bahdanau attention이 아닌 다른 attention 기법으로 구현 되어있습니다.
                        다른 attention 기반의 코드를 보고싶다면 튜토리얼 링크를 참고하시기 바랍니다. PyTorch 튜토리얼 코드는 batch 학습을 하지 않기 때문에 시간이 오래 걸린다는 단점이 있습니다.
                    </p>
                    <div class="link">
                        <a href="https://github.com/ljm565/neural-machine-translator-GRU" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Attention과 Scheduled Sampling을 이용한 GRU 기계 번역 모델 GitHub 코드</a>
                    </div><br>
                    <div class="link">
                        <a href="https://tutorials.PyTorch.kr/intermediate/seq2seq_translation_tutorial.html" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">PyTorch Seq2Seq 튜토리얼</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>GRU 기계 번역 모델</li>
                            <li>Attention 모듈</li>
                            <li>기계 번역 모델 학습</li>
                            <li>기계 번역 모델 학습 결과</li>
                        </ol>

                        <br>본 코드에서 구현한 Bahdanau Attention (바다나우 어텐션)과 scheduled sampling에 대한 논문은 아래 링크에 달아놓겠습니다.
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Bahdanau Attention 논문</a>
                    </div><br>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/1506.03099.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Scheduled Sampling 논문</a>
                    </div>



                    <h1 class="subHead">Attention과 Scheduled Sampling을 이용한 GRU 기계 번역 모델</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>GRU 기계 번역 모델</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        여기서는 기계 번역을 위한 GRU 코드를 살펴보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">코드는 PyTorch로 작성 되었으며, source 문장을 encoder를 통해 represent 한 후, 이를 바탕으로 decoder에 target 문장을 넣어 학습합니다.</span>
                        만약 Bahdanau attention을 사용한다면 encoder 결과를 attention하는 데 사용합니다.
                    </p>

<pre><code class="python"><span class="reserved">class</span> <span class="clazz">Encoder</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">config</span>, <span class="var">tokenizer</span>, <span class="var">device</span>):
        <span class="clazz">super</span>(<span class="clazz">Encoder</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">pad_token_id</span> = <span class="var">tokenizer</span>.pad_token_id
        <span class="var">self</span>.<span class="var">vocab_size</span> = <span class="var">tokenizer</span>.vocab_size
        <span class="var">self</span>.<span class="var">hidden_size</span> = <span class="var">config</span>.hidden_size
        <span class="var">self</span>.<span class="var">num_layers</span> = <span class="var">config</span>.num_layers
        <span class="var">self</span>.<span class="var">dropout</span> = <span class="var">config</span>.dropout
        <span class="var">self</span>.<span class="var">device</span> = <span class="var">device</span>

        <span class="var">self</span>.<span class="var">embedding</span> = <span class="clazz">nn</span>.<span class="clazz">Embedding</span>(<span class="var">self</span>.<span class="var">vocab_size</span>, <span class="var">self</span>.<span class="var">hidden_size</span>, <span class="var">padding_idx</span>=<span class="var">self</span>.<span class="var">pad_token_id</span>)
        <span class="var">self</span>.<span class="var">gru</span> = <span class="clazz">nn</span>.<span class="clazz">GRU</span>(<span class="var">input_size</span>=<span class="var">self</span>.<span class="var">hidden_size</span>,
                            <span class="var">hidden_size</span>=<span class="var">self</span>.<span class="var">hidden_size</span>,
                            <span class="var">num_layers</span>=<span class="var">self</span>.<span class="var">num_layers</span>,
                            <span class="var">batch_first</span>=<span class="reserved">True</span>,
                            <span class="var">dropout</span>=<span class="var">self</span>.<span class="var">dropout</span>,
                            <span class="var">bidirectional</span>=<span class="reserved">True</span>)
        <span class="var">self</span>.<span class="var">dropout_layer</span> = <span class="clazz">nn</span>.<span class="clazz">Dropout</span>(<span class="var">self</span>.<span class="var">dropout</span>)


    <span class="reserved">def</span> <span class="method">init_hidden</span>(<span class="var">self</span>):
        <span class="var">h0</span> = <span class="clazz">torch</span>.<span class="method">zeros</span>(<span class="var">self</span>.<span class="var">num_layers</span>*<span class="num">2</span>, <span class="var">self</span>.<span class="var">batch_size</span>, <span class="var">self</span>.<span class="var">hidden_size</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
        <span class="return">return</span> <span class="var">h0</span>


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">x</span>):
        <span class="var">self</span>.<span class="var">batch_size</span> = <span class="var">x</span>.size(<span class="num">0</span>)
        <span class="var">h0</span> = <span class="var">self</span>.<span class="method">init_hidden</span>()

        <span class="var">x</span> = <span class="var">self</span>.<span class="var">embedding</span>(<span class="var">x</span>)
        <span class="var">x</span> = <span class="var">self</span>.<span class="var">dropout_layer</span>(<span class="var">x</span>)
        <span class="var">x</span>, <span class="var">hn</span> = <span class="var">self</span>.<span class="var">gru</span>(<span class="var">x</span>, <span class="var">h0</span>)
        <span class="var">hn</span> = <span class="var">hn</span>.view(<span class="num">2</span>, <span class="num"><span class="num">-1</span></span>, <span class="var">self</span>.<span class="var">batch_size</span>, <span class="var">self</span>.<span class="var">hidden_size</span>)
        <span class="var">hn</span> = <span class="clazz">torch</span>.<span class="method">sum</span>(<span class="var">hn</span>, <span class="var">dim</span>=<span class="num">0</span>)
        <span class="return">return</span> <span class="var">x</span>, <span class="var">hn</span>



<span class="reserved">class</span> <span class="clazz">Decoder</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">config</span>, <span class="var">tokenizer</span>, <span class="var">device</span>):
        <span class="clazz">super</span>(<span class="clazz">Decoder</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">pad_token_id</span> = <span class="var">tokenizer</span>.pad_token_id
        <span class="var">self</span>.<span class="var">vocab_size</span> = <span class="var">tokenizer</span>.vocab_size
        <span class="var">self</span>.<span class="var">hidden_size</span> = <span class="var">config</span>.hidden_size
        <span class="var">self</span>.<span class="var">num_layers</span> = <span class="var">config</span>.num_layers
        <span class="var">self</span>.<span class="var">dropout</span> = <span class="var">config</span>.dropout
        <span class="var">self</span>.<span class="var">is_attn</span> = <span class="var">config</span>.is_attn
        <span class="var">self</span>.<span class="var">device</span> = <span class="var">device</span>
        <span class="return">if</span> <span class="var">self</span>.<span class="var">is_attn</span>:
            <span class="var">self</span>.<span class="var">attention</span> = <span class="clazz">Attention</span>(<span class="var">self</span>.<span class="var">hidden_size</span>)
        <span class="var">self</span>.<span class="var">input_size</span> = <span class="var">self</span>.<span class="var">hidden_size</span> * <span class="num">2</span> <span class="return">if</span> <span class="var">self</span>.<span class="var">is_attn</span> <span class="return">else</span> <span class="var">self</span>.<span class="var">hidden_size</span>

        <span class="var">self</span>.<span class="var">embedding</span> = <span class="clazz">nn</span>.<span class="clazz">Embedding</span>(<span class="var">self</span>.<span class="var">vocab_size</span>, <span class="var">self</span>.<span class="var">hidden_size</span>, <span class="var">padding_idx</span>=<span class="var">self</span>.<span class="var">pad_token_id</span>)
        <span class="var">self</span>.<span class="var">gru</span> = <span class="clazz">nn</span>.<span class="clazz">GRU</span>(<span class="var">input_size</span>=<span class="var">self</span>.<span class="var">input_size</span>,
                            <span class="var">hidden_size</span>=<span class="var">self</span>.<span class="var">hidden_size</span>,
                            <span class="var">num_layers</span>=<span class="var">self</span>.<span class="var">num_layers</span>,
                            <span class="var">batch_first</span>=<span class="reserved">True</span>,
                            <span class="var">dropout</span>=<span class="var">self</span>.<span class="var">dropout</span>)
        <span class="var">self</span>.<span class="var">dropout_layer</span> = <span class="clazz">nn</span>.<span class="clazz">Dropout</span>(<span class="var">self</span>.<span class="var">dropout</span>)
        <span class="var">self</span>.<span class="var">relu</span> = <span class="clazz">nn</span>.<span class="clazz">ReLU</span>()
        <span class="var">self</span>.<span class="var">fc</span> = <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_size</span>, <span class="var">self</span>.<span class="var">vocab_size</span>)


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">x</span>, <span class="var">hidden</span>, <span class="var">enc_output</span>, <span class="var">mask</span>):
        <span class="var">self</span>.<span class="var">batch_size</span> = <span class="var">x</span>.size(<span class="num">0</span>)
        <span class="var">score</span> = <span class="reserved">None</span>

        <span class="var">x</span> = <span class="var">self</span>.<span class="var">embedding</span>(<span class="var">x</span>)
        <span class="return">if</span> <span class="var">self</span>.<span class="var">is_attn</span>:
            <span class="var">enc_output</span>, <span class="var">score</span> = <span class="var">self</span>.<span class="var">attention</span>(<span class="var">self</span>.<span class="var">relu</span>(<span class="var">enc_output</span>), <span class="var">self</span>.<span class="var">relu</span>(<span class="var">hidden</span>[<span class="num"><span class="num">-1</span></span>]), <span class="var">mask</span>)
            <span class="var">x</span> = <span class="clazz">torch</span>.<span class="method">cat</span>((<span class="var">x</span>, <span class="var">enc_output</span>.unsqueeze(<span class="num">1</span>)), <span class="var">dim</span>=<span class="num"><span class="num">-1</span></span>)
        <span class="var">x</span> = <span class="var">self</span>.<span class="var">dropout_layer</span>(<span class="var">x</span>)
        <span class="var">x</span>, <span class="var">hn</span> = <span class="var">self</span>.<span class="var">gru</span>(<span class="var">x</span>, <span class="var">hidden</span>)
        <span class="var">x</span> = <span class="var">self</span>.<span class="var">fc</span>(<span class="var">self</span>.<span class="var">relu</span>(<span class="var">x</span>))
        <span class="return">return</span> <span class="var">x</span>, <span class="var">hn</span>, <span class="var">score</span>
</code></pre>
                    <p>
                        위 코드에서 나오는 config 부분은 <a href="https://github.com/ljm565/neural-machine-translator-GRU" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">GitHub 코드</span></a>에 보면 <span class="var">config</span>.json이라는 파일에 존재하는 변수 값들을 모델에 적용하여 초기화 하는 것입니다.<br><br>
                        <span style="font-size: 20px;"><b>Encoder</b></span>
                        <ul>
                            <li>4번째 줄: Vocab 중 pad token id 값.</li>
                            <li>5번째 줄: 토크나이저의 vocab size.</li>
                            <li>6번째 줄: GRU 모델 hidden dimension.</li>
                            <li>7번째 줄: GRU 모델 레이어 수.</li>
                            <li>8번째 줄: GRU 모델 dropout 비율.</li>
                            <li>11 ~ 18번째 줄: Embedding 레이어, GRU 모델, dropout layer 선언.</li>
                            <li>21 ~ 23번째 줄: GRU hidden state 초기와 함수.</li>
                            <li>26 ~ 35번째 줄: Source 문장(본 코드에서는 영어 문장)이 학습 시 거치는 부분.</li>
                            <li>34번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Encoder는 bidirectional 모델이므로, 단방향 GRU인 decoder보다 hidden state 결과가 2배가 많음. 따라서 decoder에 hidden state를 넣어주기 위해서는 차원을 맞춰줘야함. 따라서 각 레이어별 forward, backward hidden state 결과를 더해주어 하나의 hidden으로 취급.</span></li>
                        </ul>

                        <br><span style="font-size: 20px;"><b>Decoder</b></span>
                        <ul>
                            <li>42번째 줄: Vocab 중 pad token id 값.</li>
                            <li>43번째 줄: 토크나이저의 vocab size.</li>
                            <li>44번째 줄: GRU 모델 hidden dimension.</li>
                            <li>45번째 줄: GRU 모델 레이어 수.</li>
                            <li>46번째 줄: GRU 모델 dropout 비율.</li>
                            <li>47번째 줄: Attention 사용 여부.</li>
                            <li>49 ~ 50번째 줄: Attention 사용하는 경우 Attention 모듈 정의.</li>
                            <li>51번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Attention을 사용할 경우 decoder input 차원은 사용안할 때 비해 2배가 커짐(Attention 결과를 다음 decoder input에 대해 concatenate하여 들어가기 때문).</span></li>
                            <li>53 ~ 59번째 줄: Embedding 레이어, GRU 모델, dropout layer 선언.</li>
                            <li>61번째 줄: 다음 단어를 예측해야하므로 vocab size의 크기만큼 내어주는 fully-connected layer 선언.</li>
                            <li>64 ~ 75번째 줄: Target 문장(본 코드에서는 프랑스어 문장)이 학습 시 거치는 부분.</li>
                            <li>69 ~ 71번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Attention 사용 시 거치는 부분. Target input에 concatenate 하기 때문에 attention 사용 하지 않는 모델 대비 차원이 2배가 큼.</span></li>
                            <li>75번째 줄: Attention score도 결과와 같이 반환.</li>
                        </ul>
                    </p>




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>Attention 모듈</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        위의 GRU 기반 seq2seq 모델에서 attention을 사용할건지 여부를 선택할 수 있었습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">만약 attention을 선택하게 된다면 아래의 attention 모듈에 GRU encoder의 output과 deccoder의 이전 output이 들어가게 됩니다.</span>
                    </p>

<pre><code class="python"><span class="reserved">class</span> <span class="clazz">Attention</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">hidden_size</span>):
        <span class="clazz">super</span>(<span class="clazz">Attention</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">hidden_size</span> = <span class="var">hidden_size</span>
        <span class="var">self</span>.<span class="var">enc_dim_changer</span> = <span class="clazz">nn</span>.<span class="clazz">Sequential</span>(
            <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_size</span>*<span class="num">2</span>, <span class="var">self</span>.<span class="var">hidden_size</span>),
        )
        <span class="var">self</span>.<span class="var">enc_wts</span> = <span class="clazz">nn</span>.<span class="clazz">Sequential</span>(
            <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_size</span>, <span class="var">self</span>.<span class="var">hidden_size</span>)
        )
        <span class="var">self</span>.<span class="var">dec_wts</span> = <span class="clazz">nn</span>.<span class="clazz">Sequential</span>(
            <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_size</span>, <span class="var">self</span>.<span class="var">hidden_size</span>),
            <span class="clazz">nn</span>.<span class="clazz">ReLU</span>(),
            <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_size</span>, <span class="var">self</span>.<span class="var">hidden_size</span>)
        )
        <span class="var">self</span>.<span class="var">score_wts</span> = <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_size</span>, <span class="num">1</span>)
        <span class="var">self</span>.<span class="var">tanh</span> = <span class="clazz">nn</span>.<span class="clazz">Tanh</span>()
        <span class="var">self</span>.<span class="var">relu</span> = <span class="clazz">nn</span>.<span class="clazz">ReLU</span>()


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">enc_output</span>, <span class="var">dec_hidden</span>, <span class="var">mask</span>):
        <span class="var">enc_output</span> = <span class="var">self</span>.<span class="var">enc_dim_changer</span>(<span class="var">enc_output</span>)
        <span class="var">score</span> = <span class="var">self</span>.<span class="var">tanh</span>(<span class="var">self</span>.<span class="var">enc_wts</span>(<span class="var">self</span>.<span class="var">relu</span>(<span class="var">enc_output</span>)) + <span class="var">self</span>.<span class="var">dec_wts</span>(<span class="var">dec_hidden</span>).unsqueeze(<span class="num">1</span>))
        <span class="var">score</span> = <span class="var">self</span>.<span class="var">score_wts</span>(<span class="var">score</span>)
        <span class="var">score</span> = <span class="var">score</span>.masked_fill(<span class="var">mask</span>.unsqueeze(<span class="num">2</span>)==<span class="num">0</span>, <span class="clazz">float</span>(<span class="str">'-inf'</span>))
        <span class="var">score</span> = <span class="clazz">F</span>.<span class="method">softmax</span>(<span class="var">score</span>, <span class="var">dim</span>=<span class="num">1</span>)
        
        <span class="var">enc_output</span> = <span class="clazz">torch</span>.<span class="method">permute</span>(<span class="var">enc_output</span>, (<span class="num">0</span>, <span class="num">2</span>, <span class="num">1</span>))
        <span class="var">enc_output</span> = <span class="clazz">torch</span>.<span class="method">bmm</span>(<span class="var">enc_output</span>, <span class="var">score</span>).<span class="method">squeeze</span>()
        <span class="return">return</span> <span class="var">enc_output</span>, <span class="var">score</span>
</code></pre>
                    <p>
                        <span style="font-size: 20px;"><b>Attention</b></span>
                        <br>위 코드에서 나오는 config 부분은 <a href="https://github.com/ljm565/neural-machine-translator-GRU" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">GitHub 코드</span></a>에 보면 <span class="var">config</span>.json이라는 파일에 존재하는 변수 값들을 모델에 적용하여 초기화 하는 것입니다.
                        <ul>
                            <li>5번째 줄: Encoder는 bidirectional GRU이므로 decoder 차원을 맞춰주기 위해서 차원의 크기를 절반으로 줄이는 linear layer 선언.</li>
                            <li>8 ~ 15번째 줄: Encoder의 output과 decoder의 output이 거치게 되는 linear layer 선언.</li>
                            <li>16번째 줄: Encoder의 각 sequence 별 attention score를 내어줘야 하므로 차원을 hidden dim &rarr; 1로 바꿔주는 layer 선언.</li>
                            <li>21 ~ 30번째 줄: Attention 모듈 학습 시 거치는 부분.</li>
                            <li>25번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Attention score를 내어주기 위해 softmax 하기 전, encoder 데이터 중 pad token에 대해 masking하는 작업(-inf로 선언 시, softmax 결과가 0).</span></li>
                            <li>29번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Attention score를 encoder output에 곱해주어 weighted sum 하는 부분.</span></li>
                        </ul>
                    </p>
                



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>기계 번역 모델 학습</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                       이제 기계 번역 모델 학습 코드를 통해 어떻게 학습이 이루어지는지 살펴보겠습니다.
                       아래 코드에 <span style="color:rgb(86, 155, 214);">self</span>. 이라고 나와있는 부분은 GitHub 코드에 보면 알겠지만 학습하는 코드가 class 내부의 변수이기 때문에 있는 것입니다.
                       여기서는 무시해도 좋습니다.
                    </p>
<pre><code class="python"><span class="var">self</span>.<span class="var">encoder</span> = <span class="clazz">Encoder</span>(<span class="var">self</span>.<span class="var">config</span>, <span class="var">self</span>.<span class="var">src_tokenizer</span>, <span class="var">self</span>.<span class="var">device</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
<span class="var">self</span>.<span class="var">decoder</span> = <span class="clazz">Decoder</span>(<span class="var">self</span>.<span class="var">config</span>, <span class="var">self</span>.<span class="var">trg_tokenizer</span>, <span class="var">self</span>.<span class="var">device</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
<span class="var">self</span>.<span class="var">criterion</span> = <span class="clazz">nn</span>.<span class="clazz">CrossEntropyLoss</span>()
<span class="var">self</span>.<span class="var">enc_optimizer</span> = <span class="clazz">optim</span>.<span class="clazz">Adam</span>(<span class="var">self</span>.<span class="var">encoder</span>.<span class="method">parameters</span>(), <span class="var">lr</span>=<span class="var">self</span>.<span class="var">lr</span>)
<span class="var">self</span>.<span class="var">dec_optimizer</span> = <span class="clazz">optim</span>.<span class="clazz">Adam</span>(<span class="var">self</span>.<span class="var">decoder</span>.<span class="method">parameters</span>(), <span class="var">lr</span>=<span class="var">self</span>.<span class="var">lr</span>)

<span class="return">for</span> <span class="var">epoch</span> <span class="return">in</span> <span class="clazz">range</span>(<span class="var">self</span>.<span class="var">epochs</span>):
    <span class="return">for</span> <span class="var">phase</span> <span class="return">in</span> [<span class="str">'train'</span>, <span class="str">'test'</span>]:
        <span class="return">if</span> <span class="var">phase</span> == <span class="str">'train'</span>:
            <span class="var">self</span>.<span class="var">encoder</span>.<span class="method">train</span>()
            <span class="var">self</span>.<span class="var">decoder</span>.<span class="method">train</span>()
        <span class="return">else</span>:
            <span class="var">self</span>.<span class="var">encoder</span>.<span class="method">eval</span>()
            <span class="var">self</span>.<span class="var">decoder</span>.<span class="method">eval</span>()

        <span class="return">for</span> <span class="var">i</span>, (<span class="var">src</span>, <span class="var">trg</span>, <span class="var">mask</span>) <span class="return">in</span> <span class="clazz">enumerate</span>(<span class="var">self</span>.<span class="var">dataloaders</span>[<span class="var">phase</span>]):
            <span class="var">batch</span> = <span class="var">src</span>.size(<span class="num">0</span>)
            <span class="var">src</span>, <span class="var">trg</span> = <span class="var">src</span>.<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>), <span class="var">trg</span>.<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
            <span class="return">if</span> <span class="var">self</span>.<span class="var">config</span>.<span class="var">is_attn</span>:
                <span class="var">mask</span> = <span class="var">mask</span>.<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
            <span class="var">self</span>.<span class="var">enc_optimizer</span>.<span class="method">zero_grad</span>()
            <span class="var">self</span>.<span class="var">dec_optimizer</span>.<span class="method">zero_grad</span>()

            <span class="return">with</span> <span class="clazz">torch</span>.<span class="clazz">set_grad_enabled</span>(<span class="var">phase</span>==<span class="str">'train'</span>):
                <span class="var">enc_output</span>, <span class="var">hidden</span> = <span class="var">self</span>.<span class="var">encoder</span>(<span class="var">src</span>)
                
                <span class="var">teacher_forcing</span> = <span class="reserved">True</span> <span class="return">if</span> <span class="clazz">random</span>.<span class="var">random</span>() &lt;= <span class="var">self</span>.<span class="var">config</span>.<span class="var">teacher_forcing_ratio</span> <span class="return">else</span> <span class="reserved">False</span>
                <span class="var">decoder_all_output</span> = []
                <span class="return">for</span> <span class="var">j</span> <span class="return">in</span> <span class="clazz">range</span>(<span class="var">self</span>.<span class="var">max_len</span>):
                    <span class="return">if</span> <span class="var">teacher_forcing</span> <span class="reserved">or</span> <span class="var">j</span> == <span class="num">0</span> <span class="reserved">or</span> <span class="var">phase</span> == <span class="str">'test'</span>:
                        <span class="var">trg_word</span> = <span class="var">trg</span>[:, <span class="var">j</span>].unsqueeze(<span class="num">1</span>)
                        <span class="var">dec_output</span>, <span class="var">hidden</span>, <span class="var">_</span> = <span class="var">self</span>.<span class="var">decoder</span>(<span class="var">trg_word</span>, <span class="var">hidden</span>, <span class="var">enc_output</span>, <span class="var">mask</span>)
                        <span class="var">decoder_all_output</span>.<span class="method">append</span>(<span class="var">dec_output</span>)
                    <span class="return">else</span>:
                        <span class="var">trg_word</span> = <span class="clazz">torch</span>.<span class="method">argmax</span>(<span class="var">dec_output</span>, <span class="var">dim</span>=<span class="num"><span class="num">-1</span></span>)
                        <span class="var">dec_output</span>, <span class="var">hidden</span>, <span class="var">_</span> = <span class="var">self</span>.<span class="var">decoder</span>(<span class="var">trg_word</span>.<span class="method"><span class="method">detach</span></span>(), <span class="var">hidden</span>, <span class="var">enc_output</span>, <span class="var">mask</span>)
                        <span class="var">decoder_all_output</span>.<span class="method">append</span>(<span class="var">dec_output</span>)

                <span class="var">decoder_all_output</span> = <span class="clazz">torch</span>.<span class="method">cat</span>(<span class="var">decoder_all_output</span>, <span class="var">dim</span>=<span class="num">1</span>)
                <span class="var">loss</span> = <span class="var">self</span>.<span class="var">criterion</span>(<span class="var">decoder_all_output</span>[:, :<span class="num"><span class="num">-1</span></span>, :].<span class="method">reshape</span>(<span class="num"><span class="num">-1</span></span>, <span class="var">decoder_all_output</span>.size(<span class="num"><span class="num">-1</span></span>)), <span class="var">trg</span>[:, <span class="num">1</span>:].<span class="method">reshape</span>(<span class="num"><span class="num">-1</span></span>))
                <span class="return">if</span> <span class="var">phase</span> == <span class="str">'train'</span>:
                    <span class="var">loss</span>.backward()
                    <span class="var">self</span>.<span class="var">enc_optimizer</span>.<span class="method">step</span>()
                    <span class="var">self</span>.<span class="var">dec_optimizer</span>.<span class="method">step</span>()
</code></pre>


                    <p>
                        <span style="font-size: 20px;"><b>학습에 필요한 것들 선언</b></span>
                        <br>먼저 위에 코드에서 정의한 모델을 불러오고 학습에 필요한 loss function, optimizer 등을 선언하는 부분입니다.
                        <ul>
                            <li>1 ~ 5번째 줄: Loss function, encoder, decoder 모델 선언 및 각각의 optimizer 선언.</li>
                        </ul>

                        <br><span style="font-size: 20px;"><b>기계 번역 모델 학습</b></span>
                        <br>다음은 기계 번역 모델 학습 부분입니다.
                        코드상에서는 5 ~ 26번째 줄에 해당하는 부분입니다.
                        <ul>
                            <li>19 ~ 20번째 줄: Attention 모델일 시 encoder에 들어가는 source 문장 토큰 중, pad token을 masking 하기위한 mask를 device에 올림.</li>
                            <li>29 ~ 37번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Teacher forcing (교사 강요)로 학습 하지 않을 시, scheduled sampling 방법으로 학습 하는 부분(Attention을 위해서 decoder에 들어가는 문장은 한 단어씩 들어가게 구성).</span></li>
                            <li>40 ~ 44번째 줄: Loss를 계산하고 모델을 업데이트 하는 부분.</li>
                        </ul>
                    </p>




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>기계 번역 모델 학습 결과</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        먼저 본 글에서는 4가지 모델의 결과를 비교합니다.
                        <ol>
                            <li>Attention을 사용한 모델</li>
                            <li>Scheduled sampling과 attention을 사용한 모델</li>
                            <li>Attention을 사용하지 않은 모델</li>
                            <li>Scheduled sampling을 사용했지만 attention을 사용하지 않은 모델</li>
                        </ol>

                        <br><br><span style="font-size: 20px;"><b>Training Set Loss History</b></span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EglvNwdhR0QSS_UT5aDGvLWvghUE3D1u684F4Td48aTDZXGukdwcsEvJflMomg_g_pBBDh6O3ih0qrCuyEfZjn8bP3U5qHDzRs760xy1VSf6WsyQ31YfuE2iMztYsv_a4ac3HfyB2ufyzwuz2eUH_NRTV_zmpPOei7HsQVDdtPeV1TlGqpEqVGvJ4wLq_js0OC6Omabv7VF7N-iJYIIb-fZIiFJbncVwswamBzMBndBruiWlsJLePmX7SittqQqbqdfSJfm-eHcDWYodVL3Lx3oq5Es5xBcxQWqhfsdwxFo9DWci4i0bErkn3x0jKfdart0HXOrVJzarprDerxwVxrxWKrPYVRY3gI-CeUv5KO2IEWR4vRqGaZecrK62bLV2qPIwEQkTgb9sakPxh9ttqaFONyt70sYSqE4jaiyqGMQGkUflPgKqfP-hfwB1PqBPl9qYSGMq1ekXoZS6tFFJUpE-MZYbFhN2yxcLrwkmNiOQcopHoIfebYXRn0AdAhBf4rxOfc1Snn635T22cWyDwb17nj-__8tR0mOKcN-HtxBFVAlAL-xmm-iNQ25kPRIN4QqSDf5QNDkjqeYbKaL7vpxwIunYYY9KSXHlgbpDmDp93llcgiyAbFA_u9PVCfirHQOj5ZcVy49jVlKQkL7PEH5SVtYcXOQslwITX4dR1f4R5sq09Q4cvwXocgaGc2IH_RDppowV8MP_T0TRnPxB8xnJntcSc2OkATz8ei-6qIyeGo5hgHvgNO06dTkgQe6o9yujpxecYi7QEk94XYopdcP9KxR9U8sERWYNd_pTJ0l5xcC5gGOtUw1NGLtTemZP9uIcVgRpe09faoL--2lTafVTDRQm9UZ5zZW8mXhxIGq-xe7txyI2MSAqOTQXvx49Ljuekx3B4VcVYuVif1OvV_aq2_vMt8_JOnb8xrggeKkxbNnZn7XdU-MpgUyealyCb-FDUd4FTblPg_Y5aCwH3dmepLXa-c_F7PcZj_mayDtx0Bsnf93po1v3ZkIaFxKuPazzypZl-YrDDYISatt6PfBhPPgWZ969ofoZ-n2ieyToootVbwJJfa40H89end1Grfay5z6RLhVx_4ejQGro-H2FJ6ZWbdz_f1vPwz5LRB9s3sknJpYByY2V7bv8vumcJ0oldml5palaVXhpXQBlRgP1L18XZB4wnojoGS6PmGo-2kI0z3KrArh6_NbEH_6D7T_a-Xm9uU4rCJrhRh6ZkPN9txCCgmThYpJr0ly6ZDSXSR2f3GdBkGRt9Fard59vklHV-AG8zNsF_RmzvT-PlE6zKzPmP77CGrIp8A4MziGqGYwcjY1qloj2XOccCmmD4Omq1hLbYz98w6uwYQz8p3e76mKL4TnzICuu4Ol0FNa3hiAydjIDFY6dmvMiF-aAvOMoDgwi0tHo3PWQnyL07HjPKxaESDvwNNRKcMTWcv4XdpY7jBdaHjNN1LV0wiHXRtEFwLPKuJ0BaMWcGAHdxrXVHTclN7NTGoC5bXzAg5RR1z6Jtxar08Iq_9NEAyjdwA1QuE8Q" style="width: 100%;">
                        <p class="caption">Training set loss history</p>
                    </div>
 
                    <p>
                        <br><br><span style="font-size: 20px;"><b>Test Set Loss History</b></span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FfnYXcGsbCDsNdcXpJ6GHwk08mGnUBoXJEz2ZGRM3SnL9z0tpbazFsg4HQ5kj7Mx6Cz1pYs6ngkZeL8icarKGmFO7UmzzWNcJ7BCwsrZ5QgqN6-GDbqknakoDp77DCoFJ8s2ohjlURdPcF9Jmf6TQNcap4ltN1Vn4YZpK_XGc8EzOSQbQ1KcjfnbAMagYh2fTMgMQ39Cu8xNlWTzNwedWez41yWvRqH8f4q8YDzlBavps4DVPm26Ugnyfes-sGpJeeXg2xM806DP5gturbfWSRk0LnVH3uEBPb1SNPPjJX72vI9tcOeNNT-USHMU3e-9m73y6PEFDSlOmkVbDcl1YPtzfx0IH4pexgqeRf6wFwRPFjsd8jeb1Mr1hrYgvp7lualcw9epltYRdFWP7KUDrA9gVtMy2hvphk9YIcyt1WuAAJFMdqL3noEYZjjMuEKoAU2XbRodYu1Irq4Yjz8ZAv6h3K822ZfzV1qbYDSQOey8wC5z6K94ileK12LAL-aYVZDQSl5MmsEDuRTy-v-MohQ8-KHD4NRltM0F-CIjxjfg2UT63d0Smt_ilImhJBO2ygcx7XCGmzvu0RzQ_M6i0E0fWT8dRyr8ciNmowW_9XnEjqb5EhydxNnFYFB6kmxwaPjpZXNgGJTJWTYNbN9VwiCqJxtZAam_CF1C_EkotgW_p2XemasjZM2fzUCbCbQThP6VJZCG02z4ELEHt7sIG-_KN0DKu7RWEad_SOHpuNmgs3_2hyoUNsg_L9FkIbIsbZ8GV2evuprjaPGon-eW-7zlrTCXkDwh6cmUjHKdIoRCvs86dJRNiSB0uy0AZ-Ht9HzXNllDrYzGNKdyE_5u03AGsiOOUsMH2FoLIIch9wVE0z_GzVb0xvF4ViH7HHD64eWNujS5yheG7QnJ8UDFBOfFPEpBlJGvaV-_MhIfnxYnLfGCKcvUOe8qJ-2VvZ4k-ipSyiP1cMl2smFZnPWI_BOvX-PhlX9b8-tbHp6dr9i-hJCH3_5GNUUFKG6TLUrAqgtvDNaw0LAFf61DccYmkuorv2AUHFkPUFs7RO-h66MmKu7THP7Bz-35si4SeBZdnA_uWht2eRzaz1YYa4rX8UPJvIci-8ZkxxbZ1onquvfzxIAe1ATtiGkeZbf_fh1xdfeu0cTfFxFUkRAShdR19CXNuSoT_oNLB5MfSiXlLToaHuoyQ1ns8mhI-oau7Ta4cql_h3SoBQHvuFfgjwi5RBtQBsmnWUzl15V5jHF8YSTgI5ibEOHr_0g-UvigE1a7nDWYKuktu2H7dbipaiaSWlFd9NujdLRzfUweYuXdugIeyxvGhiqVa8rt-Gk0o4xZthEf3MlFHxtiJapf7Dv2ATjyGzzHk7QPAWwNhH5hfbQPoK1GArj4FGmZjLI04gaNsadbFawnAO18JH0_mwDuDzYJdHhEKMP7fX5EZZeUBykPWwA-5ZTpy9_KiFjVivmlzlQ3VOd9PCFSkCdRORWAyATCXvN5EVhX0cNhOt8Q8ASA5YpbONrXl5qxW9WMSx84sJYY93-JcW" style="width: 100%;">
                        <p class="caption">Test set loss history</p>
                    </div>
                    <p>
                        <ol>
                            <li>Attention을 사용한 모델: 0.3367</li>
                            <li>Scheduled sampling과 attention을 사용한 모델: 0.3509</li>
                            <li>Attention을 사용하지 않은 모델: 0.3366</li>
                            <li>Scheduled sampling을 사용했지만 attention을 사용하지 않은 모델: 0.3491</li>
                        </ol>
                        Scheduled sampling을 사용한 결과 loss가 더 높게 나왔으며, attention을 사용하지 않은 모델의 loss가 더 낮게 나왔습니다.

                        <br><br><br><span style="font-size: 20px;"><b>Test Set Perplexity (PPL) History</b></span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_E2N0rMubs1meF0eUWIPcf26ToLQZID9DEnOd-sEBuYFDYwUIljibZ6eSS3ery6HOFOUyTceMh5jeOJ4FTy8sfz8jtrsSj2cOFaAXamzZT66siM1uOyjvKJNn-M35YxQYQlYJ0zjQ8QJeFOHV0MuMsj4oLXU7OlUfS2ts2x3fn3ODjTkuu0_99tk3ohNKjiwzImUl19__wXNG3ZtlIkSKtwrtnQjLE7A3KHlGv1fWyZg2yUANBsxvX_Qf_7nurbUnrg43mEaSYBQEqkQ-GS5qU6mOO48Wmuk2267gmOPgaUGrORr08UPFjafU_PdCzYjHVuXOjrtZdZX7sfZC022o40DytZW3d4i-qNrHuJvboNWdf2FVkTJkEWaUGRkPd3RFPNzaA9EEMASgZoE8TfsIppS9cLPWnd_juHkYBVXsKk8685QyYDSm_2NLxE2aoj2d7adH1C6UxMMx5cmvlmCyc3HTxLXp__OD_co8mpPB1oeOjG-oO0vwHGYd2yMhk2u2q9opqRLbzAfU20ZWls35d7L4OVcaCfaXCf6Di-1UctglFTv65N1MNjyBUXUCUBkzVX-wF6SNn8mdyv7_PasIe3HGasaEfqUrAter3ppDMuFeBD6c_UdfURMnas9ZOP8STjBAkwA8Xq0snras3Hb7CETZ5EtaG3IRrnSOTXshj5X-gsJ1UcN5ay0v_KWzn87diQKAKwKGQMjPWChhJ4B9k7ZyCWzm2uCQ34C8ksValYeCkiBaNR2FAVkaErajzYALRBpLUKej6S4K8p7s3ERZl07YoHW9KHiVPxtVBUmQLFfsfdSAWbuG9nRiZF4OyRwarOn8V9VNJF9WAOSt-uqFprdZ2C0XNNNRHe1W1FR0GyVA8v_1gYuGCzW0hmWhCBDL2FL1pzp-fWKKsldGIq8xKazvr--u-K4kOuOpU8NxMtUHBvCxeUkM80lWPVQ0gZ-Y72kHw3NKL33-N4MXj1bBp1qFz7_IoUcGk4Rnagkhd4vb84UFk5BKJcKEzyiTZfxZc7oXOCfYtgGbZstSPe4WFqG-AilD09IDzISmC5WIfrIVu8juhWocjn06iPUW_IUVoBq3Giu8cyuo6WsU6uRXXPmsmtk6OYeKMXvGotpDgpx0YnwSuwXDC_OdGOvvdJpiE3KBqJtzUndwVTc3ZisVGUvjDR5SVkR4bzDYF5Ho0Szc1bnZe9QBD02DqHkYae_MDen610R-HZFxBikCp8pLSmiPL5Jj5vS8LyKJ0vTSlx4yMXF4p2rFZz8N68NU-11-b86vSTMisJLWOfeS7U2bo9lP6TjDH_mMaaAKO1HxZEn6Gc0MNw3aB3PVNO4ZJ9lPpr7ox6u8pspftePzY7RNHjlNBj2XE8Rc8KWczWJjiMOhv0cc61h7bRkuhm6QQuyO8jLcyqkhezwBlYFnDi5S8SpxOs3o3-2i9ga-I5w0fDzsMUgteNHxAVm_ZWVWvvOx8kLdoUlwHucZE5OMBSTkGRWQf1TArgIQNMBHNo_uK1k6AcfRl0RZYATqXewTkSznd-GXIUCQ" style="width: 100%;">
                        <p class="caption">Test set PPL history</p>
                    </div>
                    <p>
                        <ol>
                            <li>Attention을 사용한 모델: 1.4003</li>
                            <li>Scheduled sampling과 attention을 사용한 모델: 1.4203</li>
                            <li>Attention을 사용하지 않은 모델: 1.4002</li>
                            <li>Scheduled sampling을 사용했지만 attention을 사용하지 않은 모델: 1.4178</li>
                        </ol>
                        PPL은 낮을수록 좋은 결과이며, loss를 통해 계산됩니다. 따라서 test loss 결과와 비슷하게 scheduled sampling을 사용한 결과 PPL 더 높게 나왔으며, attention을 사용하지 않은 모델의 PPL이 더 낮게 나왔습니다.
                    </p>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>Test Set BLEU-2 History</b></span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_GgBf0uHVl7T00zu2MVAZvh5tUtguS6JIV7uaQt0jdhwjHx5djQlyGFIUG9DbAjNtFXQI5WxSpweDVsOG79cxDoOFQp4LDMS2P9wAWYADMgQ4JqJqtuHKSyFFHKwhBrmZ8wMBnbIcJPedAUaYDtvZ67nV6TUDSFpUCWDvRBeBESzWG6iVw0iGIqRGLLFOtAEML_agyG7dVMi9dZCz1eZIlb5tYGXBPlS8ikVsnwP3oai1Qiqmz2vC-hKcVW4XnwiViuMLxKemBNn_Ky3NTw3q7lClcCpyeflsKV9x9jBCV68vEBKSSJ7cx8YGEZtMRu3ChYmnl1awrufQ3jpkJjsFJcLBXzfnw67FjU7BbQRB8OQm3Rz_jxx4Go_zu_qR7rzq-qTo9SVcXeDoQg6QjSLuG0x0doYhbRCXgdPPhN2_hIkURieCamxul0M1fZY3RplN9AIKxwhAKZxsWpuZe4HcZPx9RGOmSnOg9DeMJCl0cXriaZc4eWM99zmpTPTsSZKAtb-gDt8ymJDAd4XGGh8R-SR3hvwOnIpsC0s3KzKoPqdfmDiTLqcQw4wa4KjGw8gae6abPb1pYrVYL8b_bpZ9PItXePE9Ps0QCoOTwj0hk3vu4S9JW4anB1048S7rHKTnUvviSaFBpS7fpYg7-0YyLtOYldx4zpbF5rU3S40fEoIJJlYKKhHeCEspoJyQjJM3HsHuUkDb9MBxA7dle9tJ6xJanz2TysFGAsvZKEkJULtBMhSzp7WvDfT8Ihg8JziTYQ02BmuIV1cdcEhyFHt6-y9fUPwl9GE03XHe-OZKk6NpOwnWIdI3eSGeByBuMV6KkiO6zj86Ta_TG6TIAeM8RdmxK386AiztuzQs1YXO93PA3hj84ky4aJR9q6tbWabEzin-eW0SHs38CGwYEJNwhstrcTjFf0EpZJmhmdfWFMXlVk1J_xDXZ1tqwgp2lxNXHoG7cQZj88M6Jvis3MSTSR73jHXIYbFDaI8bAWI33dNlA-VulicBD2re5S3LwWshMPZYMTPOp1iacDorGlbnfiVELaaMSNOpHg5hh3BO-7JjVEZ91gBqLCjNiJFjwxdEZFohSS8oHmuBTlYrhF4VGi16wAWIqxWqOyiAWlLo0v9BgrKkyVkq80r5IOcFwMCCGVvi19H3aBIlYPLEKPZLmPpM3vXigILfctPQaQz4YWaP5AZWg7Mx23vVM1ujH7i06aip3pdcEQY9jFcuvo6MlcHfYLlY2bARo0m26dvuu3JeSExAN03KuLSiKxyZjd7O-62OS5lOyoUEBSF94IpJz9-VDfNXJ2S-oXPArqBPWYN_qlPnpSFX3ItYlaTX2FvkCHbqj4SVxfXRxVOaB13ZuuihcDm_sJnkEXrNGZMbqSozuzWHUxq7duz7Jq5B6jSc38kuV9-nq5mta_OOHy2b9QtlZYvrE41gs1oJW2sagN5L2aGdbutIhsbWNENoVvMXI6f5lUKnqhIr6ABo0K6UCe30RwD4cjONXtLCc6esYwEAufE-YhFnqy5tPyACwFw9HLBKo6jy3-" style="width: 100%;">
                        <p class="caption">Test set BLEU-2 history</p>
                    </div>
                    <p>
                        <ol>
                            <li>Attention을 사용한 모델: 0.5789</li>
                            <li>Scheduled sampling과 attention을 사용한 모델: 0.5646</li>
                            <li>Attention을 사용하지 않은 모델: 0.5735</li>
                            <li>Scheduled sampling을 사용했지만 attention을 사용하지 않은 모델: 0.5656</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">BLEU는 높을수록 좋은 결과입니다. BLEU-2의 결과는 loss와 PPL과 다르게 attention을 사용한 모델의 결과가 더 좋았습니다. 다만 loss, PPL과 비슷하게 scheduled sampling을 사용한 결과가 더 낮았습니다.</span>
                        
                        <br><br><br><span style="font-size: 20px;"><b>Test Set BLEU-4 History</b></span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_G8RhCa-57jMQCwXNi-_gIXHM8z37J9F_ZfJgps04NDqHMU0aaBdbp7H4DX0aiWS5n7tkOPXDlyyyTjmgW9oi-K_Y6KO5z6CRgoE32UtBjA6QeOdYL1vT2DW9EJAHx5SjX7Az5GhHNKk4WuOvvZgncgmhfo2amRAsQSM1FfgZLa0VEBpOo590DVjKxY1m9Oyi1KVqJnVXFXSAUijP9-GiHTKeMnqdj9oYVpZlSoSUGFfbi5FuVzV2u9LdCfYpZmAdJLscqajwvLh1Y6vG39I15JWPUkJK4qhBpRJZk1jRbMFAWo6QaAqOxGrWwEFoEaByOho0bpOLPEJg8xin_BiaWa_KA3-OotsLkMWLnHs753VN9mg-B6SL5RQJ2b61zP7muEDmLPnd2ZSpDj7FFOkt_zbVHFGG7UQlcwFiPvN3a8U9Wj2-sjnawIeCM2uK3OKQFvzdqOX_EvB6ujdjyO7SPl_YQSzcY7mXJS-iKPfVa4QFv9LgNEf6Qb9fX-CjSMz_ddWE5RuVNYCWx8YAf7JTBjg5xiU7_qzXCUWcsYegkib4_75WgqadFqyaydembt1O5eOKTr1NRbT_WXvJ_Gfy-IF4ckUCnjSqSa6_pLqsgT_3HfQqkhmZZuo_7Ubz5mM_ZK9Ol21J4TNDvlwGQnU1lpM4ntlRQZGt0oPdX9F3Vm5AGnHwm3fdpCQnZ9Mw3edh-qela8AYhPzfjOFtOSmLhddFIr22S2sakNJOJT8I5OQEGxG1ojwlqBzcM4smpPDt7rgQj1fek12vUPS0h8NT1IcNJwAIPQ42-UBqXVTB4uAC5zP0jc7Ra7vG1ZQAVwffe_j_KLrP0-K9K-hGW2Hkk7VpZzN1vEGahRDNwVTNAZNc77IcnLacePKCtxHYbachLIdFfLnjTqGNRfgvhMwiYF0_Ys1YLpyX9Awb33xlswDTXyt_gGneUUQ1D7IP9gch4-k_R-Hc_uEUCtkB5qd3BwxPO4hz_I6xLjTfInGfv2JVK_9MIFJgf0oo4IA_Mj98PxP8KgF6L2-mJR9wAXOU-fhBFOoVYkV6s2rSAYNxGyx602br9As2kyQu8QR5O0xGErrIPwavfq1zfJpthITnfBsRHRjtfxmr5jfOkxIR90kxHJwztuGrcNUHaEDivCY5yYk8ChU69sdnnQCtx1DM7v9q47mQ7auZlIQm5q18oYGuKHp69rXPWxbEchXvWmM3A_Z-NRkmDnMVoOhJjEASbM2zGCULLkGxxJIcp702Z1hcXgSSIY3VSBqwhErc52-6CfjCyBGCbsfeEw4Gbwc5-pKvV1szPuul5uUg5GSgZIotOt562KX23iWJnUga8QaPTAFAgl1RljJRBQ56Fr2jVEeFr--NKb0OISYbxjSJgcGHWJUlb5L6jBfJzeomVYnFlq0ZcdwB_PC0UIa1FmUEOzIM8ZdOe93BkPNfmDtPi7siENxAvbVHXOtYcBHYOCtAUaed4RxUk6GEZp67humN5BVL992u9-So9MPscZTokTA9t1lIEQCwd4_qRAwU1_WSTRNkaKTA" style="width: 100%;">
                        <p class="caption">Test set BLEU-4 history</p>
                    </div>
                    <p>
                        <ol>
                            <li>Attention을 사용한 모델: 0.3996</li>
                            <li>Scheduled sampling과 attention을 사용한 모델: 0.3834</li>
                            <li>Attention을 사용하지 않은 모델: 0.3893</li>
                            <li>Scheduled sampling을 사용했지만 attention을 사용하지 않은 모델: 0.3849</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">BLEU는 높을수록 좋은 결과입니다. BLEU-4의 결과는 scheduled sampling을 사용하지 않고 attention을 사용한 모델이 가장 좋았습니다.</span>
                        
                        <br><br><br><span style="font-size: 20px;"><b>Test Set NIST-2 History</b></span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FVbxDo4aDJK3XzPZ-IEHZD_hLMsO9UWj_ynLcV8T8YioVkEWEdiYRm8kMvcwuXMM3VfmvF84IQTsL9I_yKHLCJ654yKEnpTVcbjwg0dX9AOcB1YAErUjkdd6noV5XHZNBbPP90VtQuIwDNGNG9oRbpVFzI05Zr4NYSzsliYO6rhV8_sNT2YZNiJju0307vUep6cdGPSvZ7IHSRa8BwQwU3zM5ziXyRGso91_KbJ1vFUXbkAtAHTHOGIv_OGiLE-diy7wzaCRVHJG6DKgGgyP0-fYvnYDYv6Q9VMSWCrHcWqvqS1Uf0ja0u1lkyTdi_NjNClQZ_iE2pOkEhsdvkzblz5PnuXnuAQcW4U-jUsogf1cSRLyIar-6gG0F4FT_Qo6f8eSI5qBYbAKMZeMP2NAZzYlhsnc-maXfzmRLX3ZnLmkTLFnDtka8K1tLH9-KThxEWuL5Yzan5SyIT9lArnrJO7kkDtVSsPe3jMIqkq6Y-hadTVmt-4B_n-rM2y5jhUSlp23Taw4QsC92LDN6gErjYhdpnvnVaitfAf7tbepFCg6iBlc8vKI6FVVBH1WpKrVC6iQD_Zc7P_x_xtSBWpBzivqn8rPUexqzISeSjUDmWUHtIexzJB8a0fz0TkfcQ6-AX1YtEJJoiLCqZNxbfOWgPD9MHpZpejpnSXT8ApVufMi9y_UCikpXgUfMrolllHr5umC9B9HPNAeMkVskyH98n8hWaXnqksVGe-hZJio_w5BjZoqv_hSDKZDezsfWISEBgDhvES0XTkQAYXdTwky9Z73EnibA3X8tc0qJlUWNM42TfZXAU3ct2USwjUb4Z4Iz63cIZAslGooqOnZKO0y7oP3jcNJT1H-37SEMlpf68KDWnGexZ22_WojbpV8emPh3x4ZiNvWjGOjyjUWrNfrYK9CGIFpdj_E8wl7PvJSVmNqdcpCHBRoeeNYW19RXvAdSyjoE-i6CqCgjPwa8xpEvy7ugwtVZV-7sws1XQBFhTlTySVlZEOsWzQG2bnNLxGqVBij6ugzABA0RtEoYuO568-hSeqId4I63OWmlLOHVNeVq-2vVJPG2aIdmDjhVuLDtKVdAXKU3RKBbY9NYCwvTvXKRXFFA3RS8vAKnBA-KN5h9W59LSQdBhxBoLSKYNuaiFJd7h0kGpyr7BsOn905CFqeVpCGhIcg0eyHFuAnC8WN92VRb7v13Lmul4i9J0OagIE26uaSu8_SkL-6Ql1ybWiopAbFilHDMJUzW_eii9hyxWwvol9amoF1PY0DzwPmzZRRcT4soFrbIzO4FrKH3mX-k9aw4cFI50bnEEmMYG4JU8Xw0cZniGSTWWHF6Aw7kMIH0QbUYlOxOWvjEwamvLXHvnXhAlhGiw__Zt0hpgPgCRqOjwsW6ZdQClUGZPGGV30chySGY0ICQUIWVQBDq6HS10SR7HNGLYFJgykdykysK7pCr_RG3g_3jpEorNXJy5JaYwlAaJtm_Bad00se45PK2TgxSsFyhJt0tqv740PUSPr02orV3UO8RBRDeBfNdP6OD7WMt6" style="width: 100%;">
                        <p class="caption">Test set NIST-2 history</p>
                    </div>
                    <p>
                        <ol>
                            <li>Attention을 사용한 모델: 6.8475</li>
                            <li>Scheduled sampling과 attention을 사용한 모델: 6.6922</li>
                            <li>Attention을 사용하지 않은 모델: 6.8016</li>
                            <li>Scheduled sampling을 사용했지만 attention을 사용하지 않은 모델: 6.7098</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">NIST는 높을수록 좋은 결과입니다. BLEU-4의 결과와 동일하고 NIST-2의 결과는 scheduled sampling을 사용하지 않고 attention을 사용한 모델이 가장 좋았습니다.</span>
                        
                        <br><br><br><span style="font-size: 20px;"><b>Test Set NIST-4 History</b></span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_GHhCtl85TeHvggpvJ9Gw9ehlqltweJWWXmmlnBh9lh0dR0NYqSzXjbvfboGlFWjdFc8Of6bsXKRZ_3vV5UGgrN27E_B1UkGGYtCN6rFy-rgItgNhNR2QqYDiGu8AjxHFi4wWPvF9_Gb-GIvugY1f_jEJSQfxdIxP3bt9tdHek9hhORWZ99Hu4KOckLNCht6SLEg59Ezw8n19bF4w2FIDV26JNWXOVASGHGLjGdKRfYVn5cqQE6WGAXeOw5IBeRSRjxrYquryqw6u2lNX0HWKwaIttIHpfM900rQDS8eZP8igDo630fcaSUKqtpei1iJ0X8jGLKsjgKwsK8RBTmUy9xvSz25OOExdtpICY54Fb6uqOJvcVD9r7Y37u4-MoMeIfVZeU08cWPxbjX_j4S9jm-yh3Y5HzbHH_DlqziXiXDUAtl1ldaj8BOaND2pMMysDG2Qp2XJuyZWBiPUIna7sj8wW9Bhi37MYWZXnxcjm0nF-JlGogXwvYQuWdWCnccdqAUyveADCzi6xV-2ddX7CydH17xceeBls_frdI1nLJnYF0RNzebQLRTWtk5Cc4I5fARalpmvEhpuq0R0KKbxQpvEm0_FYJ6STXVnSWZS_DWmXEOuEKxk4RS0rIREnia3c8ge-r7rhYbYHMYyq9oBKtmWsdl0m8N2Jc-gxVERwfwY1HcZW2JAr0-IbJe_Lm7V-1_hhX-UErZ0g4s1ABNYfpqcaCNGPml4NKQ5DJzds2__cs-5tIWBHuBOGsYUkAZR0HOnyBFFeSebnAIbBFULUh0mzm86KXiMKq41AEeSUdR114_NZ09n2K8i0T9Fnxj_orJ-l3Q5Z9pQDbwTaJaupPprUUHcfpQVCWIZnNIPBQ7tNWxEsVsRNaN1xN5NSfVaKmkYgyiKZzUs_6Uw71meUR_FeGUeeobmmHrWdWGMDh8mNlOufhH-1Nt_obnOeUyw8AO7rdpN2mVuX9xDgmpeaNKQUoHb5CVQO8wSODUhdZZoWLTS8_iXnz4HGTnyOWlqosTDC35nSM_PHlc-R5b1I-8fSTekkWZBnNu1_v2rt4-p-EEdbH36Tj52S_kdozQrsW0741hxNPcBJsvBZW29hlmjiU320B1UgNhdTUr_AndU78y94XWYbZk4VQEILTUq64cjC1RX_Yh6XWIc9wmtMBPt8epNm5adW0kELb862h7yaIZIpi5iu_CxRaFfiYJG8ILYbBJMhg2xKCKjIRvUrVvqoloeZE8_pXw7h2C-OJZ637ScZFpcdzpAYMTSqqIBxN15X6WzGrPLL2vUBDd0b8tq9GOiuOG8PPtn7xPpUbUdTKv0eCnaA6Fr5alDEd4q55yAOQGwZRVe1AfLtzC_ZKMOnqdxf6uWSSSx1XUgRpQ5ka2piXEvyT5c_ejlSB8K8q6s0acT0qouNtDRXCeUH-xYSgsQj67ehSfs1cO3IaER_sFWUYvO3E0nwFpoCHrdVhO6GAEvEvvgH3uiAIG-np9sgoM6Fc1fNAXXNVc-U42ksjSI0SW2i-EBUT-bBBCzEyggF8QkN59" style="width: 100%;">
                        <p class="caption">Test set NIST-4 history</p>
                    </div>
                    <p>
                        <ol>
                            <li>Attention을 사용한 모델: 7.1627</li>
                            <li>Scheduled sampling과 attention을 사용한 모델: 7.0052</li>
                            <li>Attention을 사용하지 않은 모델: 7.1178</li>
                            <li>Scheduled sampling을 사용했지만 attention을 사용하지 않은 모델: 7.0177</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">NIST는 높을수록 좋은 결과입니다. BLEU-4의 결과와 동일하고 NIST-2의 결과는 scheduled sampling을 사용하지 않고 attention을 사용한 모델이 가장 좋았습니다.</span>
                        
                        <br><br><br><span style="font-size: 20px;"><b>기계 번역 결과 샘플</b></span>
                        <br>그리고 아래는 예측한 몇 개의 샘플입니다.
                        <ul>
                            <li><b>Attention을 사용한 모델</b></li>
                        </ul>
                    </p>
<pre>
<div class="codeWrapper">
<div class="code">
<pre>
<span class="annot"># Sample 1</span>
src : when i was your age , i had a girlfriend .
gt  : lorsque j'avais votre age , j'avais une petite amie .
pred: lorsque j'avais votre age , j'avais une petite amie .

<span class="annot"># Sample 2</span>
src : he gave me some money .
gt  : il me donna un peu d'argent .
pred: il me donna un peu d'argent .   

<span class="annot"># Sample 3</span>
src : please answer all the questions .
gt  : repondez a toutes les questions , s'il vous plait .
pred: repondez a toutes les questions , s'il vous plait .
</pre>
</div>
</div>
</pre>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Hf5FLH8qLBxVPIqBlO2I-hwG8YMPqPWXJG1LnfaR-MJFlxWPZTYkeFyFoJLt6O6aY9arhUJrjgXDq6p4aGXbU8mjpAKtyOGMNdJbzQztgOzbG_1dAUSu0VQ3pju2oxjtcFgjUJAVytwkdDgiK1IO6y3wlM9Bnt6_DjAPpIlLqxvOvjd_MQOhwde7n2REIXgOLodROIjBVEyVFaJ79iVi02Ddyzwx3tPoW3H6jpHfLoCFugW4cUPI8Ga3EXesyt4ELRb54jNZNgJm743D-f2xLzrP3n6OPYZVIzn4kyT-Aafr9j3YCQ0vCds-fkfmoUx3DuA3RthuzNTdEQTIsjx2R3KP9-QGdmtDaqYnMREvs4OwF1chwLgf0lIMpb3RqD_WPNOlPa3-edpWsLrkZXNmmsq2cRLZSrh_EImzRynLPpKFT3PvA17TrxX_i2RDEh_itk1rIQkruQV4oSrsKh2RTzRbTOcj-Hj_QzVbKCIUGSduiKrZE9vYsu3DMcLm3ylijFTB58NL5tmAmN9AJZZF10h_huao4u5EaZmaWxJthTBVWSDarCexnIww4_1yTlLLzyrIgLjIqgqfWQjIPbJJE3O3Nf3683fvQFeLGpmFHj4NYReZDVDl5eGmk-4qZ3e4CIh84zP3fn6BwyghaDUhoks6yPKi2H2pbTEUrJbzktv7e8ISoAw7v9G0JsSSXB0BVgVuL4xTSn1XYsAZXuz3WB4OwgXkLircNR0BN8ZUGRU6noD2PxBYO8YTi2-957K0BUznlB9bK9KxdmWK0uBjQ5oq6Cl9SQnE7GMnDpGKM58ZPNALEg3Yho_yAgZMC7767gRKdhNJNyiRHou89g3xpaYprCgJQhG_WtlxTZYzd5fQHqXYRICrlq5AyoK20lc5_YRUtFMnNfrDPAIKGQL6VQYfCwU-csb0sZ2IutbTuzvQgVUSfe7Pc4OKPjTrCI9NvGqKk_WqDt38T278Ft2cvMY6Wr_gfoI8IH9XLp8Z4ZMSPKCEnJE0MH0l0pPUTExLJ00RIVZcOw3Q4_r-1JdnzOgDX2Y1O7U80X-bVHzQY4BUgnJoLKIdpqdlVcNoGvqdmg3BReYdOo3N2caOakQsM-tKdbfiM5w48947fU2W37g9iGe2TeiuwRqOTNr7dhFRhgE2wwKNBYNYBbEU4onys0L55SBrR3zMlj6lDIbD9AEljDn03Zy4G3sdcT02a1eknTWKncgGNTulTS_eW0H-gMVXIUd7Y_GlF9YNMGEVHxLq8Lc3G5C-iA7Bpchwom_GuIQt2kqyfbPv17hgn9bF4NdMBAUzdYvkbUYAZ4jSnDQBYpE6JpAbxrJhKYn1I1FrWKbr3uNUKOAKwjKr0aAgs-ChQ5zYiGjl3KGYVgqaQZ-sSgSZ4FonEvXqbOMyKWEsB9Oj8T1HQwdzJptpsZkWaLZN6d6_YQ7rCFFi5ACl0OGmkTQLZVjK8aWa_Gtq8MN3tVte6tByA81m2AuvZpV-MP6g7sh-n_wOy3hYCKqWcUZkgpzuFcb9ChW4PyrufUmgzKJRGD0roO" style="width: 100%;">
                        <p class="caption">Attention score</p>
                    </div>
                    <p>
                        <br>Attention이 source 문장과 target 문장의 순서에 맞게 잘 align되어 참고하고 있는 것을 확인할 수 있습니다.
                        <br><br><br>
                        <ul>
                            <li><b>Scheduled sampling과 attention을 사용한 모델</b></li>
                        </ul>
                    </p>
<pre>
<div class="codeWrapper">
<div class="code">
<pre>
<span class="annot"># Sample 1</span>
src : i'm in love with you and i want to marry you .
gt  : je suis amoureuse de toi et je veux me marier avec toi .
pred: je vous amoureux de toi et je veux vous epouser . toi .

<span class="annot"># Sample 2</span>
src : what's really going one here ?
gt  : que se passe-t-il vraiment ici ?
pred: que se passe-t-il, ici ?

<span class="annot"># Sample 3</span>
src : we do need your advice .
gt  : il nous faut ecouter vos conseils .
pred: nous nous faut que tes conseils .
</pre>
</div>
</div>
</pre>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HqwYNy8XDPpEG3JK2jaZ98Ghxz8_8z4qkTqS-pZtcBAykhx5gmRzpuX-fc1BIRSRdYp6LO-EX2oeP7lfeZ7yNmbPwkTJflkP41M8QPpETOJnfwXFQJHvgA2OCgcF60YQCZ4Pb2BzAuZCqq7j7qcdQBivKFvNBI3WZOFvg8euHgeB_RNICYp-rMUGEaCS4FGgRnt7LvFAIBC6pQDRp8l4UJCefAMivUFnYOf9oihBJejgEk3P_pjpYRg-itXK6sXv3yd-zuMk16ddu2xViNyn_2U0zRjpH_3V9Vg9C7ycYf8gIg0AdpcqgLN_Pa-c3xt4_VaZJmLn9BRVbsrSr7ZGZNcQ69HpWOHUSCOgAxf43XZYHIeAWuHZ-xspKtuoWzsurfIZLH3hqikGZdFHEz7xYI70sLHqo7BFykY6p4AtuH1CAKUBxju3-t-njP37RxyS2c9vymKXv0yiP8iHZjv3la76A2WGJHHSvCGnX0deMdZvKS4qgKGINiVmRZ4hMiJHUrdsKAj_RcnhmGrgQihsq9XCp97GFadsik5H5lqwmQXIlKYnqyAddBd9Wb90F3HQYho5LWkp_zrSJDcNsdipAVNE1f71LTqI8ItgJVirmcOHsLDPfcuMQLoM82v0WEA6rfba5gKc0_xoNEy8_k6TjkBS6jUW646ybzWQsAPC-vznxZetggsBMM3VdGROM-k-P7yrt6ciS321umF-MEEvoD8ubJF604AX08wLXepoYpDF_nZiFYmWYEyUdYBm-XGbdUQzK6WBCRB4a6n36thHIrdDVyPm0-GQ5vdpU_uheWs2XCs7umijmsuhwUOo_nbOvedkRvI4uL6QF6Dau5GRgLUyZXuevw32fzcYxw4lzLn5q3DupoKv_sw6Vf8-DC41xEjNf4uqtKyWL4dCJWzYaXesVni2FAo5XCfpn3XGD86X7fEpEz_1uvf4zyaDoqvoQi2LWy5ZAh7w3LewzrONhEL14YqSK33kgOM_Uom2N-TsX_ufN0f1KP6kcmpcneqePojF0K0tTtN_A8wQ5QpiKwLV19uaSLUFF3w0q6dWQ3aNO7bXE3EY2_u-v6nT1W2tNeiem3y70aBUsB7_0_fMpuPMYGFZIkso9L-P7Z0wne4MwCR2aSjWPC_5XbHD5T5Fh2UCw3B1VxUTLJAw6oX29ven8iGFKucRV1dTJB2AFBh3Mbi84BPbKZXrSoxTjdfPOTc7UDB5efBq3jsxdJm8sr2AeTR2lNLr-cojcKVLBVOHFV2IqQLnxXxUiWsVbp8qHbeVBrNs3hQ0a6G8hjFqdGaKfgs-6mJDvB_si0JlAN1xQcRXyiMzxiix0pF8WO-b9_sGFchW8C6qqWwU1-pIycWu-K3kL4m4OQNk4OODXUtSAOdMvBt-kOTKC-NGHSN7uwAL6d--nXdcJDUr0-LEp7euBy4kHkZm_-8qxSfXEmbIvueuO4OA9dvkuQa112aZFPROLqVq1R_yVP4DvuWvEafFwQjFG5cYTWghca33vePhxpmSkFxryAD8fiFZYwimMTjkXIRAKZ" style="width: 100%;">
                        <p class="caption">Attention score</p>
                    </div>
                    <p>
                        <br>Scheduled sampling을 사용한 모델에 대해서도 attention이 source 문장과 target 문장의 순서에 맞게 잘 align되어 참고하고 있는 것을 확인할 수 있습니다.
                        <br><br><br>
                        <ul>
                            <li><b>Attention을 사용하지 않은 모델</b></li>
                        </ul>
<pre>
<div class="codeWrapper">
<div class="code">
<pre>
<span class="annot"># Sample 1</span>
src : tom asked mary for some help .
gt  : tom a demande a mary de l'aider .
pred: tom demande demande a mary de l'aide .

<span class="annot"># Sample 2</span>
src : you see what i mean ?
gt  : tu vois ce que je veux dire ?
pred: tu vois ce que je veux dire ?

<span class="annot"># Sample 3</span>
src : i haven't talked to you in a while .
gt  : je ne t'ai pas parle depuis un bon moment .
pred: je n'ai vous pas parle pendant un moment moment .
</pre>
</div>
</div>
</pre>

                        <br><br><br>
                        <ul>
                            <li><b>Scheduled sampling을 사용했지만 attention을 사용하지 않은 모델</b></li>
                        </ul>
                    </p>
<pre>
<div class="codeWrapper">
<div class="code">
<pre>
<span class="annot"># Sample 1</span>
src : let's take a little break .
gt  : faisons une petite pause .
pred: faisons une pause pause .

<span class="annot"># Sample 2</span>
src : they live on the [UNK] floor of this [UNK] .
gt  : ils vivent au [UNK] etage de ces [UNK] .
pred: ils vivent au sujet de de ce sujets .

<span class="annot"># Sample 3</span>
src : tom doesn't understand why mary is so popular .
gt  : tom ne comprend pas pourquoi marie est si populaire .
pred: tom ne comprend pas pourquoi mary est si populaire .
</pre>
</div>
</div>
</pre>

                    <p>
                        <br><br><br>지금까지 GRU 통한 Tatoeba Project English-French 데이터를 이용하여 기계 번역 모델을 구현해보았습니다.
                        학습 과정에 대한 전체 코드는 <a href="https://github.com/ljm565/neural-machine-translator-GRU" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">GitHub</span></a>에 있으니 참고하시면 될 것 같습니다.
                    </p>


                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#GRU&emsp;#기계번역&emsp;#ScheduledSampling&emsp;#BahdanauAttention
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="pjaxPage('RNN3.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br>LSTM을 이용한 IMDb 영화 리뷰 감성 분류</span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="alert('RNN 마지막 게시물 입니다.\n\nThis is the last post of RNN.');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br></span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>