<!DOCTYPE html>
<html>
    <head>
        <title>One For All (OFA)</title>
        <meta name="description" content="OFA에 대해 설명합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/ofa1.html" />
        <meta property="og:title" content="One For All (OFA)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="OFA에 대해 설명합니다." />
        <meta property="og:image" content="" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / One For All (OFA) / 1. One For All (OFA)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url();">
                    <div>
                        <span class="mainTitle">One For All (OFA)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2023.04.21</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번에 소개할 논문은 바로 2022년도에 공개된 Large Multi-modal Model (LMM) 모델인 One For All (OFA)입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">OFA는 이미지와 텍스트의 modality를 다루며, multi-modal pre-training을 위한 통일된 구조인 Sequence-to-sequence (Seq2Seq) 구조를 사용합니다.</span>
                        OFA는 이러한 놀라울만큼 간단한 구조를 바탕으로 다양한 task에서 SOTA 결과를 보여주었고, 이로써 multi-modal task에 효과적임을 입증하였습니다.
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/2202.03052.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">OFA 논문</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>OFA 목적</li>
                            <li>OFA 구조 및 학습 Task</li>
                            <li>CLIP 결과</li>
                        </ol>
                    </p>



                    <h1 class="subHead">OFA</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>OFA의 목적</span><br>
                        <span>Objective of the OFA</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        OFA의 목적은 3가지가 있습니다.
                        
                        <ol>
                            <li><b>Task-agnostic</b>: 어떠한 Task에도 적용 가능하도록 함.</li>
                            <li><b>Modality-agnostic</b>: 어떠한 modality에도 적용 가능하도록 함.</li>
                            <li><b>Taks Comprehensiveness</b>: 모델의 일반화 성능을 높이기 위한 다양한 task 적용.</li>
                        </ol>

                        <br>일반적으로 위의 3개의 조건을 만족하면서 좋은 성능을 내는 모델을 학습하기란 어렵습니다.
                        따라서 많은 모델들이 pre-training을 한 후 각 task에 맞게 fine-tuning을 하거나, 아예 특정 task에 특화된 모델을 자체적으로 학습하는 방법을 많이 사용합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">하지만 저자들은 이 모든 조건을 만족하는 모델을 위해 Seq2Seq 구조를 이용하여 모델을 구성하고, 이미지와 텍스트 modality 모두 하나의 vocabularay로 통합하여 다양한 instruction으로 multi-modal 정보를 모두 학습하도록 합니다.</span>
                    </p>



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>OFA 구조 및 학습 Task</span><br>
                        <span>OFA Architecture &amp; Training Tasks</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>1. OFA 구조</b></span>
                        <br>OFA의 구조는 놀라울만큼 간단합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림처럼 단순한 Seq2Seq Transformer 구조를 사용합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="" style="width: 100%;">
                        <p class="caption">OFA 구조, 출처: OFA 논문</p>
                    </div>
                    <p>
                        <br>이렇게 너무나도 비효율적인 언어 모델 때문에 저자들은 contrastive learning을 사용하기로 합니다(contrastive learning은 실제로 현재까지도 많이 사용되는 기법입니다).
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 contrastive learning을 사용하면 아래 그림처럼 하나의 batch에 \(n\)개의 image-text 쌍이 있다고 가정하면 총 \(n\)개의 positive pair, \(n^2-n\)개의 negative pair를 얻을 수 있습니다.</span>
                        즉 하나의 batch로 두 종류의 데이터를 얻을 수 있고, 총 \(n^2\)개의 데이터 정보를 얻을 수 있다는 점에서 데이터 효율성이 엄청난 것이죠.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOYag2YLKwg4Hkl3OPI4MS58D5L4JcI9sguJ0vR2v335Fzn_tCz5-sBu0RxMDTCbAPTQeKLTGL1IhqTpgbhL9Yg0vUQ-DPotQb9fF6YhCrgjG4gFMZWJqx_CrCIkF_OgdbQIuLOPNybLkHwhIgZia82nsYb4MmSzeIrwQ4J9Lf1B9aUfGitEw10CvEEjjMshPpNOU_0Im_wFcYMcivRUQib_Y5RkXe4CFhCGoPl3JB0wn1No1nv1GX_u2j9A8oCp-GWShaMW97LHl1jEVRL0hvbRn3YrgkzT0dibsG3lB_PAOX_v5AKixkmZ-D9c7bSXF5YdbCfzVcbgXAAZ3VNkfamGUe6zkfflOGbzn_jDtxhaOh7Iw_fF0tSfIE7fmoS3dqbmed34cVwvpyNNvaWhlosI4wC3gaOTlQhGNRf_2E0FM2gnsNaQuHv3TPIhU9EM-tkfV3ecdB5mc5wXURD6Ify9NPOHmjPRW4mJPL_v_5MoLPanUz4T9MZPO9yTF4oDHZ9AZKhXRggeACjyktVtahRgIY6Sqvq-FxGe5X-_lRaiW5VUnYU9VK7OMteloDYOWvDS1Z3PDdV0KiP-zxd5H2sG688Sxy1U_2-DVexznnyRzBlgVootfbhP3hHpXWh6Ud_rnpTIywe-ZDMN-x717YhKiruIP3yvws65mrPRSXY_rrpKIjXjjWblbaaFjalaJmKAYX6JhZp4tQj42Bhac1mckaYw_2wqPOpncAhADxI98r-KMkN0ZJzfNuKGT3bkeaeejdugrwWJDrLTUvqQwF5uFeSCccHbFzNXCzvZwYymZvpK5zNhofldhzcH-VA6twcjWp49SVHzvwwPEDjgu0F0MfGfZnS_FGgS_s2GfRQCYS5JNXA-b3qc0b1lr68pzcxPgJqzgCSHJ_h6ac2U_us1kDwKOfs3cXnvzvMSzIi8Lkex0w2DjodKNUV8HCauYYhOns2LxbnLQuS8ThlbYm9WR9BhVpqvv8FfzdakZJQZnm5X8r7yDM7lYYVw1Elc3zmpjOX67-DmAO5JjNZ5_Dhn1vLWlG82mV8RCdyXZtZrAkbNIlSRQU5h4WBfzC546zYzKpo34z9EvlHyAcUvs6ukAui82sqt1h16JBp_LLIZrm4tq2FAYtvqec98K5q0KXsSgpbZYws2QTNBJurhQvTtUlAIcJ_o08gVS-6QZF7Q1ZRJLYfPqnMSJK8mx91v77Gb_oVLEuZc5FPaTu4ZoIM5T0YvO7E3JQ--5CSrst4xDwijNDrHVJIfO4fF29Ugm30s7_aAfKGs2eZUpPkMAKNFJ7glrRv9MYk0r7qnK5aOLRUCQdygkZ7nOv8VG5qFHfwM-C8pyvwyHC4bD-JjDgxXyP50iYUMXgKtzbZn9Jz3dzKaF_VY0AxURXyVHDmCYTrYIpQ1bz3yFf8hhXpQYm_DXBKBIBIDPRgyQPV9RFWk2XXYq7kzd6fiCoqYO916c98gHrvHDusLzX4r1aWDWsA" style="width: 100%;">
                        <p class="caption">CLIP, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 아래는 좀 더 자세한 학습 코드입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOaI3-qPlIo9X78EpCM6MO6uN01HVmrAOTTWroWgKBXXXAzcczl4wj2B6-jlvNwvWIQpxRuQoHEisGZUUDA02b7uarTCEgV00hcstcBFqRRw1yWHrSTRloISpTewaj1v5rAHWjJU-eHdLs5OcyLYhh2YBGWMwPjK15ODaQSlaGJfGhAiIwnQG0_9QR8MY8-vAjvaDyKKmXwcYhRhpLkEcbUulhkb_gmx7Y3wQSjhJmiLWPxjwTpUkDviUtEBoXmMps5cweshd9GdoRQJ0CRef9KNKeI9xvGWK2F_udZcVAnXfUn_zSaEvKC53cVEw-RlDzI_HCOmpoOWkNRtdWd4CQ0rU6jE_MSrsXJPbdh7NKzBXE5O8WFTB_SXkc2u9XPPPHLkeri417n1oBczQfitHz2UYhzOYHI26x1MRMhBhxYKeTXKQ6DyBjceryvs1Rsn33z0sFv2K1qp2L-oldPHE6YM6_uFAToYj-1e-O2qobe1Yw6v_tFz82r1IaUQTh_gjgCkcsrhrIHJLyLlyFzy6ffVZ_XS68pdAUI1g4wY8ba_fEo7B_e7FrebuPEHGkGqosjE0idvJai_VuiXi_sKMWekQtlMmcVnEe-fYiNyJXlAel5pIZK0OhbRqd2K0pf4P6S1itatFuj7KOyRyv0HrMJ-6P01UsuUOjnOFVIeKvTTigQ2JxP6lvSbNrNvWsFGiTL2zRs6-vZHZ_fburCUMrFkn8NcFfJOgES9AP_8zmB9GLrbqEnR640AUQKjgHmml7VntUsAHwBU3_3XGvIkzQtxeSJNgFfuVB_itPtRDNg2vuyXPPtBEic8lVXQRTBI4b2mGWoV9-sVriCuYj_lcx9GzOfVLrHa7BbLOKDCILgVwTqSbbSA0-n28dQ4hBCqtekWGAFjQCxy4HjbRGo7ChgPOD7-FSUd4bnxYkq55cN5LgsKtXaDlSBgPgwY7txvFELnNGQrh6uZanpiy3TdtFAbEtbpugI0zBFkEaPRTk2Zks8XVYS3SaJAkR7PxuUPwf_dYYjp1u-Yp9uFznOmelSiV8tJOx2o15YDDZPN4rggI4ccye4dKxssS4yfBB1MYAj80K_wvalAaSyVU5JIMenh6tqNk04amhLZdlT78P0GUWsUIFNdC9eyw8MNrDlAREIRusdicKeoRNJex3KggplPXIMSq_ggWoOsNAllBiZylwo_Ch0FFJuPUWUu50ABYlLQ3RMuOmsonWiUaoZgF_pprb9UOgaxzI9hZd8RKm9Cc67dArOgfM6VuI5QnlkFmIr9Zb0C08Ur4Sv1ER5CSDYO_AlleOZWPMFqbSeSqHCODBH15H-R34oTA5VcIN2sXBfLOZXwr6_tYVRZTOw5Kpx2gFqyT1xh0UCBY6yxnyVcAdRPdhWHQ1VvhLdox2cFAsYxKkLBYe5_GAJZyfNJAUtGQarNKEj6-6vy0WdCNe0kwqNZCvufM36AOzRutqRWmfk6SsBYFWZenbaRdA1qtxY" style="width: 80%;">
                        <p class="caption">CLIP 학습 코드, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>먼저 이미지와 텍스트의 인코더로 어떤 모델을 사용했는지는 좀 더 아래에서 살펴보고 어떻게 학습 되는지 살펴보겠습니다.
                        <ol>
                            <li>이미지와 텍스트에서 인코딩된 feature들을 추출.</li>
                            <li>각각의 feature에 가중치를 행렬곱 해준 후, l2 normalization을 수행.</li>
                            <li>이미지, 텍스트 feature를 dot product를 수행하고 학습 파라미터 \(e^{\tau}\)를 곱해줌.</li>
                            <li>그리고 모델 그림에서 보았던 곱한 \(n*n\)의 dot product 수행 결과의 diagonal 부분만 1에 가깝게, 나머지는 0에 가깝게 학습.</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">이러한 방법은 같은 쌍의 데이터의 dot product score(코사인 유사도)는 높게 나머지는 낮게 학습하려는 과정인 것입니다(이러한 과정에서 이미지의 label은 필요 없음을 알 수 있습니다).</span>

                        <br><br>저자들은 이미지와 텍스트 인코더로 각각 아래의 모델을 선택합니다.
                        <ul>
                            <li>Image Encoder</li>
                            <ol>
                                <li><b>ResNet-D</b>: 기존 ResNet 모델의 global average pooling을 attention pooling(QKV 연산을 진행)으로 바꾼 모델.</li>
                                <li><b>ViT</b>: 기존 ViT에 layer norm 추가한 모델.</li>
                            </ol>
                            <li>Text Encoder: <b>Transformer(L: 76 ([EOS] 포함 77))</b></li>
                        </ul>
                    </p>


                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>CLIP 훈련</span><br>
                        <span>CLIP Training</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <br>실험에 사용한 모델은 아래와 같습니다.
                        <ul>
                            <li><b>Image Encoder</b>: ResNet50, ResNet101, ResNet50의 4, 16, 64배 연산량에 해당하는 EfficientNet-style 모델(총 5가지 모델) + ViT-B/32, ViT-B/16, ViT-L/14(총 3가지 모델)</li>
                            <li><b>Text Encoder</b>: Transformer</li>
                        </ul>
                        그리고 아래 결과에서 후술하는 CLIP 모델은 모두 가장 좋은 성능을 냈던 가장 큰 모델
                    </p>




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>CLIP 결과</span><br>
                        <span>CLIP Results</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>1. Zero-shot Transfer</b></span>
                        <br>저자들은 모델이 representation을 잘 학습했는지 확인하기 위해서 zero-shot task를 수행합니다.
                        Zero-shot task 수행은 아래와 같이 분류 task를 수행하는 과정입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOY2nkxVgK0-OoLyzwf8ctbYHStQ93be3P6jYM_kU9lKqf0sdPbRFBuMczvxyGt_xd3p42zSFVG6kZam_66yXNvay4HrxPyOQ4tqOEs6-sq5_6amR-ZmPBgyg25VgkrH-fZfOXiHUYBCJkBXFJ9bF-axIBAQl_tWpB7K_NLnR6Nyh6fyOY5cb5t7g3M0TXmEm7_kZJIqpMqEuuW1p5xU5OMZfXQ5FyLgz2TiK5pKoWIOGdVJTM9Xh7AcyrO279ypygcNq7MGG87uAfLNCUxUG4bNPFGqS4qP3HKoFtE6SSx0DX6lcGVXztVxE-8thmR5Un7RSob2XlaH0yU2W5gZcuTF-lCOHfUZHcmEhLV5Wa6hJszU2K_UFPTSuEo76F56aP64xrGw1laUJ8BLIZm2RA5pUhkjdlY5tBBuSLEjTuFicQKOV0SfkDALZR2Vn0rAi5bt6UWmD2ROogQtOtdHfp2vId5pGeKdsQX5IXXBG5JU54tkwH1I1bXXAOR5cEbQdTxGAXuPiOYDE0p4mfTg7MWH_U3C6EjgNqfAooIVK6gCUVbC3Z9rtKFjVm3MDBnclHMqsgpbwKkLYKt039qdFhIh172nqdYbyAU9XH2V7pNP9C7-bYAiBxShjcLvM9oBv0nDjOjATxt1ETbNryyWMG2wEapb24c9W1Qzjh_BCwJRP2lrieOCyGz3Mv0RtZbluglDR-KpK5KzsRkPGCfTEjNvnoMGFn6XnTVj3ADcUiVJAogy8M32datUmigSPkRsZWJfuQZLOGU6-6zcOAWLRWX5qtyVTAvzJT3PLkmLai-h_MAgXcDto-d49626hNZiM61NIGvY0ksowmOeLPesipr9YfFGXDPg8icqT3i3l5EvhlJBT7vJfOLp6Cmg6kC5jxIcDHk7mqVw17OzsidE9zbOiLJdvSZ-8546IZL9gHXqMG33CJwGSdVne623eXEeBZsjHCpbe5DVO2BMUWGS7MDw5zyHrSB2kmCAGurt4cvgHggochZx1Ez_tMuDDbE7xfTZSVr-tydlgsHi-vo-ClO3wKpMHmdkYC4VacVE_Fh-eg2B4oVijz5SeiDjv2oXxOCGvBG0_pw82ck2JXfCrvqAaXLqhKIIAYm5A0zwir_ccaTg6xHu2cWu21ODZLVak0S0ZY0fpKKMwcMAd5-14gkMJqZzkOU4HPzIzkC43ML06b6Bygg1-bhyIQnk9drnWoZ4sVcu7qtzj64h1kehbE3rjCxuGX9oCP-0PJsNpCkkJv-Lv0Wp-3pzINzb_4CaQZHYLap7G5JPCAFfubArEoc2fsHgl4O1ZHt8668BuDqPxh3lXxtobPzMrLVyjdx9vjkNvfAnWnfO1m4S1NLQNkiUuxloGIGTC6lEUsNSu6SRXIegd6HYahpxvKI__puh84MGc8YtkPBj8k7xKTAB1eNeafok0jorqITITB8lOxr9L_ZlLX-jzdpLZZ1MciFU1uPL03zLHQ6za0OsMWx0rRU" style="width: 100%;">
                        <p class="caption">CLIP zero-shot task, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">CLIP은 분류 모델이 아니며, zero-shot이기 때문에 한 번도 모델이 참조한적 없는 데이터를 이용해야 합니다.
                        이 과정을 수행하기 위해 CLIP을 이미지와 더불어 class를 문장(프롬프트)으로 넣어주는 방법을 채택한 것이죠.</span>
                        그리고 이 task를 27개의 데이터셋에서 수행한 결과 baseline 모델인 ImageNet pre-trained ResNet-50의 linear probe(pre-trained ResNet-50 이미지를 분류하는 top layer만 학습시킨 모델) 결과에 비해 16개의 데이터셋에 대해 더 좋은 결과를 보였으며 STL10 데이터셋에 대해서는 SOTA를 달성하기도 했습니다.
                        하지만 일반적인 분류 이미지 데이터가 아닌(e.g. EuroSAT 등은 인공위성 이미지 분류 데이터) 데이터셋에 대해서는 baseline보다 성능이 떨어졌으며, 이는 어찌보면 당연한 결과일 것입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOZtKS1hM1yTq3dyqNAfb0A-nBX3nnBSYBoAzEmg_4CeWLM-zOXpEyS-cavauvjP3vtdqdEeqvFfutikFcr9G6bKAsC1aVHmBvhdyM1zXRzZbAYSr0NAlkKd9mkRKSh-VqwmzGZu-xiLk4MJ11S3y_OTh2qkd_ZvEbY_cD0BKZXH0sk59zxqo2JQR6OtUdGKVP5EMftrjJPocUmo0xn_PLCui8FH72YciiSC6ta7eZPHHNF4FKQz93aY0tLay02F-YjR1eeirUkS6KRvbUTCSCAolJsURH9iOqdCQUTRHYRCmqhYefaLV6nr7qwDmDlTMFmsj2zGhKfrNFPK9zn0PVS5Iae9Y_aElettYkNwJELr4WUUP540fpiyYHPDy35cWH2_z-2yUI1Nft6IJ-9i5d_-mJoYsomftseZb0m66eoFOszGZHyS4c98hzDsmhW658QGyosUBfJvyEwWjkXDZPrJxYXRnjLvOOChuAgdemyOoA5-iC86UOn2UfLhwvn9Wx7IMBsDbgq96eEUpdf_YvfL021sc5FS8v8QuGNaPK0gm-Ios1039MAhD0eaqUk5PjLFyJLC23V7O-gn3NyxQqhBHojmUjRF24Ufz-cKKQRggXLItEPKi_l5_z3flhbVBg-j7BkNubw2qiK5gTlj1VaU_QfF-IkQ7VLgXLMbQ9ZtC7OKKOLeS-TOx3IhWrJjNTbMawkpgHuEi3k1DqTqS28lh0veuVQC62CChmHDSeFhgmt-ZT0sNy8jFYzvoh6mJ3hOYT7d52MNe9b2FI7imNLsYHs_crEUkTTzY4Ua1bOAfrD3L18Hxg2gAxLalET_gY6mjqvXTQY2ytM0FHB7A4Ss0ZRKB7gHPicUa9cs8P1ilOxhS_IuhBx-qMqpYiizyyXU0w9XxBBHCrfSafJxfWizIde217Gmo5Z0fkJ4WVMjgiyjP5jygBRvMqDw0FTAZWv5OVmNsFmEjGdbBlWu9FvOpU4Hjqj4YTKCbAKZ_YynXGWK-MgnJvj8bbzsK2P_i-5vDWXyyud1Bp2UTce3CBeb8NZPlmzlB7tlrBjUe-YV3hQz4kUMJsp-B6kdpZLc3MDnXaEup77hZDCJf8dO3ITc5I11JfGmDBe0-rO-b1zdwc6CIYZGeYZUOznMNl_Ll1WzKJnb-NSX8JNKzhcMh6oNqzdKFEKb0_Tcq_aU8PcN20R1NVddxBqiy9HtrMJB2qSDWjbk_KnL8g_r3G5D77AkSNigqZWjILI2eDv-1jGDoIJtdcVyeyV7bKoEDPHxYS10peMGYpzOqjV0B1u7OwpK8y6TE3PYaFT5K_LScMn04HOofB5p74FS7RfNLKCI58rRxL4icz832OsiUalJ6bf0n-0_hSj4S-rzKBObIACF1NeGI84EvvxEdsE8JheiyjrNz7gkZ654gN2ol57N36O2Za0ItB683tkAmqCF8RaStgtUy15VqC7eAmFqvuzMkX0-cH54xHwU8-DE6hSc-Q0" style="width: 80%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 놀랍게도 CLIP은 다른 모델들의 few-shot 결과보다 좋았으며, CLILP의 zero-shot 평균 성능은 4-shot 일때이며, 이는 다른 baseline 모델의 16-shot의 결과와 비슷한 수준으로 아주 뛰어난 성능을 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOb18gwWECP0zgo-610nUQVmwt3iTCBwNRQ2xWp9KYpNi9BDCLyODv6pW5t84UCyhcnGJzJlqCeZIWXi-_jCom0yHJucNAsmMeRSLCLl7IBBXO5IbugXtqtpDCNRT6gUD62tEsQzCO4oEd7DU_TECnryZyE3ZIW07WIN6r-GvxcAASBN7PXfxCI-9mdbXx_SD1N0obHsOps6hanqd3aswcfQQUaNCtPnOtbZjqEo_70pY43WdkvA4ffnPjQnrpooiavPJzY9yIhcdAN2LjH3rH7BoU9B72YiVtqFnAwMG6ETARUFsDBdQDe6bRTHDkaMzNwHEiO3alyKcKDeGjLoh6er5AgXT9d8mj-9upz73PAUN0V9CB_O_542RJ_PGR8E9C5QtFlssY9tzG7Ll9fx2VDJyT7aFmRN_JTYY5CuZrTBWj3_X3ZM4_BPZmmIUIhjNCXbJvVuqoQPGCiN_nYJ2CXJJanqFdwTdAmkITYGTC4A1qRHgGK6T1cXPQWmY5hWd5etKCrbGC8GT_RRYcMJEzZyS9iCFRFS54-bSUqV7swWwaxNhsEQmC49VcwNodIxFwQFWy5xomDptk5XLluDpwU5IWOs-GOj7_0adfhkdUrG48aG6kdC4mPQg-K8n8OD3fZ4C_5AcDMeR1l51O4hvxWsxFA3QfaG5IQdcrrxBxidYOhbBB_4_SzywntXoo_AkxLZsCUg7ag6bSr2XUoc-4t4D7TTyT6bxvbzKzwiR-ytwklOCI64V0uoDVTueYBD3jTgJxVnZeKJdgwkDptTygKBoJxNTM-zZ6dxfRXgqvpgEpKkgasdy48BqjXdRJFPm7ZhOG2TECxzzIe8lOG8CsorIGuA0f3ejgjRoZCxWL5PX_wXRJoHAJ2kJHLHen_S1eRPP6oylYCvQxaMAMZouXt7MSiC4gVO-pd_VA1WtNRRSZeiVjahO_tl1kEktkTNyzgWBP6WbJG8AEPtSdipvmtrhtM1QCIYcZVRzoGc1ykPGgFrEVYJqsxtyGX5fJ43ZXj3RVGttdCnRGLuIztiLnaXaaj360dpGXP4UTZMZO5M52e6U8VDqi54rkjFJKW8owfcgfu484YaXJn6WTTCgwKGcZcy5w8kUh1zXx1KOOuiPM_6zitOhLllzZOitCKIaThqgHa0T3DSAvZliGAY4t8uhRm8g-k4CNLIS9Nhly5mYlMegpAoegcabZEdgHWN3_p2GqCXFZtc48xY4F8q_e47JTZXjef9zMMt_U3b6IEGUuYRYTS01Tg4g-PfTdFauGW3wb1aUB85ubhtVBEqPfsRt7aXqZP8z0DahQTVM6hJ9bNXH-t9MrJX95EB7CAfcX76MshTj46S3srl1a89D3P9ITw5b6UVEl5isVglHcc7eEypAE0sCjctF4NiODIw2RWCAfDuJhlk439BGYB3f77PXlix9Mi-YkSs_JTtMk0tyPfft9PiKfwig3BjYcgJm4ZtL5PbUmaTPGXnVpzU9fU" style="width: 80%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>또한 Visual N-Grams 모델보다도 zero-shot 분류 성능이 더 좋았지만, Visual N-Grams 연구가 나오기 전에는 transformer 모델도 없었으며 학습 데이터 양또한 CLIP이 많다는 부분 등을 언급하면서 이 모델과의 비교가 완전히 공평한 조건에서 이루어지지 않았다는 부분을 언급합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 요즘 ChatGPT에서 프롬프트를 이용하여 다양한 task를 수행할 수 있는 것처럼, 당시 논문에서도 일찌감치 프롬프트를 이용하여 zero-shot 성능을 더 끌어올릴 수 있고 다양하게 적용될 수 있다고 말합니다.</span>
                        아래 결과는 실제로 프롬프트만 바꿔줬을 때, 4배 더 많은 연산을 한 모델과 같은 성능을 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EObM_yYhhueUnUqtVDlvVvlQLp3DTVzytfTqC8a4aeblPfi3T0IEVdmz_mcs92_ryfFRHO0y1MjFeTvJDk5koHpCEBBR4PkF4cl_x7a07D71raYxJu5GOPB_DZVZ3k7bvdgAzVfyzVuDFo50EqymrSQSt7zVj8b1I3ehNqQ8iLV0oGXNzIiXUoKQUKeKKAb38p9z2OpqZjkThVpdOGwosOgDJCwXajk8N_hPjG9d1I4NMB8Lh3covoC-1VneRT05ZR8OJ-MSz8_WCHwxougl4eluvEIUNe5bxwb__95qoNNt-aOF1irgIjV4XUafW82ujiTVjn0rQVK2I8cMJ3hUwsrHUnh81-AVgfkrJTOX9zy30rIDwR2156X0iTUD4LP41M4lkmD4gUHUHG4wxWk57INPgezYZ7JZ6UmNvVXe7JTakVPAE46C76crLft3TLfXtXK2J2wx0GBSM_H2BLj52rZcQNd5jF7grRcBEwT0FPzfT7rBf-ftBtIueZvMxQJRyg3f6dDYTysLnTIE1JrnBcYMJauVzub_jTn80BSOPF-w_C3Tje9JsjtExYnZOhOwq00_OPrKUvuHbeE5DWoLArHFIp9Q5nxudgQfS4EFVRBq3Yp8O7PUwCbkT-g9gNavL3n5uURH6dsM-VqDItx7YiEU6EE2KBrjFgg8xAXWxzDHc2aLqjj9Iev7YmbqNI2FBqUpy4_FkQz5DeVCc3g7uwBPW-J_TvfVU1hJZ3-wIHPqD72-kXrt47K5iHUxHGO3vT5PjXd16Bi0Dv35LMil17yF0LKpu9Xx6isECmCz1NhSyNKAWZMsUpDBJbImYa5-P-tO3D9mkQo9Rzmv1MHWT_uCxFJZPsUECmU9XsxPOSSyYkQyOd1dtvRb2HK_sS-3W0vKdsarn4Afkh2xuCrebu_U1aI5Wm_kslY2xYJO_I5V4NcpRXQowbtX8rZNDQesDlA6hBSMLbgXB2PLVemHC_gF0olyvhcM_3Y7XRPRo7dFAytzjwOzxhN9KjrjAbCn-tZXU2sbxzfIH5gLInzY7zwqbe2r7zCK-a3YbOd8oMel8DgsCWi31r-0L7LhC39WYd9YCHVEsq2Xrx0toJPep0wjSQSbBy-1kMULAwjGU1FPbXLXH7ks_i3iJSsiQYhFo75GZC9jRMmmRtTWS44THbA7_wXGzdQpq6b5RBsXPMJTAT07hO48WquuEpO-JsHFWxp_7D0VAjnY4kv-JwBWYfxoJC4EQ0Cx2v-s-DvyaCyyu2fFfSe2rZr3HUsQUXBBoTdNlreu9_1Redl8n2iSRfqtZ4SjzW0wwYxGFtKuQIpoLNRyocPhDC9vnvgdA6jTV24rMoX-rg0OTgsID2AMsvA9IUqgSK__tYb6LhNJBu-znW7mqfjGFibBsYNfhOsqg6V-UOqmrf9nou9qrT8H6m5R1bXuj0OrvQ4O2F7oBvPo1zKQCFhIGrAmAwte_s15b1YmfHwyFAm2n1JNghD0aN0" style="width: 80%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>실제로 zero-shot의 결과를 보면 상당히 괜찮은 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EObOQkxvTj-KOfn1isKILUzOl_KM8oLJGOoMwgJTdbdebW-R2Cs2_mO3Io3Ieh1dgX34Ge10pCEqNp9e2dHWX7UZ0jMUNXcfDACs_yKb6ot9D3sNlWscF8dEJJjg5CsETiOerticPNVlC1JAT7_qVgwYAnyBxMu200ATJ7z-hOX52NoQG28Fiik-Es4JErmwwtCRlO1oMC3Bg3g5hPsmd3qNjC3a-8z_H_AQDGxp1bDUUpZqD__NCn0o_RXZfgh_xcZOuDrkkyaDlYru0ovg7hoSgOZkZ1TWIpodK0fXupgisNLtwpbokH8Aqequ72Wlj5-ddSz7sW88N0iTLoZdpJC_Wi5i4xPB3no2lpoabGXwUXp66qfvryT38BADpIgttZosskFoczzdZrzoO0AS8qz26JPiMMJfmEs_WjMjyEi1ONd22SEA9Kk0ndlfEC_xdHdKCxar5WnAWwTxQTeOZFx1GxwLKdMM5s6fdb1LIJE7MfQWSE9RCrspiWbVYY8K8_3XQ9HPSLJKoLNJi5ozFQB8iURNqtczPCGMC7ir2xjAAC00wVl0NTm5YIhCsX4DCatcV9oxJiURf_LzVjJpWuDoW9cTQhqKRVTvydF0MprCKYwO6zVElP2miUzF_aOavc2ZEqfXa4wCVC-hjO3znsK82iEQIBClH3u4E_tjyQuxSphf0QrvbEYSpdnvZoi42VQ-610rYLlh7SnOAptCRJulJ2pUNEx7h7PMS6E8yudwqljncXtHJWDNvJAzjH15fu8g1GO8nCr4GJlt-IlEQlWapt3m_f6vK1DnOQhQs4OuPnxnxE61aU0vwrMEh-JCLYvwBiBJolEB1FMSeMqpj5fuRTru4UgiVYUTyEwZ2olEDeOEgqGU0Yk7UqMv_JNT4hxWrRgFz7ASXuq5iwIPYSEJ78ATf1wP7uBh6c7b2b_HWBreoIPJEBbqljS54rPhL-Rove4dbyxHsoB85vsnNwPZCuNumq8LnRMUpJGlPoqOdu1lIGBfCgBTMyBemA61Q88ns49WSQKb0bUjF9NWT4PbN6V_x4nagJr5QpE8LiPycovtX3pyD0UkVpWuvg7_wKlodlavGa46fSG5NFEhnGzqQIOjyBCIRLts8zykHCti42G8PuHfnW379wECLlujWmVhEI6E-1wvWkYCGvYCmp3-u7L6BLQ-5aL1KlaWIAFkm8NQ-BeKsemOet7iN8pIihbhuWSG-C17juNQU7qtt5jotbXaSjOwdXSc2YpI20TnB0unZM75fJvmyfJAM6B-H4hGKDcD_fYEP9wpbKFcCxP_70os1JVXpyeDdmpmt9CDsmQ-V4aiSQK2MPPeccEruXsIzhUt0Bs5fPLk5yrL1-RhnzxF_1fkfxkg5S03b_IFYxNWrW9jrV6gVeZSbOKNBq6ocU6dM5TsAyxeMCQJubPd5iEUQZidJs1iZKuR2jqsxSSjrvPQs4oIhaWiY21gjU6LjJxylkG4HPME-pRct3Y" style="width: 100%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>2. Representation</b></span>
                        <br>CLIP은 궁극적으로 feature representation을 위한 모델이기 때문에 저자들은 feature representation 결과를 보여줍니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">먼저 CLIP을 포함하여 baseline 모델에서 나온 feature를 바탕으로 간단한 logistic regression 모델을 통해 이미지 class를 분류하는 방식으로 실험을 진행합니다.</span>
                        그 결과 ViT를 사용한 CLIP의 결과가 가장 좋게 나오는 것을 확인할 수 있으며 SOTA를 달성하였습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOaAujbcQQ-w6TOBbPV8-fivg7KD2qlN1SCsl42zdu5SqfFcBuK9pYrev7umHUUVbpPoufn5th7OB7CAhW7CTGD6oSvPJmizWimjeA0z3TwwoMFvSUB6XV6SVLdvoERRg8J-L8evc1zOjeaVFvQVLjkQUiy9t4tVI_WdzWvh4tWqjhvnF6J1_lBn33zHlF6wuateyIFE8KkI8XtqE1TkvWe3jresHRV-SbT18n4Ha1-beckOt9fhKAYzttr2lbGWwPALbx8ev8ngiZeyqCdPkw0abaj4LZh4iNbZ5r1rYDF1_wx6imlbvH8QKOg2voR0JvUlOs8IgHKJGtgtPfLQZ3HMiWAqxmF7AoimgztG5dqB_z8dZqpXfb0yyNIUsOkvhl-i1RaczBv9Din41B1eorNQT2QdhlgtHzsJYkWt2f_Cru-ECAIH3LCPX1FknFRmRt4a6wTqWHSNCBEtTjrcAzoglTRbpflkHAdFHSxrIk5THuWtcR0n8SISnUOQTp-WzUcuHp5l5sQd6FyKIe-BoC6pTnOsoIhVXgz7Z8D08TH2kmz_K7u82TwXYH3_4Ea7FBUpc3N8_8LEooKrtRBM_JNjHGQ4rGVpBTNTF7PdnezPGrdIIJgmzxsZBQd-F6i0gM2KVnWyoFjCDNF34gkOYlgGBMgGwgDEd8KyaT2b3y-GVNXa2dGun9C1e-tZG84iMKbPmCfNQL4hDpkzxBMEBX3DnrLsnhBo7di5h8DsiXCNLS-RF4o_oNpZrqcpVc-8ehxtOSe0Gbrl0TG6XoFkWnn2bHvzLgajISSvY7Meuz2BAUSMlQtVP_EycbuYG9nK_naCc1GARyItM4D3ehUNN_0bmYBFH8y4pkKef2Tom_IJow16Xh5q2Rq8-2iEMogRnplrUMB5PPt-Sj4VXAzMJyxWln9igEw7r1Oa3YzNNXfJmZtwikk7Kj7A1oodLmgCPNwVPZPg2y9QezR1cOUdEI8kAt5JJHQdtlFf6s3q8qgo-zRqfvEaL_lDZ02UN6QhmNwo6J-IsqFy5v6LjwfkR09CN0xU6kSwYF1fujO9N0hruMTW2JxvJBsI4Yzcw67EahTz0nUTYRr2EGJ1_bBvHhJVN-1G4GTaNCYxlvkQ_3Ebuc0I6uKxmSxtTG3C3xUthZWquGNdSa_EDLGM2_P6oSqLQ58B7jkAfDLz50n7USA9XP1qN2oic67dGLzy5uvccqYU6wkFwCLnzWBBNaY6js8MaZsnokfCoXHfnxa9_AQR-OEGF1A1kWSDvkEcugOxerJ1xZnHUBgg4xrKztX2xexeN0DtxCl9wAmi7SxUyy5elLZaX_K0FBE20CvKG8brSn6PunhZcNwU-8MpM8dwEVpPccvtHzbMARmW18w4GYo3zvRh51Nv1u-fzJ0OsMN-uQC1d474gOpEmXmpeIakGOe-rBh0U5krEciiOG9M48IWIWlOT50rZJduDXfLNWhHEtVsd-UohxyxB5ktJao82jo" style="width: 100%;">
                        <p class="caption">CLIP feature representation 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 CLIP의 feature를 바탕으로 27개의 데이터셋에 대해서 분류한 결과 EfficiNet의 결과보다 대부분의 데이터셋에서 더 좋은 성능을 보여준다는 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EObjOOY864EfB7dp4utCKiC69CtNggDhPP77LIxidGfDQfGR26TJYsjspkgg35mCTdo_USA3C68NfRgl27zfVGvLg4RDldW9nj4XjdaIjf6rzISviNbbEmMxVAhptWltN5eN1edOFQvlmHHQybR-JYBhuxk238O3R-aKRYKI9kO8dKy0IbW977BqG3hQWYWWSSiMWdLEcMDgdOAqSmYuBk5ZbtiG1b05oSFBzh2BJv26T474N0n7vWjflkyodmhGcVDOTCuUZlj5sJxayDpsHKqlRZo8ZldeTX4ZTIGKvzetTRUsHsyzLR9KB_DIc2gI-Sxr8tpEp2Xs4UH32J93tEjBdVc2WCl3UrmIsKE3CwnFYhKEbY-ULyJSECAhS-2pVYNotH4Zy3YD0j9umeahkUSe0GeEJWth_fvfbfzbg3CsmuviiQLxRe0wMMG9k6U_6hn3_Vwhg_2eOrFvWMw_RRFfI0HB0BGLS0fLbY_Stogu70MICA2QVyEEcSDGhkOglImA-5mn-qKqJJSlfsup3UMT96nFxHUXn0BLZoLoElb4IjMKbnbDvT-AoexbpegmjNX75ztIxutflcv1g7HtqyKc6KeiCs4e6WnLZRfVPmrsR2ngcSGA6vEsBnrDx902ydzh7xVlDliN45qfOLrI_itXw1ZehMsORojBSd-BdItxHpJP9jtcGcnGxgNY3D25lBkuM5N731KnAPVrgi-64RCXmYED7Xue82j4PG_VvuMCYCVDoaWJQw-bag58WOxU8y8Imai4IUI8xnHqLcp_vsiRmf1TaMDHbgs3CUB6YQYFuBlWA9KPKhmaf6VMH5jfcRrBRNu5w8d_7s-11mwaszAo6k1CfjRO-VDbNdgDvOyek0kF9vTvfgWox3y355ebAPRJkJWuk4e34P1kkUqxSYoYmNeASzFI0gKyU9wKDZ9YuPQ63y2INc6OMonccJgcEpk8ti8GZ8emOCxhQv_7wCssQMEhiNIcn0DTujABD9KjgTzDTHBjHCurL71RxiBg2K16Plvrth--BmfT_4DJmUQa35OFPFzWvSde7j6hGzHrqaT6HnUV_DIuAl8Wn2kOIDhPxHlK1xhk9NMU1iTbyizd-ucVwLnejjbIqvR-6byL3V8y0yjWesbwmx3Zao17r7gxDgTP8l7n5j7Hk5Y4YAHlzmEj85C0nLsrREjHvNRQXRMpb-3HUAT1ixtNWNF8umB0IPHI2QzQCQKeqg0HqBymRIuQfYlgQSF69M2ezpCwtNkREEt-cDTWjgABNP8cBB6FE3Wg0ZhgV6gIaBf7-5KPHlAvNOosru4Dbp4t8M9DTfy7f7LbKoNlDYDgQuL-AaksS_wbnd1opQY5vCuVXmUaxXmFiFzze-MYeysVeYWAQfviJJQ2hMRjsK8bekERX1pUxp-VLE1MAAfjBldklLa8PeHILU5oezlV87_2t4tmJZRByFRfuqnfXU1SG1hnejdPkXZFIL1VVM7AVjslebg" style="width: 80%;">
                        <p class="caption">CLIP feature representation 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>3. Robustness</b></span>
                        <br>CLIP은 다른 모델들에 비해 그 결과가 robust하다고 주장합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림의 y축의 결과는 위의 Representation을 설명하는 처음 그림의 결과와 같습니다.
                        다만 x축을 ImageNet의 분류 결과로 바꾸었습니다.
                        CLIP을 제외한 다른 baseline 모델들은 ImageNet에서 pre-trained 된 모델이라 다른 데이터셋으로 평가하였을 때 점선 아래, 즉 ImageNet 결과보다 낮은 결과를 보여줍니다.
                        반면에 CLIP은 데이터셋의 상관 없이 똑같이 높은 결과를 보여줍니다. 이는 CLIP이 robust하게 학습이 되었다는 것을 방증합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOZd5NrQyz3xLnarCnVT6W7_4ElO88rKET4-xoowwTxWXP8DVdC5wl2Ww8Yr3P8IaJteht6-vcGQsDa8cxhTpW5yk_boGIWVguiKcQoZdBXaxsCiAXihbyyCZJxz8duYinz-s5QARr2YiPPFKsvNECYLqa1_euvwaKS6fHXdTW8JIRM2FeK0DJYs71yTur74X68pJFmMV2BtPi67hp7wyGGWWZ0pQyiV2PH2907VaWGM06qXjgc2_bo8V4iYoEjw1NF8BHdd8Zt2MM_Vq2jBh8LhkAOa1PAdkNAeohQ6OlAYCAiIA_lIVHK951VLzYo9SMcAsi_COpPxZyjD_OmVwC7v2CDrAih_xELWeS_L0NmwlPQfbvQSAxqGZlfzzqdJ3cZ-g3BwlrjhZuIo8LHqbkvF4GVXCkuX8ZSsOIO7JIfFpH3WVavHe3a83qTKz6bCoL1Al_LswG6q5luLaMlA3VxkT-CUbUWcuKIfQv5EUZ-XDwvn4DkL6rd31HMrPhEd0JQdx4UKoiK_XTmzrIZrrQ8hXxk4PRQMH_oaHO_FKf_CUL8MkjGislpVlYqeG8AY0HNeZUk3-mQiPun5N70-KYrddmjcNiPuWWjLbadBFkYPh6CTowo0tShF6X9cUglvNiBjLOtpVHn8w-W5PRi18VNv0_h7aLkcqBPKL1Gemg60-MfCJgpqtLN-zTzEOBjaOcmPkKhUHBtFcafxm118lTrqpDZUeKEoulT8eZC9Dzot9Bk7AOESNLEs0uDUU1PKHKj4rX2wcCN6w4dZb8QlkBKlJlTQGb8csq6wT9aJtnDuF_9ZZ19gNy6-vcz-doLnLdU-z0bCuDj9Ts8_slUM4fJ5Xe5guzKuRYMoNzfmU8pQdARlr9xzXhlCKkXY_-f3K9PBgSoY7moVsJ8yyKeQL7-DbyZw641WhLpmYSbwlw-4W6cZCdzwoiEdA73K1_FwQz3dzn8N8RxM_HRHSta0o_2gEQy9aaSF2H19MQn_biu4hAc_xvnIlp3gWTj-esc3Mv2s0VzgPkPoF8e_fwQzFOOhAiSZ73Zixi6Vxe2HEoMu2cjN1rsR6YlbFaQm8ZKdpRjiTrt___ZmvWkTNszy84auS0lhQrYy4-9UTAzmCu--XQQn_w0zvfMEV6hsm76Q_9fRa_3dwvoGZoIeDql6dr43pD6HXse73T6pJNpTUrFJ9pD3r8Ak1dydgKn3IBwi19VC7KbXgae_WWl4HjA6CFOW2onfdaEcT3sY-KS9g9pOFb-1EZBxyemRGCqqgz9t6jJxDcA7EWazyk-j76x2YNQThU44vP7JKBnAf8fBjAvjow0-7X0m5h7u1BNFa7lUSplP2v6-a6aUYGONy5FQRrsIipNHUjRGogtHnd1LZQ-oiod9bLNJ9n3YuG6bX4CGU89Evh1hsQMvwPzje_eNL7fdHwNwKO1Vw1cZnClVdl_g6dW0_9NNMimfrYniUmMiL6NJ0y4VNoHwluWJsUt79X8" style="width: 100%;">
                        <p class="caption">CLIP robustness 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 아래의 오른쪽 결과에서 보면 CLIP 모델은 ImageNet으로 학습한 ResNet101보다 다양한 데이터셋에서 더 좋은 결과를 보여줍니다.
                        그리고 왼쪽 결과에서 보면 CLIP모델은 이상적인 robust model과의 gap이 다른 모델에 비해 차이가 적은 것을 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOZF_VLTkUsZc4DSx5E85BOvTHbfzSMBkQIQKJONDuWgEXc09dfv5wOgIIAqmCZzOwX5urnVO19iyfhwik65TcMIHntxtwitqdCQkTVI5vI4QT9zTN7DeGcJXAMmokAfHrg366F8AfqIdFiw7EdpKqwnH5GMookHc8QfnH7E-KJBtr5_DBKdcxhoO-Ng-ZsNgA0joxRPIgpC2nH-apsiNpdEc1Ieoa8iQAzP12UOChuBkJdhP2ryWihmwnP2lGARQ3eIW8OLcfbR32EK7R4r3-5NtvWfrbgowgjQAHE5BRNZzlPaI40mhT92BETgE4z4k_dtWUlzjHiI9p7w6k_Y2sou-srZqCZPwP8IuqEN69ID2vnP0N6h8_F-ggCnxUTEVJVhhiC_Da1eZFiQbvfYo23N50T694jmZA281KsJccVH38iZwSUERUxqfHXA8LB5E0tLbl2eJXjSnKhkB_7IhlJJxaW6x1siCuhWW24N_6jH3eQGXUunSk0MEjKHFngfcOPLEa8h705ri384QCXh6zlTO4lCTMoDVDe88LqVMVphZqP8aCNl9mQk-pkiIjoi0tGD8aYtFaBRe_5GXzpOK_B6vx7D4iXspoVx6prd8tmO9lHa7hEbs4icHV4RwCUWyEeIvZVB2xhGkVEuKA3Mf2XZHGrK7Szm-gGMGNwuPPRlauggnQiTtPxNoxnXRJp993MBWdqh8tywmtzjEEwqX4dtH4_w5hul-sPfkw-X37sCQSPkNpLJvUOcHD8k1eXZ2Ppk-4YsAVqwPV6ty406bzhfYrYtv_Jp3ypfEOuZNdp_hleDrtP2L0xNSP7sJVuTDBrKn1v4w0ks_c-3AeluT1hSyqieaOf3igrAe7DoxH6mvlecT_os8GX3nz5MHA-wqTWFI-CLhDwf51gr5m3iiqzFqVCZaAZXzzAeVPtpYfKEVCU8x8YefLIM3RlkLkPk8ts3Ei2FvCVwBqCubYHu9sEXLqUoKPZtf1RcviRUrGy0YaUnRF6_uuOXmKt94hPexJTK31kHeKFQF4WJVP2P0pJjXRUZmOuDQbKi159ki0H8egnT-HsDsolseQdzl0xW4450KyVqEFPGn9f96UYzOZeaUAQGKcSwJHYqKdIkywVXVqq589C_vvR16Jq8SKc93BnPmYQy_D_2Tbui_uMtVrkEns0BheCpcy_NlPuT1BQ9SMSxJwYszqXBNlaUGClhDz24RDYI_49DfOleXbWp_owvBq7btJMJvcgqgNt0v7eNjJZ5OReQY3JMZj3-sGGGXrtGH8dg7KdRZnUEGULERdkMkmK1jRPzP3O87VyKnUNtQVN-XZBAHgilQ-noJSTVBvJE3UIayyKSNJeELTSirq1qExMl9mhTGyibrewGKrhJiS-NHgHKmaG_QsiqfyUjB3cHgYAeFSOaIBlBN3o7KqOg1YevcTso5zkC4iY6_eW4w_KBU_hZonhKYGImIHZGcZYs_Ztrn6n3keHc3cVFpO4" style="width: 100%;">
                        <p class="caption">CLIP robustness 결과, 출처: CLIP 논문</p>
                    </div>


                    
                    <p>
                        <br><br><br>CLIP은 OpenAI답게 4억개나 되는 image-text 쌍 데이터로 general representation을 학습한 모델입니다.
                        이 모델은 추후 다른 모델에서 많이 활용되었으며, 그 성능도 엄청났었습니다.
                        
                        <br><br>다음에는 image-text가 아닌 text-text 질의 응답 쌍 데이터를 통해 주어진 query에 대해 적절한 answer를 데이터에서 retrieve 해주는 text-clip을 제작한 프로젝트에 대해 이야기 해보겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#CLIP&emsp;#ContrastiveLearning
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('CLIP 첫 게시물 입니다.\n\nThis is the first post of CLIP.')" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('clip2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>Text-CLIP을 이용한 질의 응답 retrieval 모델 학습</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>