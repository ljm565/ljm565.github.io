<!DOCTYPE html>
<html>
    <head>
        <title>Transformer (Attention Is All You Need)</title>
        <meta name="description" content="Transformer에 대해 설명합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/transformer1.html" />
        <meta property="og:title" content="Transformer (Attention Is All You Need)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="Transformer에 대해 설명합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/ALs6j_GRFexHyBTyxARfgUVShQmI2LtnU4vt4kW-DAVjYlFfvGDk-BccMJFgqVsEG89ndIcLnPq64xZolwt2SqDAKw9Lx_4KY6bx19BVcRzh5RNAGwaDkES64na96yTD_xlUTLH3y5NmA8QiavCmMdGS_HgmCx20XYeHwbjs5wz9CphZSFOFkPKUTasvsqNEM-yOM2ssKx4GLGh6M5vFqL10yVRgvDT7-JkM-Aj7L2Dm6hmAjCIFkE-n0rKwHpIkx_oW4BREEEg8jmKe_ByfA5kJpfBE2OeMUbs9nzr_VOqZJzS-4nFxSLfZbxTDaZU6zY--ABUkZufsE4Q3-WqYyytXfD1fa6eNNKHRmHXhBAy7n22FGR7O9wDW7jLLM1WIlvH1FH45jKUJNjqm9NyA_b9KQiecKyZ1hPRPpbe6oEmbmGrj_iJF2UDt_71JbrhKjlpvXrtlWOJxd5BO3iFj743EuhpT91ufnbXlw_M6KvVb0xbnD3RaKacZZ7c0VGuAXhm6xvjh7XfCkwBPLIIfhltrd1fbYjTlodmnZGJrkCDp7C3tap0uCv9eQ-LgUVrdWKpuqm1yFxrgPEHWTZtWVEZE0i5Ap_qYGnOKS0pJgnWvseHYrxqoYKcqYD6DzMF-s8SAd1UhuaNrfnhPmVW1bXqrQ-Uk8RwLLx3q2NsSQ3tMtfQAuMVMqyuYkU6RWuvBQxVU8uEYqqAa35SN0flC5ylAxKonvNZzV1i8z6U2u9am_0WKBuldOldDO9_NBpL5tmUXUdQU0Aroj_i3YhLOjYSWsBoKYPE08oMZlf4ePjtYOT_NZqEF-oJemP10nTeRMn5gUphn61pkaed3qTIlDY7O0iRss5nUzSpbTsv7MBVjrtNEHKKp9gd7Ao45JIpF3GqoLf_8fmG5LAYff_I93KEV4FZLMPTfcKswyD71BzqrERPmFAOcJLetOW1GOkkHks2SSMeO6m_CgsfrBwEcd0s61dWizvtCMj5B4SGArxwQmOXxSFXH4DwGXoUZZjGpKt_G-x5Zne2PJhMKyrIfkiF41kkmPGmdMYRlk1dhs8oCsixuPsXZw5f8B7dXThO3db38tmVQTDjAwJXyU8MR_6fit0F112x5rKN0MncWFk4eCdSyPVeLm2SaTfJ2QLwLkvt4tYg1raGMg1ZmBIhb2AWyGXNpCD-OrUXN3A_NEP-pttlwDDmg--ebWvdvzDaQrwyLPwQQKpJgt4clHjYwmoV6VPGs3F0KIrVIQbf0GOesF0rp5UvmvTESRz-vEZgW2MToHffbqqQ1y1yIjnm7Q0auF727jY05IzOnjoKNYiUJYo6U9yTUNckWALVv4nZN-MRQx7fn3DRSoK0Dw9ao89BfT4qJyIQqWWwxmIrJX1Lo6GCG84BZamTnFFmTDZKEANEFWkmO5NuhuS0VrjqvVlB0dDYhCA_IEplDlGzB8iiwaipSO3Bl2aF9LwwyhafGq-vH0CkFKcKU6JGkkqaY1TtIuHmnTcRA2Mx54ZfjwrvpxUDS4PSEn2J6OxRIZyiqcANCEJHqXdK_JwcK" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Transformer / 1. Transformer (Attention Is All You Need)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/ALs6j_GRFexHyBTyxARfgUVShQmI2LtnU4vt4kW-DAVjYlFfvGDk-BccMJFgqVsEG89ndIcLnPq64xZolwt2SqDAKw9Lx_4KY6bx19BVcRzh5RNAGwaDkES64na96yTD_xlUTLH3y5NmA8QiavCmMdGS_HgmCx20XYeHwbjs5wz9CphZSFOFkPKUTasvsqNEM-yOM2ssKx4GLGh6M5vFqL10yVRgvDT7-JkM-Aj7L2Dm6hmAjCIFkE-n0rKwHpIkx_oW4BREEEg8jmKe_ByfA5kJpfBE2OeMUbs9nzr_VOqZJzS-4nFxSLfZbxTDaZU6zY--ABUkZufsE4Q3-WqYyytXfD1fa6eNNKHRmHXhBAy7n22FGR7O9wDW7jLLM1WIlvH1FH45jKUJNjqm9NyA_b9KQiecKyZ1hPRPpbe6oEmbmGrj_iJF2UDt_71JbrhKjlpvXrtlWOJxd5BO3iFj743EuhpT91ufnbXlw_M6KvVb0xbnD3RaKacZZ7c0VGuAXhm6xvjh7XfCkwBPLIIfhltrd1fbYjTlodmnZGJrkCDp7C3tap0uCv9eQ-LgUVrdWKpuqm1yFxrgPEHWTZtWVEZE0i5Ap_qYGnOKS0pJgnWvseHYrxqoYKcqYD6DzMF-s8SAd1UhuaNrfnhPmVW1bXqrQ-Uk8RwLLx3q2NsSQ3tMtfQAuMVMqyuYkU6RWuvBQxVU8uEYqqAa35SN0flC5ylAxKonvNZzV1i8z6U2u9am_0WKBuldOldDO9_NBpL5tmUXUdQU0Aroj_i3YhLOjYSWsBoKYPE08oMZlf4ePjtYOT_NZqEF-oJemP10nTeRMn5gUphn61pkaed3qTIlDY7O0iRss5nUzSpbTsv7MBVjrtNEHKKp9gd7Ao45JIpF3GqoLf_8fmG5LAYff_I93KEV4FZLMPTfcKswyD71BzqrERPmFAOcJLetOW1GOkkHks2SSMeO6m_CgsfrBwEcd0s61dWizvtCMj5B4SGArxwQmOXxSFXH4DwGXoUZZjGpKt_G-x5Zne2PJhMKyrIfkiF41kkmPGmdMYRlk1dhs8oCsixuPsXZw5f8B7dXThO3db38tmVQTDjAwJXyU8MR_6fit0F112x5rKN0MncWFk4eCdSyPVeLm2SaTfJ2QLwLkvt4tYg1raGMg1ZmBIhb2AWyGXNpCD-OrUXN3A_NEP-pttlwDDmg--ebWvdvzDaQrwyLPwQQKpJgt4clHjYwmoV6VPGs3F0KIrVIQbf0GOesF0rp5UvmvTESRz-vEZgW2MToHffbqqQ1y1yIjnm7Q0auF727jY05IzOnjoKNYiUJYo6U9yTUNckWALVv4nZN-MRQx7fn3DRSoK0Dw9ao89BfT4qJyIQqWWwxmIrJX1Lo6GCG84BZamTnFFmTDZKEANEFWkmO5NuhuS0VrjqvVlB0dDYhCA_IEplDlGzB8iiwaipSO3Bl2aF9LwwyhafGq-vH0CkFKcKU6JGkkqaY1TtIuHmnTcRA2Mx54ZfjwrvpxUDS4PSEn2J6OxRIZyiqcANCEJHqXdK_JwcK);">
                    <div>
                        <span class="mainTitle">Transformer (Attention Is All You Need)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2022.11.02</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번에 소개할 논문은 바로 NeurIPS에 소개되었던 transformer 입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">현시점으로 무려 6만회 이상이 인용되었으며, 초기에는 자연어 처리에 적용시키기 위해 나왔지만 현재는 이미지, 음성 등의 분야를 막론하고 다양하게 적용 되고있습니다.
                        특히 모델이 커지면 커질수록 더 좋은 성능을 내는 것은 물론이고, 기존 LSTM, CNN 등의 모델들의 성능을 뛰어넘는 모습을 보여주기 때문에 많이 사용하고 연구가 꾸준히 되고 있습니다.</span>

                        <br><br>Transformer는 자연어 처리를 위해 나왔지만, Vision Transformer (ViT) 등 꼭 자연어 처리 분야에만 쓰이는 모델이 아닙니다.
                        뿐만 아니라 그 유명한 BERT, GPT 계열의 모델도 모두 transformer가 기반이며 이외에도 BART, T5 모델 등도 모두 transformer에서 파생 되었습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">현재는 transformer 기반의 모델이 너무 용량이 거대하기 때문에 이 모델들의 크기를 줄여도 그 성능을 유지하게끔 하는 모델을 만드는 연구도 활발히 이루어지고 있습니다.
                        그 대표적인 모델이 바로 BERT 모델의 크기를 줄인 ALBERT가 있죠.</span>

                        <br><br>Transformer는 엄청나게 powerful한 모델이기 때문에 서론이 길었습니다.
                        Transformer가 처음 소개된 논문은 아래 링크를 참고하면 됩니다.
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Attention Is All You Need 논문</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>Transformer의 구조</li>
                            <li>Positional Encoding</li>
                            <li>Encoder</li>
                            <ul>
                                <li>Self-Attention</li>
                                <li>Add &amp; Layer Normalization</li>
                                <li>Position-Wise Feed Forward Network</li>
                            </ul>
                            <li>Decoder</li>
                            <ul>
                                <li>Masked Self-Attention</li>
                                <li>Add &amp; Layer Normalization</li>
                                <li>Encoder-Decoder Attention</li>
                                <li>Position-Wise Feed Forward Network</li>
                            </ul>
                            <li>Transformer의 의의</li>
                        </ol>
                    </p>



                    <h1 class="subHead">Transformer</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>트랜스포머 구조</span><br>
                        <span>Transformer Architecture</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        먼저 transformer가 가장 먼저 소개된 논문에 나와있는 모델의 구조를 살펴보겠습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FrnlMysVeItY6hmhi7qfkPaTpUcMQgCdGNi-42m4__0EsSYh69lhoyCpHvTjlKG7btYHmhBJFf0TbYDlWXy2m2x7Bvm3KVEPc0X5VT_p50B5m6rNMIMlSSiPHe8KblGSMiPpE_UKLwbWi4hOlMspj0ioxOTXrKNbPIK-7lMChStX-y5tv24V4-y6PICxJGwikpc464tXCLAVZH_L0utNcru1VqrkO6QtGbLeZDIZLsvoglUiuSZGKv_Pkmk5HlfcZE5gDybBfYaKIpYjMNnwNPFXv_Ej9yOSpeSMyYx6yvRgCt5MOvTs9nDTsSTobKePy9FCE7fOwtUiISkXUwgw_vN-qQL5IUGyZC8KmxCyNo3hm9tQ0hSoUnmLwvpzsBzUX988C1QS438hXbEnsKcUVyRVcsdBG0uXGbpR-dRtrSiGUQW4DE5ig4F-TtAYyp_x0ltj3xjcETcsX3bDUY4V2m4ActyApU45JOIdlmp3sBn1hVNbQY5MPK6_hTsgcCE4ivaozhfmFScDCTxjDal8epbEeNhfAOaJnWjlbKToGSFmLIvPRNNVK_gzwow0Sybiad1QAvoRfkJxNogmUW9Kr4Fx-mWzG3jE1VzwTOKuhT_pfIJWlfDlSikJX2yqG2g6a7A8u7BEOAadeeSZyN8G40lpvErOWyHq1O-pJzA6REez104H-Tp7FBnu9yItpmVeIN7bPW3xmqNnzXTL_PPZIwYzN6J_nCK-hc7eXnEtXxZEW1pZvehOM3uasujKe7T-hO9vvC180oUP_eDWWjDcykx4x-kA70QqabHaEMBuP9nLU-hD88ZLqmZveBVODJlkcf2_EqdfmveTtbVrmkog4gNJGiyAuz316Nykkm2SzEjOYGGJAt9Rgrwzi20flrLWKu8cHAyPgUm_RCPJk2mj8jb3k2qdBOuxA__Fr6VAY70pW9eX8l_cqf7MCdRxSq5MUTVv34NJiOo5TiCxCIQhQi4KN6JhINO70uTSXelKbq4EvstMJKRFomKEfNNJaQOjSsbvW1mfxbLRpqxv7KxGnwP8CcPdpHSB_LxgAF53Uusnd2yExdsV4XInDqnzlaZncgN0dgQ3LxsuZew5V6yGFz67LA-s5Liyj2JGlDKzpeFwLwww9gMMs7WumbmFBJenNyeNyS1A50ojQaELe-NfQC-iC8UXT_w5PlvqSAEJhscJ19bJbzorvYeZV6nx5T2y2viZHqekXf80qdZSw7Gpcv_Y-gMLYT857IwSKdFVXNSLldOTKrxZx9GKf-UR4Sis6kRoAnwXpsim2xg_G1J530Qbh2JrFwPm70rh7WXXEnP6Ix5uxwTVqzWCIAl-1KP45MUDrJyRs4f-r3CZyIoA-ft6Bey3sDz47W0kf7te3ArgrHPiSJrJpwtehWfXn8AMq2RNTVkkJzR1GTg3e9VmXpDLc7sn_9Rea3pljzjwc82U99zeKLCKCgZMaNRiEmH6JfioCJ8tRbG7j7h6k9ZUCtlhHW5UP5-5TqLljErdB5dkFhUTGEevaZmHvPPlDf0IHWQ9cu4VJE" style="width: 80%;">
                        <p class="caption">Transformer 구조, 출처: Attention Is All You Need</p>
                    </div>
                    <p>
                        <br>사실 논문에서 소개한 그림은 처음에 볼 때 직관적이지 않습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그래서 아래에서 자세한 구조를 그림과 함께 설명을 해보겠습니다.
                        그 이후에 다시 논문에서 보여준 구조 그림을 보게 된다면 이해가 될 것입니다.</span>
                        
                        <br><br>사실 transformer의 전체 구조는 sequence-to-sequence (seq2seq) 모델 구조와 다르지 않습니다(Seq2seq 모델 구조가 생소하신 분들은 <a onclick="pjaxPage('RNN2.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>를 참고하시기 바랍니다).
                        <span class="highlight" style="color: rgb(0, 3, 206);">Encoder와 decoder로 구성되어있으며, encoder의 결과를 decoder에서 받아서 사용한다는 점은 transformer가 seq2seq 구조를 따라간다고 볼 수 있습니다.</span>
                        가장 기본적인 seq2seq 구조 활용의 예시인 기계 번역 모델을 바탕으로 설명해보겠습니다.

                        <br><br>아래 그림은 French-English 번역 예시입니다.
                        Encoder, decoder 구조로 이루어져있으며 encoder의 결과를 decoder에서 사용하는 전형적인 seq2seq 모습입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 아래 토큰이 들어가는 곳에 Postional Encoding과 Token Embedding이 있습니다.</span>
                        Token embedding은 말 그대로 lookup table을 이용하여 token을 임베딩하는 레이이고, 이 부분은 누구나 다 아는 내용일 것이라 생각하고 넘어가겠습니다.
                        따라서 encoder, decoder를 자세히 설명하기 전에 positional encoding에 대해 설명해보겠습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EhXQvLl2aPYcKLnCKxJs3mgE4WupA0zraW6zRcdn_Z8wv3OGlIyJXAeyNpTFrat5ZLdhlO9D8w6TiM3HDBU5awyExFDHANdNEYt69VU0lYtjqxnyA96MPwGpBRrufPAL-eTBLsoyaCx7ajIQFyTy062gvTxBhYEbG5kWWGtMJIwX6LPaVvWYX-qfrDybAwGw4d98zfk5zHCg5jEKeOVlZlL4cTGoKDZuIeStNI9xw6kIRSMeFBGh2grAOrAoKa-BBSHo0Q3dhes1W8xB33dW36Eo-hr-lGITgEIJzjUUEQJPOqscPusv03arIlZWBWxS4iOvzoBzcAIRsMDZGUQ5I28TCFOLqWRkW7YNxJwEye8Stbia8vJskALgTh4rSzBK7fUxTWBfNomvqeLfHBbM0hjHg3xU6ljwY1MO2b-KpPT9HWb_TsDJ-3CmP5fkamZiW-FNvEtQhFzbMe96KKM9BVLynwsg1LjvGhdhgBzfWitRicvYsdJ7jfQPPslUBwhQJjMmrQp0_V5qpG9fgbJ_Dx3LyMwTH_ie1UBy3YioH8p8DUyChUHMnwOC5vhRjTHgICj72PmRMpBB15GSpgBo-CwdqKfRcT5a_53-oi-zo7P0pEJWaZPZfwrnt2xkOmHjiUMItfhkPyW4LLwXDX11kVBMpbC8XpeZvRRsoXKiKezTi2t4lWz28NCiiq3frnNQFuY-4xWzzPcrGWoBeSzfuAOVjVpgKpowlSTc0lNU2PO6cWCYdlo2e1e_yb6WlvGVAYWiM6wcWL--4s6H8OBMO0rUxxIBO5S1hWqxLPHw4PegR3c_JeZK7YGcKRPravS1m70HM9pxaHfpJHqfw0MLoeellAVkCCJ7ELwaWsrM3jMBKs-78qYkgEKyTSoT8AFtE0XajoFgO5vsLBHnL9-rQvZecgBVH1TRVKuRbiFI3FHeuWu5fnPDRGJSsuIEwOIsVPMyPsrMJRcbBnSkb70jW7ZRro5VbmuSr5JpDTPPuR55QAkfh3m3Z1nNEiIGPtVmq4k-EAMTyWXdDxdKg3ttmpmkZxUlHl62ZDaVRhJqfFJHPYMwxI9W3ym7Ind4rM1Ic2fNoK9r0tohHWxOnnAHAKOnptQqJTDWx1H1mTX57XnlWTA11HjqjUvOua2O2GYc2YCNOPh8zj0HDjtCrQdBKUVyhwwC9B8V13EjZn0Sv7lJGluf1ySgWj5S-LBIkoFf0gRfm9V4dDE_dhZKmZbLLvR4Uji3N4watmPwUMxaNplLwDb8bkNPvVwSQKXUO7uwiNe9oPhB3sGjDVJGhDz9ak1546X7i_2xCplS2ENQIwmPTAy3sAVHreJcsAHOZcg9uafnd0WudY6-24gsm_ME4MM87q9qPzVi0sSy-eVlppPGwy-QDojwWE2xZNWmR69GSItHG-m43HWhow1WeDYOZ4ZWoIMvj9L34iBOGlOicQDKhYrAV-p_JT6tmqY7zXQbulTiJk52x6wU1h3hGPosFt9-igtacd3w7eh8IK_A6X4EBPdLy8FYnoTSw7q-rtNpS1w0nL3w" style="width: 100%;">
                        <p class="caption">Transformer 구조</p>
                    </div>



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>Positional Encoding</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        Transformer의 구조를 살펴보기 전, positional encoding에 대해 살펴보겠습니다.
                        먼저 positional encoding의 목적은 토큰의 위치를 알려주기 위해 위치 임베딩을 하는 것이라 생각하면 됩니다.
                        요즘에는 위치 정보를 위한 임베딩도 토큰 임베딩처럼 lookup table (e.g. nn.Embedding)을 이용하는 것이 일반적입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">하지만 당시 transformer가 나왔을 때는 위치 정보를 모델에 알려주기 위해 임베딩 하지 않고 \(sin, cos\)을 사용합니다.</span>
                        먼저 자세히 파헤치기 전에 positional encoding을 의미하는 식을 먼저 살펴보겠습니다.
                    </p>
                    <div class="equation">
                        \[PE_{t, 2i} = sin\Bigg(\frac{t}{10000^{\frac{2i}{d}}}\Bigg)\]
                        \[PE_{t, 2i+1} = cos\Bigg(\frac{t}{10000^{\frac{2i}{d}}}\Bigg)\]
                        \[t:\,Postion\,of\,tokens\]
                        \[i:\,Hidden\,dimension\,postion\]
                        \[d:\,Hidden\,dimension\]
                    </div>
                    <p>
                        <br>이제 위의 식을 이해해보자면 이렇습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">당연히 \(d\)는 모델의 hidden dimension 크기입니다.
                        그리고 \(t\)가 토큰들의 위치를 의미합니다.
                        마지막으로 \(i\)는 토큰의 위치를 나타내는 것이 아니라, 임베딩 차원의 위치를 나타냅니다.</span>
                        
                        <br><br>좀 더 이해가 쉽게 아래 그림으로 살펴보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">가로는 hidden dimension (그림에서는 10), 세로는 토큰의 위치를 의미합니다.
                        그리고 파란색은 \(sin\), 주황색은 \(cos\) 함수를 의미합니다.
                        아래 그림을 보면 각각의 hidden dimension 위치에 할당되는 \(sin\), \(cos\) 함수가 있고, 토큰 위치 t를 각 함수에 넣어서 embedding을 구하는 것입니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HVRB5Vp40byiLWpqz9wHL18y-WmiJl837rViX13tZ4EQXwoyvDoTucHDl6OuCeA6so9WOLLOouoFtsvQKsOwupIfoLw1gI6LfH-Rlfk6y3Ctl6rx9TFXyUwbYb7XHkL-gIee8Lt0Z8u-Ncs72PDsHQ3JfxPmJbYV_ea9DGRJ7iYEt7Xr_x8_NDRUvW-OPSqZGSEIUOPeOsHj1R16eBWpSLKDIwshy_5Q6ok_1hrw9f5vDeTA6lKKaaQ4D2_vZmHPBB1049m5DAMoxzk-IZmayw4B0Y2RzMOsO1xHfr9Jq-r0d-NS84N-U-qZl9MX6I25tlMqQde-NuvkJMuhqdgB7TVXMY2rP9ShH_5-hCuMnh5emw3GcPfiYjLWBEajghDP9e-iIYUa834La9KQb9Iv_aIZNJzMLSfbeI8PNS48YPo5G9NQGUgARBB_obF4hmTxjY6R0RPdrNU2SbnsTraAqoT0vxGarvMQI15R5UKcEKjUK4TMdu0Xnrv4tRY9RYbruE-v6JrkU4EVmcxvkk6Cr-9KoEEKCjgYZd5TkZb02B-49TdkDVDKn-EiPFe1JXh50x4RFhJS3VuScb08w4Vu4TPj6vQU079D_nkNa4NqcdqqLi004T3Mte14s4jVSHLpTk4U6VBwDJrWW7zOlZ0qBaQ6X-unr41wQK5aA_8XC1kl8Ox4wRZLOEsKg5T4itHiQSxaq7Hjna7PzySaQzQW3Lf2lcTcF6tzAbHXztVmCvtCQyXj6lotSXKJZeIa4YxdsDx7t8j7CvW3wfpp-onSZy1z2lLis2sJAX5IL9_FrjLk-LJB-az6_OaBF9SqEiSzGKGAxeC5SEt-PenObqXC6ApGC_dmNs4diSrLqunReFo7YFKCLhY_Xj7WKrZxAMPuqgCnRSRtBoBkMBU_8L1tswtqX03R5HCxhCnBsNtHzYKmfEcFfv9nnM6Sq1LXdxOtptuOKFPWv-NyGC7jaY0aGNr1RXtKa4ASgptUdXzrLT5-t0naLRkvm3EOISvg2l_MHRG-899mQ1ho0POQcHCWo0pgm5HYMwZrT4psGrGXPsuPYoZgW-NOuJ4xI0pVu527njvVnTvxoJBbf3nG71TYF4Uh3HZcvZ6OenVHD5WDu9r1AA44NXJapPJbwvKQkBl8etvLBZsGlRlidO2-ZlPSSHU1OWFDNDXntuFLa7_P_bk0YQfchkVznFzLCz-lVdBP6k2516hfT4keeH9NTCdC9phl_lroNKkVL88FQYXV1oN6AVO_syIhOczoGHtGVvs3ELSrf_wdArUwHiYTwqHglYJ2VvttX3vtQ0fqi-txSBz5qRlX8rvHI3se6wRPXWpqHo4NG1BP0uIeombJkyi1P_VAbhmE9LUyATk2g23vNNQcfv0c2285E7mU4BdOYUHxjMG8SDzIgohGmjvcea4df4nuMAOOuxZIxvZ9GRYTb4a55DzG9ilr5xFiiJLU240kcqTWj5MRt1I1p1OHi4sgvSr4ioznoQjzhEOaz4Ob0M4UEYVPm-zHrsRJcLSXjggllhObG-gm4W" style="width: 100%;">
                        <p class="caption">Transformer Positional Encoding 그림</p>
                    </div>
                    <p>
                        <br>즉 위의 'have'에 해당하는 임베딩 값은 아래처럼 구성 되는 것이지요.
                    </p>
                    <div class="equation">
                        \[\Bigg[ sin(2),\,cos(2),\,sin\Bigg(\frac{2}{10000^{0.2}}\Bigg),\,cos\Bigg(\frac{2}{10000^{0.2}}\Bigg),\,...,\,cos\Bigg(\frac{2}{10000^{0.8}}\Bigg)\Bigg]\]
                    </div>
                    <p>
                        <br>그리고 실제 \(sin\), \(cos\) 함수를 적용해보면 아래와 같은 스펙트럼이 나옵니다.
                        아래는 hidden dimension이 128, position이 50인 그림입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EPlkFH3xCSp6H5QEQuFPRMnq7MsGsQYNH45TAy7V5ZnSPqFFZ26Zid0v8eHmOg6Q2Tvergk1Je1sXyf2cioGRXRYM2HVKYzFhW2MuNbluHbE2LR-DCogBxRiviZ8IvX5JnCjUx8FxWVZTmgKY5muFRX-Utvo_-v12CjzAwyn_qTSwo9PrarXPgSzXHcWb847Atqio9BoyAy_7ZypDU635lwgLwiIL0agipy634-NcRXEhKaUGAVomNW0E2lKdpX69_sFcNSAPzCVfgi2tJpg9ZuyKMbObDf-ZbeIRuWgT18zyIGtQ15RjnjryyKE7Ft3fdTOysL3v6bxt5GXeVAzLTbBCj0lXyAJQqCrh1rbIe-MNzO0oYkyD137EFbU-qIYCt4yJznoVsnHGKCVKiSHVzicmiGmB9faB-NUe0-xFWdVoOnPFX1t8uFbDUWyfPHp9cAN1bVo4tqzlSRe89mo2bBiurlYD_zVUdPG6uuXORW3QfNqRMORnj8NDjYWHz4GV9wA24178PongbPJfMxk64eLpcDBORs7QLFimtTLHvkaAvlHiN0WXKZG513hMuldAHsE8WLVWRHHBjHWobIiF99So0-ff_KJf8SaZ9WPYosctLUxewhlceAAspF9cGcDNQYZfgZtYGxazcKAlr72Zn8IFMnZxgXiHR-XbqmclX_cbzyj4qsYzKU2V_RAuUNr-NMwVgY9TKRgR-ByjUwUOnf08ipLXLQnoilwFO7du9icbrkPHdwL4AYeLRliPG3QwOqXOZNePj2-3A5ueEq3leKWCh6WUGZAHTf2WkWAh2ItW6Gvs4lrv8P9efJjPLK_PAHJcJ9uH23gQs1_Ydd4AfycJddVVsDieAoF7eYtSzLqj4yT3-8WXiCezavGAJOU39bW59Wycn89VGa0kivbIJNxQjmFS_wT7At6kN3A-0kafSIBpkZpvK_0WmGz8dWqR1nctM2GUa2wjk-N66nATy9fgRvG3KsinjEziKVwDS48CQWMQv1eYOM9EbM3FmMkmSR_mSDxyj-H1gXMy60iPC9KRWy6SkAn9YtBG5L-D75QmdsZpAR1YqtLNY6gNmCD2RWeDcv82e2F_-mj5-p16QS-ZwIMVOTmeonWuo6IRkxzBuGNiBXGD_UiTM1T1HhLu5fJ1xAk3yN3j_zttK2sm_XsUIpJ514z6OWH3_caQmwNYXLgFwad9nNMmAfflGNh4Okk4SKb7jt-8bHCmwsFWlBYrtznPqa6eiMdmaHe8SyAT4GfAcquzNTfLgmFE1gY64d2qjCpqYPewHVDAjgv-OjnB-8oA2HwXcJWnwTSuY_86LbK5poQkvEGR0GzM-A1YeTVWKxDrw_OEzUFuvKidd22GCoeKC0RJ7bZpwBoXNGA0unPEoTBSLLjSDNw8KJCcQ3BkN1ier3YV4_SBd9ENVsQkrtK5yEz42lWCmSRvuuNcLygeMTG_H2yhAXQgaBOGmy6Bs0YdhhveVtjW4PzCEvRxSZXmiQ518r8t6pNzMQoP3TVD7QueWe1urBum3ZZQCkBM5GA" style="width: 100%;">
                        <p class="caption">Transformer 실제 Positional Encoding 그림<br>출처: https://kazemnejad.com/blog/transformer_architecture_positional_encoding</p>
                    </div>
                    <p>
                        <br>그럼 저자는 왜 이렇게 복잡한 수식을 써가면서 position의 표현을 나타내려고 했을까요?
                        위의 수식을 대체할만한 방법 몇가지를 생각해보겠습니다.
                        <ol>
                            <li>단순히 포지션을 정수로 표현</li>
                            <b>train</b>: I(0) love(1) you(2) <br>
                            <b>test</b>: He(0) may(2) love(2) you(3)
                            <br><span class="highlight" style="color: rgb(0, 3, 206);">위처럼 train position encoding을 2가 최대인 문장으로 학습을 한 상태에서, 그 길이보다 더 긴 문장이 들어왔을 때 대응을 할 수 없다는 문제점이 있습니다.</span><br><br>
                        
                            <li>문장의 길이를 0 ~ 1 사이로 정규화</li>
                            <b>train</b>: I(0) love(0.5) you(1) <br>
                            <b>test</b>: He(0) may(0.33) love(0.66) you(1)
                            <br><span class="highlight" style="color: rgb(0, 3, 206);">이러한 경우도 test할 때 0.33, 0.66처럼 training할 때 존재하지 않던 값이 등장할 가능성이 매우 높아집니다.</span><br><br>

                            <li>이진법으로 표현</li>
                            <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_H-z_ZTsv5ZBYbqX459iwBTr-2n1ny1bAZk-V_0QzxfUjMA3lTQsO7ZmgCFzDWUWkvOLmFpNM7mGQa6H3-t7Lwh8b3JJB1wmdS3NYl1U7O3ARnwoki8H7J7iuTn1nHSBCO6k1uWWUK9EJAcU9rf5yZYJZtU0lNxSucELFjYLfMehqWerpDhZhtC1V4LNrjIecqI966t1stpIwy_Qs9dlRHxNiuok_TRxm3vqlm6vHVQHMmBlAorKX4Ou8o_ZaVwmA21JsKyTrdXoIG4SseUWVNvVuTfLGAAfR_7VUjtO8-ne6SJWB6aUmmIqgzvIQs37WJ6FlUXyijIMK5FVtv6r-nWzzw8T40zF0ttlGIi2kILjcxTUmkttc3x4OqltIXBjMBzZQrybP17-OxEchkMUhP8-z_mrehO3KXOFkTNa6cPMUhpgVqGbmLEXpCf7DLnWIzVghbFwg2iuCcZMelZ4dnSrC1D5fPnY0jSE9FZChxlocJbKV5re8lriXWVpcj1QI1sF_MNVtgYIe2OKen2geKlOnTZhgwrNXgnp3A6bBZO5doMuj3arWkrC8DZBEyZ4_j1GpuQKsMtBdrHogvZVP3ZhGBIwmTe1I0fqWOwLxLz5wvNkccW38EzA8ZMCuDP4T1XMOVk_NMqZuuKX3YlBgCxAqU_c2vhDrWlMhnw7H8aOFRFaBJKjoUNk8NUXhE6iluGP1k8mF1zTnO0p8sWFJ40a3B6K8Cpy0_nctlPhJOdT4ObOFa6-E3mOpgHLxmhdEJWPTf4rPrefCodT69M63rDBem1W8g3XsoEmcKikorcXAoPecC6lju3Nn-m-Kbs9UlDSgxhW4NXDSR2o_s21rOmbwehLvP3pthjBIlPE5lI5urFQf6Px0wxgEKp97PxE2RGOKtBW6ztE_-3jusJkf88XRh9AI7NPbW6tb-FwELdFLBiYfPDSs0AAQi84pR3hApWXn_XX8oNGAa9Xv47l2F3ygsDww3rhfl0IxOBLgzqzJhSMHEXUgF4j6EDGVHm6EzIEPASQ8AkvFDCXgAuaMAL17uZSlKcNz_0bo5rKODe6cpVrMTGq-RcTz4QjBN18dQEhIc7LbxpFu_MqypGNhvz4yQ5umWv-CnSBlRZqtD5kslrgF1ysGPYshgJ9FtqQWPIyjwHVK7yEV3NVw336JH_0f3d5-0QWo0yiuAXww9Fch_iqE2RXUfoBNXmgz17HRr7LxqPURMT_n6Ry5NLIVHCslld8EF6VPRDUOqRQM6gTYTDzKAbwM_kUICSq2889trjtiiwBY3BUJG-9eBhJNfL-JKvZY9_jwM3Na5_QfUCdHBgvedXJxj05SxvXQzPJv8xQCPPKOUujLgKwvklaY4RDaqrhXzRnuAydL_mNqzWdWLQcfbybLGr_uyxZINfP3FXIe6sdkodbng-bW-cisx_i_sTI-GgcgFbF5lC7y4DUDDyEM0eWm8cXI69RWRpfN7u4BkYtNutimQAVy4HxXDJi5jVMDnBls04aGCjx_43zrxl4QluWfiVH7rgCe4_FFYMir9vL48G" style="width: 80%;">
                                <p class="caption">이진법 인코딩<br>출처: https://kazemnejad.com/blog/transformer_architecture_positional_encoding</p>
                            </div>
                            <br><span class="highlight" style="color: rgb(0, 3, 206);">이러한 경우는 길이에 유연하게 대응할 수 있다는 장점이 있고 각각의 column을 보면 1이 나타나는 주기도 다릅니다. 하지만 float을 쓰는 입장에서 굳이 sparse하게 binary를 쓸 필요가 없는 것이지요.</span>
                        </ol>
                    </p>
                    <p>
                        <span class="highlight" style="color: rgb(0, 3, 206);">이렇게 위의 이진법 인코딩에서 보는 것처럼 각각의 hidden dimension (column)의 0과 1이 반복되는 주기가 다르다는 점에서 아이디어를 얻어 transformer는 \(sin\), \(cos\)에서 얻어지는 float를 이용해서 discrete하던 이진법 인코딩을 continuous하게 바꿈으로써 sparsity 문제를 해결한 것입니다.</span>
                    </p>



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>인코더</span><br>
                        <span>Encoder</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>                    
                        이제 본격적으로 모델 구조에 대해 살펴보겠습니다.
                        다시 한 번 위에 나왔던 transformer 구조의 그림을 보겠습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_E0Ak_ntI3INxIPtIep6zTZ4ywF8ajOERJEmj3NW-O2mRtQdPb6CvJ4HDzCoNC-mnKnRXsdAY8SRo-LlNPZtergKT5UuZ4l2pcd6G57iaMRKLoZk0QRA4hiD2cZYY-Rvpf3-0IvNzSGjxazRpBlzw2tOwltZB2q3tZCOjQkeK1O9JYRveSERDAegKuUGeYQJUQgbSKQ-IutKhUH90FQ5Ls1EPkqUX-BzcQSPsJcCpwIqwkdQSg6_4vMuNRq3Nv0D3jacwdWSayru-oUJZcrltNxXqI79dSSlYRKUSlhBuT3zJ8DKJNKlD7bmJwcQjrtA-4ekEKr6_d3RUGlQ3C6BOik7vd8QLQXZn86XtMokQSl04hVtsLYrFRdrapVSoORDHORonXuVvVSNFBUAZuzqh9IvtpGK6HAY6LxZESUf0c4jNiDN6b27nD_0Byfkor-8lwSjKbPVv1EU75BTAmGKAb_kgQbKCzfeafqJND18_PsxRhs6mba8ExUPPxTlaulGerXwz3UphYddbcFwgIscmgkST83GFL-les--wGxPO0j4TyyZnc5r9gQRtJzkN0C7yNdy9M9aH1SkLbRBYVuSpWinSfw_PUYyUImIiwsEAmQ8hpX5-Z8HCrTYuj9e-u_zPixVcscTxmAnvP4Kx76tqE1DPUxZqZIEM04SKEMsVp4oyXz-KKXznOoaJQWG35VKHgFZVV84O4246uaAjEVeYmTatrgsRrq31V_uDw_5yKDxSbSEulNaaE_vjnnliFd8ObwfiHOcuH--4iFhLmOharTat_tOin-0HQyRgXN2PN8AH8rLp3NmOWxs2ijlGhKP9rZFEG1BwN-PvE4vSwa6Wb8k9yMC6_pZC9JrVY1ZNkrhhqoNfnPgqtSyAO8U7LZwidzf-kyoihdunvHme3sjpnKtQqhFYoiNNUjkvP_nrBjtDqtYzrg4kaN_t7sCYgZCFiQiN505Y3AMfKizXrATuNVmAFJEC2Kn-jZWrZ_fiLlm6JuFjCas0ZbQHnlvSJXHGU-inHjvWRXo0o_TKltZuT9MlUNsiWzvv0f-QJNOpC5ZzCjrNHuasJNNOUdsUmH9WuL4yuOCXzoRHo870-gFWKO5CIGEjGYrffPC61ibXyO_5S13T4g8wS_TvZgdeV9vD00SluJ5IpDJItTiDGs7nqMbiTeLx8o3AyJ8TjYw5V8b-0NDPSF4UhSEp1ZrtKU4uK3qEKIYNmik0YFAipI7Oq4NwLK4mpLwHo7hlSWLe1oYkD0PzRQLYd3stJXWyb4XyQAMONcPZh0M-z7lHxfvXoUy3zNQ0pRy68ZNR9HVTLaR1GsOqsFnqu_Qnbb1n5hwfvuQ_C3BZ9RCCqEeX4OMaG4h2tUwfmfNlglATeyaUNpcorLS9jKOpeGBqALnDGbOVj_Hnbnx6bvXsPRV7lFNPCzxdjT_uVjTXjS-V5wq4xXYHaDFVCk6tW_b49RHDIrXVSKT7ZDOUyNDoBGYLr5cBtZMOVXxClgCIG1b1NVHtbBbsIftZTw_Bk6ASesX_XJB1mOmdFjJA" style="width: 100%;">
                        <p class="caption">Transformer 구조</p>
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">위의 encoder는 사실상 아래 그림처럼 여러개의 encoder block으로 구성 되어있습니다.</span>
                        아래 그림은 4개의 encoder block으로 이루어진 encoder의 모습입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FSt_KSR60qbUB5obXl8k1lg1IPnBF-UtjszdMi7KOMG6fTlClrMIE_Ecj4l7aTtrgUPam8M7XiXuw5EC3P_CoCZ2qYISWDKhQyJSCFrEXzjpGIOp39886_ZVlq6coHSlKpZEZKzKxFAt8Bwqkxo0m1rrXVGktWd7oRWJUtOXs3BbQQOeKyPI1Ohsm79bDTeBnEeQ08BK0Sla6WXhwMEjxWdq6PIctq0hxqLpJkxCEsLEqWFzsGebORT96djVH9ARfohHYLvqC0tvcetc_4Q9ZbUchzrjG8_uXUz22Fxm-iqMVf3XwQJrh1wspV3lqel5kaERaH6L_mTlcuelf9k3n0spkWHu18OCaTBHq2-AcqkrtXcKr00kDLhOLgWGq-ntbtltJm5Qrz8M5eV1SeyECSS7WS5p3YssWy9zAFpq0_WHwzK9_F1t9k3BAcNh3pnb5svcmoXIF0KrqwTTBt8BRVhZTgbrBEDWpsEteijzNIpMFkoAQvZdIq_Et1CaQ7d3426Plv1gNv7uyQMtjxrhbJVP6KcJiBQbKi_bu64RhG9y5hfernWicP-p4TNEBHA7IWhoRwN-F5aFxXHxVNXsb53Aq_nCkzWtAxNF3dk3X7KQp0VELJlZcHSu3mJ9AlQmFskvVspU8fNZLnRyM4KQyoSWRIl9vRHDaU4Bt2HKDXKwQMTZ-uCU59OluhS1vMaRgTaPadh_czOFkJ3saBuxWgXHUH_77piCgbKMjzChC6o0FkOrX0QW8NIk-hFisVjzo512Jfkz_90Au12_UTdXQmMGKw0fd1zYj7kb7DmmPXybeZmO6XXYEDEy7cnD25etZM3e40CYHP9xiOjuBJBaJ-51SmR6gWP-n3vRSwIxBEhc721U6OwGizIwDUBoYXaI0zE8TzhXTbgdzZ4B201mw-VwvCDWJHEi1uGg7j8qQ-cH1LCXUwmIRIw8Ou22LNCEOBAGWiAwaH0ipWtBBjMcamWG83Dh02BrxX_18FrA2TfpmcmOyQwTfO2hTtm7-YkfA12evRltdMiuhGWYSn8zwhm0c9haBdy8biwyLhhjxl_ShZ4lFxxipxGc4lA0Sa8GG61Vd4KWDdzfGcycmgK0Lcpwt8IVu9db7j9Nybk0OqecyB78mB8T89-nJyINsJ2vgCqs1zTjXf_ONE_iaoaIm45-s9gG0_TSLMAXIDJ-UV2muPAXHHawWdo4u4yC_Dy8DVzTIRyPI-9Si-sqqMypeuHGsGEnPvESZhXMnWHzngKQGTkO5JKVhioH1LDzdTRF5koobJpypNlJ0o_PSFf4Prx_-NMaiCaRl1l9BYqXrKKGldVz4eHOfbTx4m6K2jZMFiunbrR7d-tsdwgmbXDMj_JLzGeFOf86_IORR2l38_H1uKZGEt7gIEKDTHRMTeCk64W0I4jKbNSrogZWfeS52oaD57ZuUY6-VDRAQRZRNNf3k4URF8n1uO1NIHWxsGefXJlwPeS_uQ7t9FwsZlbzcBFqYjmbiLNz7sigrDX-WtUo_enB-SMF0I70WcbnAw98Vfm3vJ98Q5" style="width: 100%;">
                        <p class="caption">Transformer encoder의 모습</p>
                    </div>
                    <p>
                        <br>그렇다면 우리는 encoder block이 어떻게 구성되어있는지만 알면 transformer encoder의 모델을 알 수 있습니다.
                        아래 그림은 encoder를 구성하는 encoder block을 확대하여 상세하게 나타낸 그림입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HY2xChiP78vIggJQ4AkOXgkpvGztoIL261yLMAed0vdadGtKH1G9xVDec8aeMJ0SEbIgzKRnfl8sNFXLzG4qlEqt058U9izXh9wS2fWGrTnvbfV_u1p-iQYTpjRbUUZjdUHk-CECRdB3tI73VOnSLQV5qi6FgODal9s-_QRqIjsYznX29uExJXNSSbRry3VcasjraoFWImFmv6EwjddAHk96oUIOgmLkUg0czc7Q4gIVgY6MssfZdzR7Abemv7hQpmFcQchZsKyCIkvgu24v0lckd2QrHDPCWytnaPMYJ16GjqoIvmzmKlRMZygqul0_7lklrvGdqV_b7rV5YhY3pCYBShcV8hCxEnKYsOBl_JI8aRj_NJSza4BU5D1YDq3U7CLXDbyVtPv3ujyrqTN4ujlDcJFNytaD0Yu2pT9Y6wjh2Jp8K6x25IXx44tNCsZAkE1VZCEVKJM_3yRJrwKNRwf5KM4qxATVnNdG_bhj12SuEIrKof6zKd9UBdj9RLrDSlET2acVMWEJrilQbCZHaPssMyBcmBzYZhxcvEY0ukkMCHeVx6lAqdX10IkBczljnpltJNdM34uhsNPA2OyfGV_K30w5ccvKSSOFHJv-VEPeXqqQfsjRMHQ3qcbjSFC_bkhsK-suMpXHOvzpsCOc8dHeS5ObTQbrea5GRgS2NaZsfqeyP2rVv0MgHfWAizOFEMheaqJAsPviRSouLBpGZ6AViiBL3y8ZkJ8_3-4AsX61afW3znN_VUb5BVcUBYC5BjFzHB7Pwhhg0SOrV0WX_UqcaWEWKI8uxhlG87_6E4nZaeKgwpcSWMmKylv93iduRxWU_eRlynuZoi-NkdFrOwcfAAuZPzqQO91c2Auy_Dfi5rSmCA1FYpIVA2rnGMsU-Cssx4tZDO2MyQTWiqsvckuxcBo9uPSOGs_Bxz16wklRDlPcpShK7z61LEc0Ymt6gwGAUap-VYDpe2qSNxiwFlNx7CLiWiGOiEqYhhu1_bfwFMqvcYNZGN20rVVELGKNwP6C3H6TIAKgV2ntbSNmMMiPVyasr5xb4sjuJrIiPzWMfEzt23av-qDwk5xEEy5C70yf8gLSINp2jVeFsIZw99ivLhSsDYvcu4lLCMPYdq3CnvwkOjvn6TXeODZuBork4eOVw1vBrLbJC8WO6fYJfgnQsd-aX4-RmICCTzDmDe9xvFOBlz4fnnLOX82Y3YG76P9xi46UGfePPFAi7OwyunOcQ7lhAlRouNT55ASgul5lTnwD6OXzSnbeoeL-zNqo0CVct_OR5duustR8VeCuIfPC2S-6PJuMt6SIvKrkzTbMXUJLvVaw3PEOIFa-rflsQOI03Cu96SofDGK3UdxeDT4ymcesW1sCk1ZpD2OLGEAtXBasJulZmpJirJprF4e7_wJC8flyh3BTPNQk3v2v1wMjT42i8OiEoNPRR9BDO_ilhQhEWjXILz5PYBr9weObw6akPVl3U_v6q_5oQKkM0yi3sSaNRn_elkt769Gti0Jfx1s07_vvjouXphFltlWWR_0KfGufhx" style="width: 100%;">
                        <p class="caption">Transformer encoder의 모습</p>
                    </div>
                    <p>
                        <br>위 그림에서 알 수 있듯이, encoder block을 구성하는 요소는 3가지 종류가 있습니다.
                        <ol>
                            <li>Self-Attention</li>
                            <li>Add &amp; Layer Normalization</li>
                            <li>Position-Wise Feed Forward Network</li>
                        </ol>
                    </p>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>Self-Attention</b></span>
                        <br>이제 transformer 논문 제목에도 나와있을 만큼 모델에서 가장 중요한 attention 부분입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">먼저 attention을 위해서는 Query (Q), Key (K), Value (V)가 필요합니다.</span>
                        <ul>
                            <li>Query: 모델에서 연관성을 구하고자 하는 주체.</li>
                            <li>Key: 모델에서 주체와 유사성을 비교할 대상(Attention의 대상).</li>
                            <li>Value: Query와 Key의 구한 유사성을 바탕으로 구한 가중치를 구해줄 값.</li>
                        </ul>
                        <span class="highlight" style="color: rgb(0, 3, 206);">먼저 encoder에서 Q, K, V는 모두 같은 값으로 들어갑니다. 그래서 이름도 self-attention인 것이지요.
                        즉 자기자신이 자기자신과 유사도를 계산하여 스스로 가중치를 찾고, 그 가중치를 또 스스로에게 곱한다고 생각하면 됩니다.</span>
                        <ul>
                            <li>Query: Encoder 벡터</li>
                            <li>Key: Encoder 벡터</li>
                            <li>Value: Encoder 벡터</li>
                        </ul>
                        아래 그림은 Q, K, V를 이용한 attention 연산 중 sclaed dot-product 하는 과정입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 decoder의 input인 '[SOS] I am a student'의 토큰 길이를 맞추어주기 위해 encoder intput [PAD] 토큰이 추가되어 'Je suis etudiant [EOS] [PAD]' 문장이 encoder로 들어갔다고 가정해보겠습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FbQ-uYuwEhPT4YQdQjgnkS4WFunbS77dykFsrRqgTrm2KW9rdoGpylD4gcVEhDSCing-cdk7KSCdvNMcYoAFOjr6gbqN6cjUYASdqabHGKthoo7_gqrw_fyyiXxhStG6CMWH_LobOp3YK6DX2_1xAC4ZZIHTwSfsIq9zkixZZ4dfJZ26IARJx9X99Zr9ce0qw9GskVhapJGX5Wo8nmJvbZzqQbMCUrGUvM7d6lQpwskSqqFjZYlDgC9VL0YaEuLubRArPpVhNXc6Y_Yu-M86GkywB8mUOsLR3j_4cxdC3hnQupH0Aj7UhXhNQ9BjihI8pGATqLi5rr-NGE22v26NDkfwm-0LSV5DKfyhfaKxq9Ur83FBNZGbXI0npMY8h9QDrIhj8MiYQlxJvWsokKMMIn0Mii4BHjSERA1i9jHRobfRbSsIbU_MKkH-cV-CZvor-OfM0K5wyPf3SRzPO4WsYl_-F_KJJkPEpPjvNPYD_Jro3mnreRozKvSn8qUjsNcKQhDRf2XU15-sq-4KeeG6je4sfSQXVZ280XqY2E392slIMGn3AO3n5iMx6vQjgEYOXTRCDUyEHJiZrH4XDqtxWFgqXASoQmfoRhZUgHUfavVqo3FIkLaMGzK_ZMbCIc4T92n4LLqnJMRcfTisj_zBMZQJilo6uSoVQVLzUkNFKSGVD_8t2Lvv1Pbl97rzB82c9PlG6rLEEz55iNDwD95zblKF9oPMZ1SEJfJ864nFo6a9ligvkf4Jj96dEFpYw4PX_qEW0a8OElACYURptjBL3_NRQJzRcM4LvOnEjkWGgjkMmR-GhvbTNkLT8R7mlH7upb73FB4kZLyPuhKU-H9Vrh8Q7r4OOEQFzCdaATU_b0GVPlZResn3DgRaLYOyoiekJuByKcMxI7qC7cjFLj2lqB0rggpFFvuz0jaUGO2AevKf7bPteeSBofRJghC9XHBoh0DynaCsamriu_oHN6SPYY7dqwl1K32Q0v9Yin6OduwE61locJXoJegt02VJ58JVTEOXXkYuPVjjVhkCJL3n54cgN38wFX1g8fnsojxG9FxNlz8LTl4WLJh-mOIqP4-HMMtooDV3On9EGlSaU3hW80YvzVhbmUioKqIBi3_BYqGn14pGWxN_Alpe8cat_iEkRn8n99YWKvlo1G3Aq-QFg4kXCYjiad8ZlkPhy7OZfTs1XPvid1g19x2i4OesV4WeJDx0hZTV2P7zLPh-s6DVp6Fk_X-EoOqCtGYUwr-4enE74ZydUfQM5MO06Igt6UFehEOzoXCwAuQ4Dh-_KKKZ2uUG2vcQkpEy8b8eIoVpHUWE3P7hgkwG7_zJQzhnorhs9gna3Z7PLiMxU0yv0ycz0Zy1t8_L53DupyjwmWqsnaCtFUVSaxtyYch--LIyHunRcpmKlTBAsV1fiM7zTn-wWV3MEWzaTu9JgcPmPTwJNX-LAoSS8w-PLBguRuWdquQvT2p1klUxknLLMtBDJcRJatwz3UiGRCjUefSRLjA7r8W1V_e9SbFIMbYZXD_AO8qo0zcvfqaCSo" style="width: 100%;">
                        <p class="caption">Encoder의 scaled dot-product</p>
                    </div>
                    <p>
                        <br>위 그림은 attention 과정 중, scaled dot-product입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이 과정은 Query와 Key 사이의 유사도를 계산하여 softmax 함수를 이용하여 가중치를 구하는 과정입니다.</span>
                        <ol>
                            <li>임베딩 된 값으로 Q, K, V에 해당하는 각각의 가중치를 곱하여 각각 독립적인 Q, K, V 값을 구함.</li>
                            <li>Q, K의 행렬곱을 하여 Q, K, V의 차원 크기의 제곱근으로 scale 한 후, 가로 K 방향으로 softmax를 씌움.</li>
                            <li>각각의 Q (row)는 각각의 K (column)에 대해 가중치를 가지며, 그 합은 1임.</li>
                        </ol>
                    </p>
                    <p>
                        이러한 과정으로 각 Query의 단어가 Key 단어별로 얼마나 가중치를 가지는지 계산합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이때 Q, K의 행렬곱을 바로 softmax 씌우는 것이 아니라 \(\sqrt{D_K}\)로 나눠준 값에 씌우므로 우리는 scaled dot-product라고 부르는 것입니다(여기서 주의할 부분은 임베딩 차원 크기의 제곱근이 아닌 Q, K, V 차원 크기의 제곱근이란 점입니다).</span>
                        
                        <br><br>하지만 여기서 Query는 Key의 \([PAD]\) 토큰을 고려할 필요가 없습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">따라서 softmax를 하기 전 \(QK^T\)의 결과에 encoder mask를 씌우게 됩니다.</span>
                        다만 여기서 주의할 점은 Query의 [PAD]에 해당하는 부분은 masking하지 않는 것을 볼 수 있는데, 이는 Query이기 때문입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HaQNyQAkkxHU2MGwCSNAP6EtCzAzeOFmchld1mDXxE1A72E9HzpPBtBWPDRAPbEIuq6PMpLelawqvBvqNdvdEU9_jj6cIZ0EoFTZUMLOuxgwrOaJG8dr44u-h8WtETS0ad5jA0wN5gHP-MMS-MFwJ_7x7ooxdS5FKv3W1c7OBsa5cbkWqUC5X4V1wiKPeh155IFknMxPsd2do4i57QzSzKqoOhSThrleEIhBaFLRO8g7W5EuUo5wKGXTQz-YQr-UBI-L37pydJh1Ppa9woVrKlCwtuQOeL_7EIrofdhj7jAPHiDHizhaNUxXVOQiYJi3NEAABKsnPSKEcRIoVazUw3r7UKR3gbW80sMS2ioS6FQEj6tOy5_q2yZaF77ynRhkeSv9bobePQHJGGiY6u1UKrj3SyIYJiVUNmVY3ySoNxvZL251vXIA5ezkfDKqy0Czk72VgqIs-_Fjnera2-Z6F9YNG2TmB8g5vPImc8nWzOr2d2XGkKXwQGPZGR4-9DLRwpfOemG0bXYovQ9PQaSiLckm6owqkP9ye0cH44Pst7mBhhA9iaBtix_WabiNnVQ1Jvx9hyneMolGEdGnqZr7O7WKI9jUMaPEQLzHg8qMTdKW5ekVgcyICWaIM21C8FOGrlVwNlf0WfvVmOzyjDVGa3w-eGMNVYQrTlH0DbJg94lP5gPLG-GFs-YYXO3Ge61pyy8bep0Gpv4QVK08DaiaQPi1lpo3y0EUGJGHbi0Ol1XhN8_iorSj4-gJFuf26Vq07_FJB7ncA0bKJ80dY-R2ThOLASHprUraGqtYoBM3sjmsn1i3i7q3DUg8Bz2ytvdzV1v91rvaSCpofKpWB_1bmCF1TMoIqkSNZ2N0PVeqYKdY_qw_szRdWIEyuKnDynxEYbK_jse3E6lU7uxSy30dFZFjOQoWx_xXF0s9_DxnsRP6uvPBBR3OcIQuIt_8lQ58VZmhhJ20L25in0HhK8E7sbmGTknVUbQNDqItVmUbV1dD7p4YKQubAVu-o4PG7-fl3fwYTrn52Ck7cy4Bx62zivMTRWdLKGIMY2bcC1mPy_tte9NLgkkNx6gZcymkC3TBB_7NiBfd-VJmEgynMrwLTpegxEiU02MPE2BMjGXDIZnnRSaQc7rgwkKdmKrvsIEtdN6PkTaP_YZ6Bg_7_zgHtOIoQvZ6VsDQM5wi1exiotx2MvqHPE_OsC3rtyMd4oGwD4MSL9KpluvOATK1ay6oDq5Bl76tfR6p3kT2hsVo00n1PJNpnIo4crbt6PD-22uFR83c-DcSlqi8yTTJ_CKASFIGc4hQixPdmYG1GY2kPZA-ELHUKYg53CZL3T8bQpHJxr1jqJhc6KjSDIIKmyynfmGHg8tOrQWDxr7nNFGC9AwuOQqTr20GL3clIikjz2WDZpLMiDoZ2cTQh4Q0Hd1z86RrIqaTEeJ5AmZnzgWxtva3T48T0lCdUGTxhysNSvcUtLK-lrHl9d_lm8NkF_qNyamt8TmtOCjcQdUXYUraQuQ2MDQYHGqp6375kB3bBj7JjihqNRlMVJ" style="width: 100%;">
                        <p class="caption">Encoder의 scaled dot-product와 encoder padding</p>
                    </div>
                    <p>
                        <br>위 그림은 최종적인 scaled dot-product의 과정입니다.
                        그림에서 \(QK^T\)의 결과에 \(-inf\) (음의 무한대)의 값으로 Key의 \([PAD]\) 토큰에 해당하는 위치에 masking을 하는 것을 확인할 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">\(-inf\) 값으로 padding을 하게 되면 softmax의 값이 0으로 나오게 되어 Query가 Key의 \([PAD]\) 토큰에 attention을 하지 않도록 만들 수 있습니다.</span>
                        최종적인 scaled dot-product의 결과물인 attention score는 아래와 같습니다.
                    </p>
                    <div class="equation">
                        \[Attention\,Score\,=\,softmax(\frac{Q \cdot K^T}{\sqrt{D_K}})\]
                    </div>
                    <p>
                        <br>이제는 attention score를 구했으니 이 값을 Value랑 곱해주어야 합니다.
                        그럼 아래 그림처럼 최종 attention 결과가 나오게 됩니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_H6LlLXHzkyrlbIyIyTyYpGO3ljpfj3GfifP7ZVfC3ZWNSU6ti1zaEJcl_Hvkgjc7gQJfFsjGAsdMkAxiXVhwDvfM6YFCEM8yfjndexLH90Y7a2gaHbSGXfKWMkb_CmrEfbvZMTB70T3AlQfR64JA7sHWoHNO5CbuDdsf3orBcdmUFkM4nSL8oX-CA0wtpOblf752kPUHfCcwdkvVdSXLdu8YrPnWsLq2L-FuXBDhYQeaWAeSR2q4ow_I0Gq_YYb6Hw1mCybAtQjU3eNRF7a_8HnwxDqSkYm4FGJ4iJG5rUHfVvHPw8zxxf_lJgo9MZPGWuAgF247cpAuTvSYHRUc8sxzB9_B2uzKydQPCIR4QLFZs10BIxpcKQKljn_MJm3A1J0fu3GUE4RPxjJajV_Udw1Yq-zfit-5EqzQrbIOLkj5HXV1uMdBUEC7u1pglKr-dCeaH-EdiBHXoRpVOf7u892dLi5bHenwVPD_5GjMdQifc5TV_9GIDNkOeQizjKwZA8Shs399Hhym_QU_uG8UnMOWEEtbjB7a0FKgDyqRlm0AO3MEaGz1v6tmfjsFX9H6sLupqhVhPttnYmZuS8tcTEqnP3ZROLQKWac7q8JPGl8deK8LeiU8pI5NaEdW1vaQZYCjW1Hr_-7ZTouz7ZVuuHyVbhd99TdP7siFfVlximgZQuvOssYGWHh41nYakdZOQ9NxJok6yzSKGRgmJPySFLylYnf84-tUflDlZpPVf66sN9xHuPFMkQw52E_hz1GkSFPNfJMjhI7gH7CLCmuHQ6Tp5faAIaopkZegizLEzic9MfLHqeB2tPzzpIuBq6DUeB9LNMA6ZgasZItvfF3-sormdPPjegV6lo_daBnm4uIDcHtQenwbEnFLcgqKl5TQhd-ufrkF-czTuscBPRLbqsKaVSBYNY1FRnPDPyrqdjzWpy_S2H9GIBW7FkthfQWr5yk-C-djIZo30SOzmN2VIP7BCFJlV07zjrXEV6iO_vzcBiwE0LZ5St81qHD2teDFqdNQjq27tHnjdRish35KNYVajTWQnIVnipegHsoZp7Bz2DA6vnJeJ-S9xa_RArwII-bfm2NCzzOJjpi24Ugl5CTZzCTXYxj_Wd7sRmYPhRPqKE43G5QHs5aAhq22gn6deEqAS95VVTCO1wp9K6LDP2y5HA_sTt4rj7ABUHsjAboQ9ymN_4M_4u-rYXRpLX3fpurOzNdV6KRltVqPqc6UN3sqvPdFwO62pDfuxh2LKYF5sOG2SWv1xrglYSdCigTldRdWq0ExWF-Rgz2VGzsDpPIFu_oTKkkfQY4yWQzIdcNGMOg9TgocLPAtHZ2n9-oaWOYif5fPX_CjN2uQq4X1LpmW_G0JxN6YiDhEz3VvD_dZSnISOLDyilWljtnjbp61jywFKUKK4pV9B3XJH1UafZJm3we6_FSOvx40wH8uE_7CzDaGZWCYpdt78eLbqhDuod0g2q5V9J9mGM0OiVJ9vgLnLwS020GEInEpyQFL3MiOYdLUNiw7jMusge38yy8T5Lkdebbg" style="width: 100%;">
                        <p class="caption">Encoder의 scaled dot-product의 최종 결과</p>
                    </div>
                    <p>
                        <br>그리고 최종 Query, Key, Value를 가지고 구한 최종 attention 결과 식은 아래처럼 쓸 수 있습니다(Q, K, V는 x로부터 구해지기 때문에 x도 식의 변수도 넣었습니다).
                    </p>
                    <div class="equation">
                        \[Attention(Q, K, V, x)=softmax(\frac{Q \cdot K^T}{\sqrt{D_K}}) \cdot V\]
                    </div>
                    <p>
                        <br>이제 마지막으로 multi-head attention에 대해 알아보도록 하겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">쉽게 말해 위에서 설명한 attention을 여러번 하는 것이 multi-head attention입니다.</span>
                        이러한 과정을 거치는 이유는 하나의 attention이 잡지 못한 context를 다른 attention들을 통해 보완하기 위함입니다.
                        간단한 예시를 생각해보자면 하나의 문장에는 긍정-부정, 평서문-의문문 등 다양한 특징을 가지고 있습니다. <span class="highlight" style="color: rgb(0, 3, 206);">이러한 특징들을 여러개의 attention 과정을 통해 다각면으로 바라보기 위함입니다.</span>
                        물론 위의 예시처럼 간단한 특징을 잡을 수도 있고, 우리가 알 수 없는 high level에서 특징을 잡을 수도 있을 것입니다.
                        아래 그림은 attention head가 3개인 attention의 그림입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FbnHZc8PQiIiRgsQ82xnb8RX5poutnx2ZnH0COXjAJhMfXErV5d7RkWEQr2gUCd2m8GCVYlDaCa65DEolLiE_-pYYC7n7dyrx9ApfSukq2n0rYAJL_XNmuafW0xF7gX5-YNUizMoB--FuQB3Pn-w1G5OPcD6URaX-K1xNoVwRgBtPy51_n2GA7E8ce4Ylr8mNjTriDSkaRa9Ea9m4UyRURyHv2z6wUkwJRrmqSWR5L7TVx-0xys08V-iNFYv4llF_e7E4ivAChI-dM44mkLvbRE2KZFgwK3j0kxZB1TFJz7gPhhxP6Yi2aCBZMQy_7Y9Fl5Xk94pgODGBIT-Ct1bt0nucmNrRnYB4IAOQXog5lSI2KwxCxflz9Kmn7LCdqHnI1k9dCd0l7AJb-i7ZDnR1c5kfUSxDbDIiFFO7tfk8y7Pi57Rb66YmRR97pukvWb_VwtgeGIJNJ_DwKRPMuagm_rtna7Z75QSqHos5kjczCRTgiqi3MoG3WT_IE22ZHJ5K5bea1udmHiek7eQwDLYZ0i7fRL2YOZ8XZ_tkUYRJj15tSw6eql-sJcNHcjW27F1b1MCxlp5FM6ZfnSSeIQ7yKplg-3lb56anzq13gYdF0C3pL08e3qAApeq1Eu1WNFuq6PHcYKclTRnMrXsEhYP_NsYOyA7jsbX0Z0MTlegjdeWJQQmbPvFuQfzxuMcAgUPPgneIFIGRgKz_SDd03OotY4rShnYrM2sohRTHcawT71CwrfO7rO7XLd_zHStIspCpsO8Iihu_F0YaUXWTDWPLl47eNtCFhBDAa_yDePI7l8t75cCEn7bZ4PHOfzWZHw-D4MAWOXB10H3FGbJ4MYOrmNMN6eaR3efqmXlE3T4R7hqjGm_mae6NQntw38SRE8bcE1h4nt-siZ0fmvJ2wl8tUbzTI-9-RDjz7Q7dsTpp__YG_lD2ILu_FrECg7j2vw8h2unt8HMbFz_f4aDYKO3viX2nuwtLOWL9lxRsuhfeJa5Y8cI-lCS6LPWBz7tqu4tGf3ey3e44dVnpnChSHSu74yPZ6HTEVqqB6iNBcbeOVPvo7nZx-vMJg01QvmMnTZgIfogHL9LuZQObuJsT30I49-71jrLcja1zhTkQSVGWyjWPBAHbPnRVhjqVFfLhaJTOJxy_OVWwbgCKBP5iUBenIrssPJBSdplG-I8uOswHCcdcjdvBDEKY24JAt2e5Zvj6NfmckKvsZhv8hXqgKZ6uI0UOOzeaAADXbDK7wK3Mb1lcl5_CPywNQ7F_2Cb7EiTdG3NeIyIq4TeUden2xm_VY1SNEN9cCfJ9MtjJlH25X3gjEqd7Hh7zkbQHVMzZXOCYygBdvfVio5jZxHOPrqvFsdLMPhefzmtR6LF8NC3WzEsd6tyZVTW9dim1JaZ9I7SzipDhmzA9z3X3GSKSzMO7_Dfai2fk-YYJV4_zsn2IZMaYBz5J_4EYUp9mqhx9MWPI0THrxLmkJj1dIVzyifd3Wp1XQzK_AQn61uaI_vgK-3yWvBMTpReV6PRKttSFaUT3wJisYH6oJ" style="width: 100%;">
                        <p class="caption">Encoder의 multi-head attention (3개의 head)</p>
                    </div>
                    <p>
                        <br>아까처럼 하나의 attention을 하는 방법은 이해가 쉽게 되지만, 여러개의 head를 한 번에 병렬 계산을 하는 방법은 잘 떠오르지 않습니다.
                        이제 한 번에 여러개의 head의 attention을 진행하는 방법을 설명해보겠습니다.

                        <br><br>먼저 아래의 경우 생각해보겠습니다.
                        <ul>
                            <li>batch size: 128</li>
                            <li>sequence length: 16</li>
                            <li>hidden dimension: 256</li>
                            <li>attention head #: 8</li>
                        </ul>
                    </p>
                    <p>
                        아래 그림은 위 조건의 데이터 차원입니다. 이때 head 개수만큼 데이터를 나눠주는 과정을 'Split to Head #' 과정에서 볼 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이때 중요한 점이 데이터의 차원을 쪼개어 여러개의 head 연산을 할 데이터를 만드는 것입니다.
                        즉 기존 모델 hidden dimension \(D_H\)을 8개의 head로 만들면 head dimension \(D_K\)는 32가 되는 것이지요. 만약 4개의 head로 구성하고자 한다면 \(D_K\)는 64가 될 것입니다.</span>
                        그리고 이렇게 구해지는 head dimension \(D_K\)는 scaled dot-product할 때 제곱근의 값을 구할 때 사용되는 것입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EEtEBdQdv6ulLaB2gJHMsnrPxLrBM3z4059ztrWC1mqJ9Wm9p9Fi-9OyveYbNO3tqoJkGMfhYe1nmEfdd4wGaVi9mQM6s7S_gupJPB_rR3jCJJs8qSnu7Yj2xUYyLAcO_c9qWYvlbdR9e7cDwG8ARGj1WAiqHwxxNNh63JcquLS8h5Dm66cAIPxBwHc-dRvwqhtLhTKWFoYw0bDHbQPzssqX4KoZKnr6jmCmB7w-vfST2tV5mie_bApoxUoRbK05POAGzG-y7c5SbasXd4lYjCV9a8uisAHCniRwiG_m6DXodMFDXQRDQHtP7qjMGJpcI4lRRgRlX5-DUQGCz8M9MjtV5VEoRSTNKe20liZgu1Rr9hD6ugoMLdkxm_3nD6IgCYbZWo_Zylz-KzY4q5I_cgaKh34XOIzy9H87SIWq7VUrvWTqkLy_qQCEWnI5T4oX-x9jdfCh_fHRlHG42NYh7DQbFzjP2QsIzNoOyoHzZZ0WAD_P1PsSW-3Yjri4ZLlelNr-TJPyxgUffiDBYDgTf0kfDyxlAIK-MycTmgnNdJLJMY8e_iErB26fcKuQXaayN8UkJUgiqx57HV2bvnhfo1atb7wvKceOa_1-6bpg5WFGqvIEw9KI3kTjrG7TkhpnsPDGwxP3l18u5iFwDk-WlsTBBfMsg4tZpuINcmskXrhXNPPcEcIL9bXwOfYg9cE3e3R6PInZT5cW6R6wBEIEqZvSDmk6vlBwGGq47mOf08emtVSrJHwRFS6eB4Hiy3dvu5kYBULW0Jz9xekHsPyFmSt_PhgISnjhd6_tYT4yNy8LnqqAtCnUrgjiPyX4We-IZkj1bq7_ajEo9Kv6o_OM6S2A5M_ETVwtEqNJhPt8wLYE_otrU4_CeqZyt9YjFBT0J9Ul0FEbeG8NEjFD0__g0GOqwXYevK54YIk6HC2fCo6SKuOAzTTLmwbqdjRFDjTnsB8xCN7uIPBqhsosEU4PaK7rd3dNqWWwSDqQ-j3umZ3rgg8o20KTtsUDkop6vT9EgFLPOjVlFApIfrwe5aXkxep9cpwo6E-6SOu4dxyEEaTv2U8BuUV-wjyI3X_RCKZQwu2o_1dVO1fjpzFuReoY3k3LVeXN_c-YD5suWEmXnz6CtAGCeoQsDORx8YQsRni_4ly9-5WeZHXHzqGggpS5VtDBBy2pNrCsVivCk7p4KNfrtoj4Qpr78DCC8-rZx0BWj5FxfR87IsWWFGKPDzrb6YpaquLlTyAyhZmGa0lDGcFv9HId9v1TkikavA1aXklByi83FKRsLaFe9MN8wh6YUqczyWGyDdBY1ELeqKe_pkhH22wsDC-I8vz6vMo7TX__Kvx0SZBjcP09y6Uwj_jh3HxmncjV6nC8zKfqQupQ8obKVP2VYA_ns0nxnGcFQXlwErIRl1eee2XF7CkXo2Znec_lOLtvk8PMAY82OQUxeDIn1Aj2IxMFWOB0S8NUBQyi_guIaKzH40bVI9RczSvMPFOQsTYjZUoMROWZ-xSeyU3Qq6t0Kpd49oO7wGFtyn_TDkVCOaqw" style="width: 100%;">
                        <p class="caption">Enocder의 multi-head attention (3개의 head) 행렬 차원</p>
                    </div>
                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>Add &amp; Layer Normalization</b></span>
                        <br>Self-Attetion이 끝난 데이터는 Self-Attetion을 거치기 이전 데이터와 더하는 'Add' 과정을 거칩니다.
                        즉 바로 위의 그림의 맨 처음  'Data'와 맨 끝의 'Data'를 더하는 것이지요.
                        이 과정은 컴퓨터 비전 분야의 <span class="highlight" style="color: rgb(0, 3, 206);">Residual Network (ResNet)에서 처음 제안된 residual connection에서 아이디어를 얻어</span> 구성을 한 것입니다(ResNet 설명은 <a onclick="pjaxPage('CNN3.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다).
                        그리고 이 'Add' 과정을 수식으로 나타내면 이렇습니다.
                    </p>
                    <div class="equation">
                        \[output\,=\,Attention(Q, K, V, x) + x\]
                    </div>
                    <p>
                        <br>그럼 layer normalization은 무엇일까요?
                        먼저 컴퓨터 비전에서 많이 쓰이는 batch normalization과 layer normalization의 차이는 아래 그림에서 볼 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">Batch normalization은 batch 방향으로 정규화, layer normalization은 feature 방향으로 정규화 하는 과정입니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Hg_sMEIavulHRNdqBI5N9N3Mdn9qUPvPTEjQHm9zAv7gs9JqZOcAyg3I8CK9ISN-bfiWV_rDv9XpUAfXx4G99KTV7b4duFBHeMnflp2X2triuIZspugS-bMWlR62-spCsu1RBz9Ftue2F6t0Y5d1VkojEKI5kyFN3Jnwv_DxGVuLLITEs0u-gyVtW94kHGiIZl6H45y0VcXC3b_57xLtfAqMTH7QJeAGp62SmP5K3rKnRJwR6R21EozTTjrnUBSgvvjWt05Vzi9dfGkj5SU0zZq75_G2A7n17TgcP6lX_qZ2HG41vBOK3qLrdtzUaBJwpwwzVvGaZwGCCqZQlE0Y3xyI79zcgJHrmYZKrr60hJ2mjkUEdzoj9Bm58WE__aiVQU5y9aje0ZLbddwxuMBPW-MemoMg8tymPkbawclFmr2ijx5S5syPTHB_GwhuPyaoq0K87OKWmwgUEkJJEKx7QiFaaccrM7IN0w42_0yFbgFagOr9_vKNMxRFcJmYWpqHbYxnuIaVnAaInzvCFMN1pHgWpJKOrtP8vxs1oGfX1sqDSw3tBXeW4sJ5CjBHbcQNW7FYy2mI7XmaeuzGq-RlQOJlgFOjHMjp1jtP4CMtb-0G0RZxhw_bSG1V_L807P5fzi4j9uAsfaEnQvPwAUgLmaRLvKNEb0SHQVJ0wcdqKU6mCEJ3ts-mzjIQGHZ3i5Qcb_iC2O7j-cNdikuH8mdIKcslxkLh1l_OQAMaxMRrCU9fW-asZL_dbJX1Aju4YCQwG5TuAYdkvyxIc_qBVt0nnW94u1C9c-DdR_OAXqK09Lfkhnho1EicOUNLnhwYWGzObt_fBSpjsMnfwrBU8DaXVZEMEhlr8tBpHamOekBxdrEW-oUfDG4wkjoseoVwMGAY2-8PJFm9EeZG-34yl0DP8AuINs9RRd-xWMpqAOpvXjclZPLb8qWhUj5wZFVNWGPo_9B-2czC_3QPEE3uugcLv0Vr05Hg6gBAEIgMFCSnZePZS460DIghEnT-U8P0mYIqP8kHktTJxzsbsDkPFZm00A7v571kMImLkVSNXSDDL1cQyo1lcuuhO5qt7SnEeELpIMr8mIIkwHgQNcZ4pwTiAvv0Jmq58YX67oByau1i_e7isGBqx8kp_1wbKf4xYUkG2decHhF9WChtEGwSUvcrRjo0Qdl0tEuPMoMiDfhg--Qw1GRVpnyFcUT1hCoWFibrL2jaqT41rGAP6jQ6KfdCK6tOCAobEDy7RnEq7O_Cre5uTiVtbaZmN209Zhb3uBcMuyB6X5o8USQTqecEKct545M9s0M2vezM-BctokF36NbtzHZQHTptcaG1ahowfibhgyhDVBgnJg2ROx9EOYu8Io04LOohKGLaCDrH_Dn4pIpuSbl1ljH4MhW2d2LB8RsIvnwTi1wGzqCVBCIhHu1TD4o-ExMY4DZMk7hFrYcJaM52Pw4S7FV2V6ZTHHZQdx8-ZBazwL0X8XerMWQvkMhz8ZSwjgz9IQEGGWCWijCzAAFT8fJRyThH9jjztoNXiHzzc1_8IKLyon" style="width: 60%;">
                        <p class="caption">Layer norm. and batch norm.</p>
                    </div>
                    <p>
                        <br>이제 layer normalization의 자세한 수식을 보겠습니다.
                        아래 수식에서 \(\mu\)는 각 feature 방향의 평균, \(\sigma^2\)은 각 feature 방향의 분산을 의미합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 분산이 0이 되면 분모가 0이 되는 것을방지하기 위해 아주 작은 값인 \(\epsilon\)을 더해준 값으로 나눠줍니다.</span>
                        이러한 \(\epsilon\)은 보통 1e-5 혹은 1e-6처럼 아주 작은 값으로 설정합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">마지막으로 \(\gamma\)와 \(\beta\)는 훈련하면서 학습할 수 있는 parameter입니다.</span>
                    </p>
                    <div class="equation">
                        \[y = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}*\gamma + \beta\]
                    </div>
                    <p>
                        <br>Layer Normalization 논문은 아래 링크에 달아놓겠습니다.
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Layer Normalization 논문</a>
                    </div>
                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>Position-Wise Feed Forward Network</b></span>
                        <br>인코더의 마지막 부분인 Feed Forward Network 입니다.
                        이 부분은 단순한 2개의 층의 linear layer로 구성 되어있으며, 보통 activation function으로 ReLU 혹은 GELU를 사용합니다.
                        그리고 이 레이어의 intermediate 차원도 hyperparameter로 두긴 하지만 보통 모델 hidden dimension \(D_H\)의 4배의 값을 사용합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 종종 모델 사이에 dropout layer를 추가하기도 합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_GuD6fMUG14tK5MV41aSu0qE3BgZYEWDpqmHs69RH4vVW16OmCOaYzFFYQxNyEJAKgm1PaEytIeBEY7CN_OTDhqMf6ZKSY3TCto8lKtukE3tyo9xRx9vopTI6chzsoRfCIJtelN4v7ssiTD87cM6OH38RpeAGaGFYT7qV4a7_eoDHRdjjt3oFrGMNHHG2C2A7q7xnXMyfc2VH6SubmAN31uHW161aj20Z4kHw5PnElnSJrd0E9Pbe1HUDo_qbIzEttVaqnqZJKv3n2oS33_XCPmnfAqD0NVLXiuqtlsWSnaH2g2TljXeq-ZqH84bajUsOI-bDgaauNWDt-0rfhYwKj44D__3RWL88xufocctWuChcCC7vLRVag4V1sCc5Pg6t5SBwZTcYrEf_Kda1w7i3igQR5A-mqx0-mj598uABJnPxQMy3lsJHNeHZ9anBkto6LIZS9xcQ8QBG-tM5b6ivSD5zS4ou10ziEUkBf6vQjRk2FwQ3wBAGu9pX1Y2wbu3zaN2J6aQRKuB09kTGh9Fj0m0X2cjyB5ed2RifbWk4Gg-JQF3otdLuIVYRIti6EwRI4QEVfAVyiZrArsSXBI6r1zdUEop5NlgFnoyYxEiKOiqutEK42F3U4nkdDsslvoe-VzoURAVPcfi2cNBgcjOZjCF8zTSYxHk2rxeHI6RPAZK1VJhz8pVNY7OL4a909Wxq5xI0l_EBrS5QKTvmBRTPPGTPOpF69VkAJd2CyP4TYep8EN9aLiZcB675c-RC-50q0sFP2qLg5AKL7qof5t7nCLEpIsqQ6-2H9y4kGorlfhAE0LRSdLSsORgioee_8LSZEG5LIUbCnCyORVsJUnwByXb1MLR0jSCZLgt6esedXt7z2k3iNDnlwgA8Ww-WxG4Gq--swcUUM50CXhP7MY_YEyPYkVZUcBHSI3ED9HjHNLa6yU1kPZAzQpZfc7e2v6Mc7iE_EEfHOjjBjItj-kWHBrHIGPkyaRsIEGiKAYGMPze7mvyNrxQ00hPnT7H3MspwfU5WeM7pQFQiUg04i8ot7yx7dehQSCQ4gAXTZYmpnnn9W-E6jC83rjRpKUREgfwcNf-q-qGFhayXhKoBsDItwuyFw_5CjDGf1h-PsbJYpODq9O6BQfhwvaqeNktcA4oiU4H5R9t3AEwlIn3FabT5jPUECAlkhTQ2OV2NQsXRrv8oMfDr0CR5AMgx86voFnY9hHAZ7JQpr_Nr2LXOYxio26rzbOBbqYTvmIUiRCHLEwB0Q2cSmBWf9FUTimubmAZTnvt7MAAwYLxW29uD_br8BYk5kVLJOumK59_Jtw6lc__vjTNKxDTBveie6iWL5jo5luFPv4ieZ9VjznQtKMWYW1ElP1WebMPU_ZmTcnL_7Qt6F7FLDbJRMpiPTMM-v_1r6_kHZj8g45-WsCbvw-HF10BTNqVDTKYA0IuBbWtigoVAHOCWOaQNq727AOtVxMaMV1KCssoMW1C0Oe23HxBk0aGK8BKbp4hjPlp53Fi-2JCWKjjayqFi9KIH7sYVbhbiAijW7oFxFL" style="width: 60%;">
                        <p class="caption">Position-Wise Feed Forward Network</p>
                    </div>






                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>디코더</span><br>
                        <span>Decoder</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제 encoder의 구조를 파악했으니 decoder의 구조를 볼 차례입니다.
                        다시 아래 transformer 구조를 보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">위에서 설명한 encoder와 동일하게 decoder도 여러개의 decoder block으로 이루어져있습니다.
                        즉 우리는 decoder block의 모습만 알면 decoder를 파악할 수 있습니다. 그리고 encoder의 output이 모든 decoder block에 사용 되는 것을 확인할 수 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_E2carvj3g7CsRVe8dgdPkZU-XwHjB-sCpJ5Mz0KXtF7frpa9wJccaWfgrfGhFF7TXcZmDpdhtDlJ5ZfSQMaM26PQkli5utjp5Xakts7BBoYB7Wo9m6LmYoPfcPYkYVUvMfSINZKzTL3BS_FQPdsq8FpuU3FPdP-B2Tefku-eBEBD4TfDE9RZg1Be2LWi2bfVD6Cw4DvxLCSO2Nln6NcLszjby5y4fbFLyKePYbELeuv5VUA7u0WJM9s4a6Q-08IthnbvUcpUmZp7n1_D77JX69XsRAFEF5WAQVt3bycpD_qxm_1rsCZGOkWEMOWRZXJIHbEvdR71zfQ5GwKLpym1IaxiaC6z769oyqDjQjxYMX-cXCcztfSVnkWK0AP1mDhhEn_yQR7ucATB2NLm_sSwiX4lBYUX9i4B0GL1M7ApV5ggtCLmU-AlUJPcs5BMMxkNiDndAmJq8IKZCJQ3GB-xx5NbOURmHpP7ylKwzGa1_185i5MwW-L9aIaPkN89qO17XlEErt5KQcmTwf6rAhIuaUkFlx1NRb2PNNg5PJi8InmXpeD8PyqMOPrLg5hBLpB43vXE9d0lELueDyw6izjS1ZSz07r5q-1wDZV54fbvQ8ZfIzl3yb-zt9zz2CYU7dEynM2lOb2JuSmzjCTUeOeZLyaKX3A9YLNEhchBb0b6opzJswoKKXobcY4769x9KJMA1XY4jWhNMKNQeIom0_0cmUtmtDKYwOVayHP1yeLx3ZtMtsVV_YPWKWB5AIppIVHF3WndDnV1RaARo1JX0Wa3c9q0oSZsp80VgfWNwL0CPxKsRAGDRw4ragR6BLAXZ4gdTJ2hxK3NSp_OKhtSOYcn-9ThmtVTrqV42Ou1BgkGVgE9KZlecLLfS4B9CRifuJooFR0vvokveJWQo3Geim7dZVRqccRcQMQKu8-wGnVP9kM8nI8pREzEadUFkOJNIKDKaAkFK5gkycTGVcR97OrI47sppV5yRp94iTDgE4zWqpSVtUGEsMCVQFAS_g8iuujO1RxPy3QRCkVl2s0rojqpuL__lz6GlWKD-IWcpWLp00Fl08SaZEelhWb9OHW8FvccwSMaN2d4ZF6b9Oq-qpUesHSBV_HxaSf1HNHaCQEWFckdhJSSJM3mfqJrlk6T-I_8VjtPFMeg5Ts3_38lLupscQ8_LcS4Ae7ff4-F9KKNIFVv4KqZsPo3MOoX3xu-D7FDWffCqRD8TqYNGfyVgAt7n13zhsgbG2siCbkpdqrzxpAbLN2KAlrhKCKBamjDpvvEM9bu2JwvYVvd2jWLRbNvxYe6ZNLgv5xIl6FL7Yqsl0uNQYLDEWW2qBw-0eJrwsXGX8vyD18dAiu_89Y0o_R-MP6zAFV5nmRIpdL6AXjKNxFBuLkK3jzlmLS8uoKQRZTreDdZKURCGPVouU5uUxFy1E01hXR4tCVh6NX_6wHHv8KKBfInXacinQi2ISHkB6BZG2lxUgUfIVe8JJXB0YrdfXHtcPacvvlCDtbcCe9l0ap2JnXRUNQcMZ124aDtfGpeem7OsK3mc6" style="width: 100%;">
                        <p class="caption">Transformer decoder의 모습</p>
                    </div>
                    <p>
                        <br>그렇다면 이제 decoder block이 어떻게 생겼는지 확인해보겠습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FgcE5Du1n6xww7BRAkok3lmPeDUTkSDsWajdSrXfMWvGyDXZewoBTIwgE3CIYv5M3IbcgU3MIjRnMW_3v6wwnGZ0bwiWlUv_YRWqQjHU_B5wllmuXTegY0XCEqNagBPhYcEhJ0RU9n88aINksMOf11Qs6e1A3rCz0Kz_RemOfkiV8yaqA5CJh30MKCQ00I9A4xL1O0HmQEKT8UwsjIox2c2QnL_oXmqpsOTyJUvMptXiWE9rMn456U6G0NC7GDP4C-TcA0KqaLyiVhe9nqmSTc7jmEH5dAHlLeUcuXI7Y-4eShXuwB3VRs5hem1FnVbtXy8WrNcWBMEdoLvwLGOZXZILbS7WBByWagSHCxGgyotcsMiSkzhNG2qs51ShDBnJWgAUv5z4zsgNPYX07oxGmndRe1NNksqUUk4yn2SHIu6vISQdvLoqCWr7fnkUSMin_G_sThS9cMBPIO0XLv6Z8_5VJIlUIz9KYFink0Ed6AuD0-M60iku16q-JCQ1CX4PT5FR7Q-WIAVEfDBA4OXGjgBP_RdbmQubDtoafeEZXD097DXWdpiH1K2H9K5KA0gCr3PWtYZXM2j2p3JHY3NkLi_zyFTeCg3zqJrh5F1sTaxH-w9xmTi3-8GWRZ1zUFer-jyOjlv-asHtJg1pqEvhn_K1xONI9Go3cosh-F3Ihr0HyQR2xTO560gYsxCFmeByY8vTyYKAfroKxoxBue7BBcf0gg9-eKm3QrQzEKJThPebSy7lREcQ_maFWVEg5xFZEph4qZ7qQP35XQlpDTBrxeQP3jRhdYfwxWwrTuqlxP1L2S-UO0tfVNRNLmjorWWgLOjpd1JmB2Znfh_TPpY0R-4H1upgXvGwxwf_xql7lhgOHr-8XWz6FI1dyuoT2fgZSW57AzS-ecu8aHCEOi7MW-TFxhdNezhj9etRAD6UsLEZvYgxWbhljNQZ2fOpf_BIjvdug3B4Xx5nJnZGSg4kXZjZSEyJG8ssi12crbJcoCDs8IwHTmlDzPrsjKyrQb5tUm68swkI0Ku0__TI58rg6BRraJq2mwGdPNhI4vvXmd6_KAkuMIZPvEhV6CWAOAJsHMemYlna5_1l0xgmlkDTLsEokIY-0lS8aokDyNlfNXtYisPedrDbCAY3nHwpsoISAh8lR7Ir7-GyNj1YeLcMH9-UgRZIq2gCKPdJDiX95do-WAeC1YtqgHUSqsiWIUYjilBJGvU9ROOTGP3HRkaiyU29_cNnkgRUnq9_kUoVuCeRO2nqwNa1sE6hBj8dBta2xkwrZX2fECNuZZxVx4MRVGmKGO-y51O8AJqi7ZqxAcDahG5FiCb92TO5GHP4lBn_JFxrinQUzU8DWRILySbXIeaJ8wHifnSIn4OeiLPUR70yHhpxC3V_R_sREvYl03UKaVLKYGLyKjJOYbzeFW4b5epbw3xHTKRc0cW4wxKEQBLv5fdXuT4ncYMrbgV34KmQ4-uEAbxRNpB8_BAf-jwXROaY96SWmMwAZipI9bHvop2j1ktkPl4tHD90d_QciFMlOPCWuK_w" style="width: 100%;">
                        <p class="caption">Transformer decoder의 모습</p>
                    </div>
                    <p>
                        <br>위 그림에서 알 수 있듯이, decoder block을 구성하는 요소는 4가지 종류가 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이중 'Add &amp; Layer Normalization'과 'Position-Wise Feed Forward Network' 부분은 encoder에서 설명한 부분과 똑같으니 위의 설명을 참고하시기 바랍니다.</span>
                        <ol>
                            <li>Masked Self-Attention</li>
                            <li>Add &amp; Layer Normalization</li>
                            <li>Encoder-Decoder Attention</li>
                            <li>Position-Wise Feed Forward Network</li>
                        </ol>
                    </p>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>Masked Self-Attention</b></span>
                        <br>이제 transformer의 가장 큰 contribution 중 하나가 바로 attention입니다.
                        그래서 decoder에서 또한 attention이 존재합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">앞서 우리는 encoder 스스로가 self-attention을 한다고 설명했습니다. 그것과 동일하게 이 부분도 decoder 스스로가 attention 하는 부분입니다.</span>
                        다시 아래의 Query, Key, Value의 정의를 살펴보자면 이렇습니다.
                        <ul>
                            <li>Query: 모델에서 연관성을 구하고자 하는 주체.</li>
                            <li>Key: 모델에서 주체와 유사성을 비교할 대상(Attention의 대상).</li>
                            <li>Value: Query와 Key의 구한 유사성을 바탕으로 구한 가중치를 구해줄 값.</li>
                        </ul>
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 decoder의 masked self-attention도 encoder의 self-attention과 동일하게 Q, K, V는 모두 같은 값으로 들어갑니다. 그래서 이름도 self-attention인 것이지요.
                        즉 decoder의 input이 자기자신과 유사도를 계산하여 스스로 가중치를 찾고, 그 가중치를 또 스스로에게 곱한다고 생각하면 됩니다.</span>
                        <ul>
                            <li>Query: Decoder 벡터</li>
                            <li>Key: Decoder 벡터</li>
                            <li>Value: Decoder 벡터</li>
                        </ul>
                        아래 그림은 encoder에서 설명한 Q, K, V를 이용한 attention 연산 중 sclaed dot-product 하는 과정입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_GRFexHyBTyxARfgUVShQmI2LtnU4vt4kW-DAVjYlFfvGDk-BccMJFgqVsEG89ndIcLnPq64xZolwt2SqDAKw9Lx_4KY6bx19BVcRzh5RNAGwaDkES64na96yTD_xlUTLH3y5NmA8QiavCmMdGS_HgmCx20XYeHwbjs5wz9CphZSFOFkPKUTasvsqNEM-yOM2ssKx4GLGh6M5vFqL10yVRgvDT7-JkM-Aj7L2Dm6hmAjCIFkE-n0rKwHpIkx_oW4BREEEg8jmKe_ByfA5kJpfBE2OeMUbs9nzr_VOqZJzS-4nFxSLfZbxTDaZU6zY--ABUkZufsE4Q3-WqYyytXfD1fa6eNNKHRmHXhBAy7n22FGR7O9wDW7jLLM1WIlvH1FH45jKUJNjqm9NyA_b9KQiecKyZ1hPRPpbe6oEmbmGrj_iJF2UDt_71JbrhKjlpvXrtlWOJxd5BO3iFj743EuhpT91ufnbXlw_M6KvVb0xbnD3RaKacZZ7c0VGuAXhm6xvjh7XfCkwBPLIIfhltrd1fbYjTlodmnZGJrkCDp7C3tap0uCv9eQ-LgUVrdWKpuqm1yFxrgPEHWTZtWVEZE0i5Ap_qYGnOKS0pJgnWvseHYrxqoYKcqYD6DzMF-s8SAd1UhuaNrfnhPmVW1bXqrQ-Uk8RwLLx3q2NsSQ3tMtfQAuMVMqyuYkU6RWuvBQxVU8uEYqqAa35SN0flC5ylAxKonvNZzV1i8z6U2u9am_0WKBuldOldDO9_NBpL5tmUXUdQU0Aroj_i3YhLOjYSWsBoKYPE08oMZlf4ePjtYOT_NZqEF-oJemP10nTeRMn5gUphn61pkaed3qTIlDY7O0iRss5nUzSpbTsv7MBVjrtNEHKKp9gd7Ao45JIpF3GqoLf_8fmG5LAYff_I93KEV4FZLMPTfcKswyD71BzqrERPmFAOcJLetOW1GOkkHks2SSMeO6m_CgsfrBwEcd0s61dWizvtCMj5B4SGArxwQmOXxSFXH4DwGXoUZZjGpKt_G-x5Zne2PJhMKyrIfkiF41kkmPGmdMYRlk1dhs8oCsixuPsXZw5f8B7dXThO3db38tmVQTDjAwJXyU8MR_6fit0F112x5rKN0MncWFk4eCdSyPVeLm2SaTfJ2QLwLkvt4tYg1raGMg1ZmBIhb2AWyGXNpCD-OrUXN3A_NEP-pttlwDDmg--ebWvdvzDaQrwyLPwQQKpJgt4clHjYwmoV6VPGs3F0KIrVIQbf0GOesF0rp5UvmvTESRz-vEZgW2MToHffbqqQ1y1yIjnm7Q0auF727jY05IzOnjoKNYiUJYo6U9yTUNckWALVv4nZN-MRQx7fn3DRSoK0Dw9ao89BfT4qJyIQqWWwxmIrJX1Lo6GCG84BZamTnFFmTDZKEANEFWkmO5NuhuS0VrjqvVlB0dDYhCA_IEplDlGzB8iiwaipSO3Bl2aF9LwwyhafGq-vH0CkFKcKU6JGkkqaY1TtIuHmnTcRA2Mx54ZfjwrvpxUDS4PSEn2J6OxRIZyiqcANCEJHqXdK_JwcK" style="width: 100%;">
                        <p class="caption">Encoder의 scaled dot-product</p>
                    </div>
                    <p>
                        <br>Decoder의 masked self-attention도 위의 encoder의 self-attention과 비슷합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그렇다면 어떤 부분이 다르고, 왜 'masked'라는 이름이 붙은 것일까요?</span>
                        먼저 아래의 decoder의 masked self-attnetion의 score를 구하는 과정을 보겠습니다(<span class="highlight" style="color: rgb(0, 3, 206);">아래 그림의 예시 문장은 target인 'I am a student [EOS]'가 되어야하지만, encoder의 self-attention과 비교를 위해 encoder input 문장인 'Je suis etudient [EOS] [PAD]'로 나타내었습니다</span>).
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Hj8qmPJmPs2v-JjfUJXvyQVfoTht8NG7HZ91DWD1wRaLPCnROX4QQvJ9IA9IFmWGGYq6KQMjFMfUSlfPmCxoItb7cJn-whA-ghxyZlsu6I45QynnURU_RaxNnU-enoxlCaHhh4TZbEEmii5isZDTN_sOjo0oS1hL93II6icIf8YVetDVGNuGkNItiaXeOwTRm8ZUVABwCLqxmoU-E8t1nHS_ETa2MdNYyaZQbEbVB4ublfRYjP4bZSDhEtRNJFJSEf3p93Rt2hvd8HFDzLiI1-IvjISzgysCCxAT_4ksXZFoJXyq8E1lBnbtelMzH_xjCRLAzI_D04lD2zu725S9G2OfPQ1enJMJinKBHir_0W3XE8SIyFxC5F0x3IbwV7A-x5RmwxOSv40Yfe1nZtSa2GUvUuDinNpGh0i6GTZap1e-eyY2AFG1WYaApbMnPKUrfPh_Nb2RtztULBkz_8IRW6Rw8N8BxdWaOE7q6A1dwKyT-lfjulh97bZZspn0TAtYEe0Bt1HkWxHqUk5glzovhONGtxCLPnYszfxzz9pRqziMgPYCtklfGGWdOgU4MLdXa-KO2dVMnKdWkvGmogNCAcSlE0eDIUgWrtxQ8puPmfnhWq6rN5vOTtNl1ZmEEu--hviBLwALor7IhXJjuANgFxXgTlUrljAdyeo6WhaZGhwqD0H-rSH4F3mr_PEh3TYCsWUZjSNgq_PQ4Ukpr3sUjWywNQcA6j8WBE9lZdXWt95fy7PxosZG2MAcIQ5otMauLuxzAWypq5QXHBDAhnefuOp1BjpGUD6n-IlUm6N0wn33yldVfuY_3IlUc_7EJ-J8LrvHxW8hNwHhtOOqX6WaGdM-WMYpXpYjO7Sl_Z21Q4WxZC_6nhPgAk-C-nH8exg4Td3DJyn58X2sVD0Zi8C4hQh5UTShp-UgTan9X_nBlmPJZmTVtFIuIUYf7ESoM3fQPCvsQGRtiFW8Ucl6UFct2HL3anBk30pk42wPMG5uxaWYF9ROG0QS4PhEke84VpWP4I7WxrZlcyIFQ3Fc0li_87lqLeLk0CQqOvNKd9x1ZUbeaWI724Q1H0e-NVUKCPKxX_xvWcBt8tqd0Kwnu6ZB_2aAy6Jrvudj3h7j3h7ETwxu8fLY8c6xeTpzJRYxW6bPZ6uEVjQUVC5KWxrImGMTiGGODycHycdaKckNbbvt8pSOwSsFHuNI36qdEB9bbTUAJuTfdGbmaJDq3fCmTdOQ0DupAbt9oLspw5CD6TxB44xa3BcWzfL3QoR8Xn4RODsffMxYTS665sgayrKuy8hrTmPd3MRnPjLwQ5fHTr10nP3uc79U7ME69adfdZ9rvbCgFmXEz_v-SAfY3jK8R0rr6w8tp_mcFHHllqi6q3ieeHa2XoBYGSCf6akK95oh9o10seM7pyjnGeZB39dbsSkK22CbfeDIHJ_VvDYx058aJwCrdOL_GJi3RVT6RLS4lFL0s6tDhqAdJD7ms_epkTjg3c2wLSyQJCh34TRPwsXjdOqEJmVn1yOEkLVds0DKRVax8FaBy6Tzaq" style="width: 100%;">
                        <p class="caption">Decoder의 scaled dot-product</p>
                    </div>
                    <p>
                        <br>한눈에 봐도 무엇이 다른지 볼 수 있습니다. 바로 masking이 encoder self-attention과 다릅니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">Transformer는 input 토큰들을 한 번에 넣어 계산하는 병렬 방식의 모델입니다.
                        따라서 모델에 토큰이 들어가게 되면 각각의 토큰은 자기 자신 이외에도 모든 토큰을 다 참조할 수 있습니다.</span>
                        하지만 우리가 일반적으로 아는 RNN 계열의 모델들은 이전 time-step들을 바탕으로 다음 time-step을 예측하는 방식인 autoregressive 방식으로 학습하며, 이는 우리가 inference 하기 위해서 필수적입니다.
                        그리고 이러한 모델들은 현재 시점을 예측하기 위해 과거에 나왔던 모든 representation을 볼 수 있습니다.
                        
                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">하지만 transformer는 이전 토큰뿐만 아니라 미래의 토큰도 볼 수 있는 모델이기 때문에 attention을 할 때 자기 자신 이후의 토큰을 보지 못하도록 masking을 하는 것입니다.
                        이렇게 간단한 트릭으로 autoregressive으로 학습하는 효과를 낸 것이지요.</span>

                        <br><br>이렇게 decoder self-attention을 할 때는 미래에 나올 토큰을 참조하지 못하도록 하며 이를 <span class="highlight" style="color: rgb(0, 3, 206);">causality</span>라고 부르며, 이 마스크를 <span class="highlight" style="color: rgb(0, 3, 206);">causal mask</span>라고 부릅니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 물론 encoder self-attention과 마찬가지로 decoder의 masked self-attention에서도 Key의 [PAD] 토큰을 마스킹 합니다.</span>
                        또한 encoder와 동일하게 Query의 [PAD]에 해당하는 부분은 masking하지 않습니다.

                        <br><br>그리고 마지막으로 decoder의 masked self-attention도 encoder의 self-attention과 동일하게 여러개의 head를 바탕으로 multi-head attention을 수행합니다.
                        Multi-head attention의 logic은 encoder의 self-attention과 동일하기 때문에 encoder self-attention 설명을 참고하시기 바랍니다.
                        
                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">결론적으로 decoder의 masked self-attention은 encoder의 self-attention과 연산 과정은 완전히 동일하지만 유일하게 다른 부분이 masking 방식이 다른 것입니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EauoMMAgbpEiN1QR9vqvWWBereU-2BQu4KynP-v9-1rIxnWLogX2OZlgS96EKK75M4E9wKmjMHZUj74a66zgeJN7Ch6dW82fGboSAf3RGUMXQlxpP1qwsoJhKcrSF2m0v-pa9t0fYgnId8lHtLw6fKkGJob22AuwsD2WykRstGqT-QrQqoEVFnxVQBzFW4Fqm6bdfNbW1Ewd4dVMRucWQomkcOOAC-NogDIXiAlwvjM2tHuBDOPpXMDaf0DSX7zfYvTjVewuvHyMNjHyKvEOih7pcG0O4YRNDOJQDSqspg8Fl2GZF8I5842BO8uOkh8xZBwLAY_PiQDE8RXrk86yDbKJX86NEnXV81xeidQcI-gbmO2oGy7J3EH91vQDIqJIoSFrnl6h7SIkwsX3We7SGARQtJ965Ta8xp08uy16WtSLiN2yxmztmTkGXVt4s-k5deKolQMUb-13Kufff3lDOpInkW5Jokfr1AYS_DfdPDHAuv9sNTI_xQrBLcGv2ZXQbBzJmFi0SX3-qpSjvaSymsPy8Fr-e_a_2PeS0GWJeMcpyV-ff904DAQU2endnlyprHuDeR-Ppzwb2VLKITqul4fxR4Q5SZS5om8oAjRAMruhxDWdoOVUY1PUXjqSw76F8DO-1JHjLyR7S38vu4ZPVRjF2gBDYAerkOAJH0e_Scw7ZRTZcMNhFFwDsIFcHe9LSUHbSC2XYD2NEosQcddvlS9Z93NLt100gYc_yuPWFZTiZj-xZJ333XWwBGB8ywFlAMxawVHgZ_KmUm3xJ1reVTGSTmbcUcae7YTWaIkQyxk6vI62khlEFHOCmF7QuYUIVK3HTb00z_EwsOvhrb2jdorFAUS_O1ss32WyqKb86XJum48bCtRcFrLdL0Wcru9TZIRzmlyyHhtkzxeETF74hokdPcMCmL7uvTlP-tho5MlVa6t_ue2IFgNE4iVchAVEXgJIw0ACvgFqZTzvaPeN4JIHCIzs2djl9w9rvxtNwSuTJ6JCMtYOQnOVRTH7vqJIdmMajAL58M7PRvymXB0Z32r0hSe8SeSpzluUQS5mJYZhrY7KUadQk5SQZNdp5oWHKn4klEYHHm8736sDNqI1CUS0LsdGpBgvqySUkVTqeguHUCk7R02w1WXqSVBSC_Yc7TsKP7t5xsX60GJ_6diXOgXXsjiPpGN7M2IgXn5ZXtOpCnn8V1LWROkaTA9P_V-bIJ1v_JGf6Up3AnAfwyeNqtbuURZNdWlxxGOXK8Zqdh_KLwEjeFEZvXH3AfxuthPxtEVP5AqtpiNcE7hqwY9JUm70DDKLm078rW82zzKTmn23X0GM8sIhdYDArK1JAHPQ9PnFCQKDBaqan_TBqYUziC0zlb9tU-xz3Ms_FXfRJTHAneZOjq5MraJkYu3Op8ovIz1b289GB-sWqnQKsAcDB7TFQc0GqzQqC9wux_6UzImspg_IeSXhdExkm6CR1GcaLxQfrS1xh0fJ7KK8775u_aT61_n45RoWMynx7Wi7dzjKrovUmXGJTpOVAJMrjpMUvX-91hfYBZ" style="width: 100%;">
                        <p class="caption">Decoder의 multi-head attention (3개의 head)</p>
                    </div>
                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>Add &amp; Layer Normalization</b></span>
                        <br>Decoder의 'Add &amp; Layer Normalization' 부분은 encoder의 'Add &amp; Layer Normalization' 부분과 동일하므로 encoder 설명을 참고 바랍니다.
                    </p>

                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>Encoder-Decoder Attention</b></span>
                        <br>우리가 이때까지 봤던 attention은 encoder와 decoder의 self-attention이었습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">하지만 transformer도 seq2seq 모델인데 source도 자기들끼리, target도 자기들끼리 self-attention을 하면 source와 target의 연관성을 알 수 없는 노릇이겠죠.
                        뿐만 아니라 우리가 잘 알고 있듯이 seq2seq 모델이라 함은, encoder의 output을 decoder로 넘겨주어야 하는 과정이 있어야하는데 현재까지는 나오지 않았습니다.
                        바로 이러한 과정이 transformer의 encoder-decoder attention에서 이루어집니다.</span>
                        
                        <br><br>따라서 self-attention의 Query, Key, Value가 이때까지 다 자기자신이었던 것과 다르게 encoder-decoder attention에서는 아래처럼 구성됩니다.
                        <ul>
                            <li>Query: Decoder 벡터</li>
                            <li>Key: Encoder 벡터</li>
                            <li>Value: Encoder 벡터</li>
                        </ul>
                        이는 곰곰이 생각해보면 당연합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">당연히 모델이 학습한 후 source를 바탕으로 inference 해야하는 것은 target입니다.
                        따라서 모델 학습 시에는 target이 source와 어떤 연관성을 갖는지 찾는 것이 중요합니다.
                        이에따라 Query는 decoder 벡터, Key는 encoder 벡터가 되는 것입니다. 그리고 이렇게 encoder 벡터에 대해 attention score를 얻었으니 당연히 encoder 벡터에 곱해주어야 하겠죠.
                        따라서 Value도 encoder 벡터가 되는 것입니다.</span>

                        <br><br>아래는 encoder-decoder attention을 나타낸 그림입니다.
                        '[SOS] I am a student'의 target 문장이 Query, 'Je suis etudiant [EOS] [PAD]'의 source 문장이 Key, Value에 들어가는 것을 볼 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 encoder-decoder attention의 Key는 encoder 벡터가 들어가므로, encoder의 self-attention의 [PAD] 토큰 마스크가 동일하게 씌워지는 것을 볼 수 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Haida3SeNUbzf3iIXTgFO7wk1nl2cGutXjEeD_pvPgsvOjK2neY3aw8LbBHWZQX3zF1uRH2gtDeCaCjDwXVGGaMkTT6vSW1w1VLOhgF5Tsy3YG3tlt8KNbWLK4WKNEjiESgpDGq_InCX05BgPlz5Qgw2wdc8b8pEG3KyE6rKvOsTNppcHNADELkEZKcBFAe5u59OZ4C8GVDF06AYF38PtrDDfmCNbZUj3wJuHlpEyLf4IVZZ_zS2CTXB-Qj-DPWbrylGDuF1jGmVmNuACPrkfOSJEtNEvK0i6KEr-wBpQgE837lrfRpFz8Fcxa4TW6KL2hUlWHVlEjmDP2JPvoba0Ii87itNh_z79FnQM1QguKdBgLujnL8nx3edzAmm94yickLOfj3Q4c3YN5jx-TXU06l9aTONE5rzfkrWdmo5BCUTKN1a7LK9NqGpbi00KEphuFtkwTkX8pZgZh0DgOs-51VJJJ2QngpxzjsJv9nNXki2s3dguSao1BD3-Fe8AJzWolFIxw35WPoODIaIlZwE1OQnaQnr_ZnFW7BZm3MNNjcswdTWR7mp2OKPMI6IwYruUSQjs_7X0jR5lW7Hz1bMKWNNluktofPtgpZJzNU4pSW6mCZGpnpxiw-_o4Mlq-XQGxnxpRz0FN964Os7Wpy_Q4gpN_prhXD4PPOMScytvZkCngMOhF7Nwg4XVYM2XSB_e_1tjmI75NEG73LSDfMwJpcptLBWE2TgNP4aInOPOJyNFLgFaMRT_cKUoEW5mHgNmUVKYwC-X9obqwlz78iaWCRA5HM7_78aCr4IYmtUsIuDLdlZWJtJMfT5tWpp6Q4NokyXhpa-aFXyeH_tT9yaFn94u3Dfvjrse4m6-ymX-bO2-So3nutBlZKN2EEAQR_Dou99eDpdxijhBBInA_lWrSSxJe9raSQJEMO7HrMO2QWcLFFSlTFbkiMNgdstytZm1_XV2gp5Y15Dk57ttOd0fLiQG4CbMuXkA5--hsOrlmjjfoREajLLjiP1Q8nDFdX9vXmDV1VVeNVXXD9ybrkzmPYT8Ol_mrd9vWfzcKWJgRqqFHubQHRSTIPzYdJEh3H_FLU7_laJAfADoz6GqCNUF2Fh_3d86SejAj3sF3AhuVyFR7Q-fSAKbOr-jSQbUq-f6gV4y6OtnUQ9YoHrdwYj-aCR8moFL5j3N3Aul2xq7FrMJWZAq2i2HvdXVFzgjihUYWuwWhkdT5YolkXIEFD04TVgHai7uhVLjntwym0Lau4w6YBe0hfPeKIj_E2NIRWmF2EEWn-YpYnoYKGy7qlM1qEGFO8xdt3Ja_M0vUuBp_GmtrxDdbM5SOzpNRivHjvj4aUoDEqlFP6rgmkRgcLQLOch3U2kL7K4r-2YvT2KWE7JXJWg0GHQshEMP324wdL2XepvbtavfmLtPdhC9TqOGtQY-6Vqu0IsHo2N4r1ueWvTYnvxi6QqgIvO5-tK_JOIErhH7BDmVJVCQkKI9gt-a3XgAalFtDhZrDgGBiF_LTCLskxOj-7PHwdR4gdNjD_akpbyCdU9qk" style="width: 100%;">
                        <p class="caption">Decoder의 encoder-decoder attention</p>
                    </div>
                    <p>
                        <br>그리고 여기서도 위에서 소개한 self-attention들과 마찬가지로 여러개의 head를 가지고 동일하게 multi-head attention을 수행합니다.
                    </p>


                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>Position-Wise Feed Forward Network</b></span>
                        <br>Decoder의 'Position-Wise Feed Forward Network' 부분은 encoder의 'Position-Wise Feed Forward Network' 부분과 동일하므로 encoder 설명을 참고 바랍니다.
                    </p>
                    



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>Transformer의 의의</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>처리 속도와 성능 향상</b></span>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">기존 자연어 처리, 언어 모델링을 위해 사용되던 RNN 계열의 모델들은 순차적으로 다음 step을 예측하기 때문에 병렬처리에 한계가 있었습니다.
                        이에따라 모델의 학습 속도도 느릴 수밖에 없었습니다.</span>
                        하지만 transformer는 attention을 활용한 병렬처리 모델로써, 기존 RNN 계열의 모델보다 훨씬 빠른 속도를 보여줍니다.
                        또한 transformer의 self-attention time complexity도 RNN보다 낮은 수치를 보여줍니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 학습된 모델을 바탕으로 inference 하기 위해서는 autoregressive 학습이 필수적인데, 이렇나 효과를 내기 위해서 decoder 부분에 causal mask를 적용하였습니다.</span>
                        
                        <br><br>그리고 기계 번역, 대화 생성 등 다양한 분야에서 검증 되었듯이 그 성능은 엄청납니다.
                        논문이 맨 처음 나왔을 때 거의 모든 state of the art (SOTA)를 달성한 모델이기도 합니다.

                        <br><br>즉 속도와 성능 두 마리의 토끼를 모두 잡은 모델입니다.


                        <br><br><br><span style="font-size: 20px;"><b>Long Sequence Dependency</b></span>
                        <br>기존 RNN 모델은 문장이 길어질수록 초반에 들어왔던 input의 memory가 사라지고, gradient vanishing 문제가 있었습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">따라서 이러한 문제를 해결하기 위해 등장한 모델이 바로 LSTM이죠. LSTM은 기억 게이트, 삭제 게이트 등 여러 게이트를 활용해서 long sequence dependency를 잘 학습할 수 있도록 개선하였습니다.
                        그럼에도 불구하고 여전히 LSTM도 RNN과 동일한 문제를 겪습니다. 
                        하지만 transformer의 attention 과정은 모든 토큰을 동시다발적으로 참고할 수 있고, 가중치를 계산합니다. 이러한 과정을 통해서 기존에 해결하기 어려웠던 long sequence dependency 문제를 해결합니다.</span>

                        <br><br><br><span style="font-size: 20px;"><b>Inductive Bias Free</b></span>
                        <br>먼저 inductive bias는 무엇일까요?
                        <span class="highlight" style="color: rgb(0, 3, 206);">Inductive bias는 모델이 주어진 task에 대해 일반화를 더 잘하고, 성능을 높이기 위해서 추가된 가정(additional assumtion)이라고 볼 수 있습니다.
                        즉 훈련 시 보지 못했던 데이터에 대해서 좀 더 잘 대응할 수 있도록 하는 추가적인 가정인 것이지요.</span>
                        <br><br>사실 inductive bias의 대표적 예로는 우리가 흔히 잘 알고 있는 CNN이 있습니다. CNN의 근본은 이미지 처리를 위한 모델입니다.
                        이미지의 특정 픽셀의 주위 픽셀들은 연관성이 높을 것이라고 가정하고 locality 특성을 찾기 위해서 kernel이 움직이는 방식이지요. 이것 또한 inductive bias라고 볼 수 있습니다.
                        이와 비슷하게 RNN 계열의 모델도 sequence 데이터를 가정하고 설계된 모델이기 때문에 inductive bias한 모델이라고 볼 수 있습니다.

                        <br><br>이렇게 inductive bias는 모종의 가정이기 때문에 inductive bias가 강할수록 적은 데이터에 대해서 성능이 좋은 경향을 보입니다.
                        하지만 inductive bias가 너무 강하게 되면 훈련 시 보지 못했던 데이터가 들어왔을 때 대응 성능이 좋지 않을 수 있습니다.
                        다른 말로 모종의 가정을 하고 학습을 했는데, 가정과 약간 어긋나는 데이터가 test 시 들어올 때 성능이 안 좋다는 의미입니다.
                        
                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">Transformer가 이러한 inductive bias가 적은 모델입니다. 따라서 적은 데이터를 처리하는 데 성능 향상이 더딜 수 있지만, 빅데이터가 들어왔을 때 진가를 발휘하는 것이지요.</span>
                        즉 성능 향상을 위해 transformer는 CNN 보다 많은 데이터 필요하며, global feature를 추출해야한다면 transformer와 같이 inductive bias가 적은 모델을 사용하면 됩니다.
                        그리고 이러한 증거로 ViT는 적은 데이터에 대해 성능이 CNN보다 떨어지지만, 빅데이터로 학습 시켰을 때는 엄청난 성능을 내게 됩니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">결국 우리는 가진 데이터를 잘 고려하여 성능 향상과 global feature 학습 사이의 trade-off를 잘 조절 하는 것이 중요합니다.</span>


                        <br><br><br><span style="font-size: 20px;"><b>범용성</b></span>
                        <br>Transformer는 등장하자마자 NLP에서 엄청난 성능을 보여주었습니다.
                        따라서 transformer에서 파생된 아주 많은 모델들이 있으며(e.g. BERT, GPT, BART), 현재는 transformer 없이는 많은 연구가 불가능하다고 해도 과언이 아닙니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">하지만 요즘에는 NLP만 이러한 모델에 적용하는 것을 떠나 다양한 분야의 task를 BERT 등의 transformer 기반 모델에 적용하는 연구가 많이 이루어지고 있습니다.
                        이러한 것이 가능한 이유는 inductive bias가 적기 때문에 범용적으로 많은 분야에 확장될 수 있는 것입니다.</span>
                        이렇듯 transformer는 분야를 막론하고 좋은 성능을 내주는 모델입니다.
                    </p>

                    
                    <p>
                        <br><br><br>이 글을 통해 transformer의 구조에 대해 알게 되었으니, 첫 번째 그림인 논문에서 사용된 transformer의 구조 그림을 보면 이제 이해가 될 것입니다.
                        그리고 transformer는 현재까지도 활발히 연구되고 있는 모델입니다. AI 분야에서 transformer가 등장한 후 연구의 trend가 많이 바뀌었고 아직까지도 영향을 주고 있다는 점이 정말 대단한 것 같습니다.
                        다음에는 transformer를 바탕으로 기계 번역 모델을 학습해보겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#Transformer&emsp;#Self-Attention&emsp;#CausalMask
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('Transformer 첫 게시물 입니다.\n\nThis is the first post of Transformer.')" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('transformer2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>Transformer를 이용한 WMT'14, IWSLT'14 (En-De) 기계 번역</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>