<!DOCTYPE html>
<html>
    <head>
        <title>Vision Transformer (ViT)</title>
        <meta name="description" content="ViT 모델에 대해 설명합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/vit1.html" />
        <meta property="og:title" content="Vision Transformer (ViT)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="ViT 모델에 대해 설명합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/AK0iWDwY3fC2YaPbfYGVrURWUKk5pGQ_ACe6XYejDNWB-VAuARAXsuAGfDca-Y7Vv69jzrspZvs8Oq1Gd1zf4Nq1XG6DVZlaG1FuBf7fpfGCHo4P9ZMJ_W3RCNoQbAP2tsQNYy94ehpHqMwsUt_3Sg4ytU0j9rCnvwzBMbb6rvVVlL8uhmwdtk8ei8Tf7L2eY3qyhxHi6gzcuqSHDt4YvNpw_myq4VwGcYyUf-f0hT4kYOLsGmW7ZJGysuRmwxW3W0BMvpRH2DumFi-8W1cXLXNmuPK8PFcENzgqZAHDFKgT9WI8HVwDF5z3Kbh9pE98eMfU_n1Zb1SnBNgV1eN4Im3HHkV_nX5obcU80szr98ixD0qECwhfdhqWvza3E4VrIdMYd1Gnyi4C887MjHhueWsBW1u3M6WsN-XFKSd6S65nf-K-3p7OPmZCKW90foYhqDAb0nM8OgeW7SG8Xa1QDNK8OGHK8GsdFlhWv9_ZZ-Bxu4hGbhpMPpLXP8yHc4LUjXxUHBAXRy-8vxN0NTD3BB-6ZpWIPloJRgLHorsTP1f9a-b1JZDfSum_xz6JZmS-Bup4cNAEx73-BHkwkHKNBaKVniQNJYnviCF6KT8enniqImmy-zx6kCvqsi5wyUKSKrzwrA_8MOJ3mQPgjEYAVyYA6o66FMkczvb096sqe6it5pxOVNR_6ntcBizkn9mMIhcsm_mGGrWyiVSrSEMvK5biP2tHDevRCoPJVUwBERvl21kBC6e1o_Z_bS7pmI3NH7QP61tQ6egAfq6g_7Qir1J24qjyE9UbTcSDFesYcS-67up4l0asRS05qdevVHKaAqz-_77BSlBvpQ7oCGiH8h5gvOoWCCt-bfceuJd9ocZttEaW6Er4pNxVyQ1jFLf3paWl0AtCTwfiUsXGa_mpWNTF4LRMpzqxmse_um2eG3lBxfk7H6lfP3oT8NN1wg21yyj-7uRUaC9WwgrWLCUkf6Rkv9RLtOzDbZZUvH-HAoFPEGaRlDIOs1uCM_953K_r75hV1t6EN9Si1GHBSTo1dJsMTzLgG5Dxb4sp3kZpAz5kUYcCesPqDNJ66g3amGQefSKJbGbHZ-VLt-mSBYESpDFTRU5wXBfZUHkXLvPMLsz_dMKXoM2H0aNbEoXULARTXCss4xZXOvLQsj4Sf-Cpcm9P_MxJ1qqvwTWDw1laJFDWD6D20Ea1fpCbZyEPRjpOhPLUOeyWYLwCM-StVu-p9VNdc-3rU2w2qnCieBj51vEXJDD4UYsl9D_doToaE312yUadEaBkpfaDvteGWzbs5II4OW8THrPQKGXhdFM_xcFvKBfnERqqGIUH1E_UCpdQcpwxUibtm1ezFLAueKg4icc01FVCJ26WoJADGSjDREShndQgG9D0TpK9u8cqALfZXytL-t-8HSCP5PYj51Fkv86G71imwMcPS0oFnZi0ApSA18186ZlU3UoPGI_J_PlTyXsxom1q2IPOEwQB_qjGRMKCXbtw57BPd_r6S_CX_zM2rcrBcR72ClffPaa-nhsVACipS5bSL9F8s6U" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Vision Transformer (ViT) / 1. Vision Transformer (ViT)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/AK0iWDwY3fC2YaPbfYGVrURWUKk5pGQ_ACe6XYejDNWB-VAuARAXsuAGfDca-Y7Vv69jzrspZvs8Oq1Gd1zf4Nq1XG6DVZlaG1FuBf7fpfGCHo4P9ZMJ_W3RCNoQbAP2tsQNYy94ehpHqMwsUt_3Sg4ytU0j9rCnvwzBMbb6rvVVlL8uhmwdtk8ei8Tf7L2eY3qyhxHi6gzcuqSHDt4YvNpw_myq4VwGcYyUf-f0hT4kYOLsGmW7ZJGysuRmwxW3W0BMvpRH2DumFi-8W1cXLXNmuPK8PFcENzgqZAHDFKgT9WI8HVwDF5z3Kbh9pE98eMfU_n1Zb1SnBNgV1eN4Im3HHkV_nX5obcU80szr98ixD0qECwhfdhqWvza3E4VrIdMYd1Gnyi4C887MjHhueWsBW1u3M6WsN-XFKSd6S65nf-K-3p7OPmZCKW90foYhqDAb0nM8OgeW7SG8Xa1QDNK8OGHK8GsdFlhWv9_ZZ-Bxu4hGbhpMPpLXP8yHc4LUjXxUHBAXRy-8vxN0NTD3BB-6ZpWIPloJRgLHorsTP1f9a-b1JZDfSum_xz6JZmS-Bup4cNAEx73-BHkwkHKNBaKVniQNJYnviCF6KT8enniqImmy-zx6kCvqsi5wyUKSKrzwrA_8MOJ3mQPgjEYAVyYA6o66FMkczvb096sqe6it5pxOVNR_6ntcBizkn9mMIhcsm_mGGrWyiVSrSEMvK5biP2tHDevRCoPJVUwBERvl21kBC6e1o_Z_bS7pmI3NH7QP61tQ6egAfq6g_7Qir1J24qjyE9UbTcSDFesYcS-67up4l0asRS05qdevVHKaAqz-_77BSlBvpQ7oCGiH8h5gvOoWCCt-bfceuJd9ocZttEaW6Er4pNxVyQ1jFLf3paWl0AtCTwfiUsXGa_mpWNTF4LRMpzqxmse_um2eG3lBxfk7H6lfP3oT8NN1wg21yyj-7uRUaC9WwgrWLCUkf6Rkv9RLtOzDbZZUvH-HAoFPEGaRlDIOs1uCM_953K_r75hV1t6EN9Si1GHBSTo1dJsMTzLgG5Dxb4sp3kZpAz5kUYcCesPqDNJ66g3amGQefSKJbGbHZ-VLt-mSBYESpDFTRU5wXBfZUHkXLvPMLsz_dMKXoM2H0aNbEoXULARTXCss4xZXOvLQsj4Sf-Cpcm9P_MxJ1qqvwTWDw1laJFDWD6D20Ea1fpCbZyEPRjpOhPLUOeyWYLwCM-StVu-p9VNdc-3rU2w2qnCieBj51vEXJDD4UYsl9D_doToaE312yUadEaBkpfaDvteGWzbs5II4OW8THrPQKGXhdFM_xcFvKBfnERqqGIUH1E_UCpdQcpwxUibtm1ezFLAueKg4icc01FVCJ26WoJADGSjDREShndQgG9D0TpK9u8cqALfZXytL-t-8HSCP5PYj51Fkv86G71imwMcPS0oFnZi0ApSA18186ZlU3UoPGI_J_PlTyXsxom1q2IPOEwQB_qjGRMKCXbtw57BPd_r6S_CX_zM2rcrBcR72ClffPaa-nhsVACipS5bSL9F8s6U);">
                    <div>
                        <span class="mainTitle">Vision Transformer (ViT)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2023.11.03</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번에 소개할 논문은 바로 2021년 ICLR에 accept된 Google의 Vision Transformer (ViT)입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이미지를 Transformer 모델을 이용해서 task를 수행한 모델이며, 현재는 아주 일반적인 방법론이지만 당시에는 엄청난 주목을 받은 논문이었습니다.</span>
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/2010.11929.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">ViT 논문</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>ViT의 동기</li>
                            <li>ViT 구조 및 인풋</li>
                            <li>ViT 결과</li>
                        </ol>
                        <br><br>
                        그리고 아래는 Transformer 설명글입니다.
                    </p>
                    <div class="link">
                        <a onclick="pjaxPage('transformer1.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Transformer 설명글</a>
                    </div>



                    <h1 class="subHead">ViT</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>ViT의 동기</span><br>
                        <span>Motivation of the ViT</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        ViT는 이미지의 당시에 원칙이었던 이미지를 CNN으로 푸는 방법 대신 Transformer로 접근하는 방법에 대한 논문입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">ViT 이후 현재는 이미지 뿐 아니라 음성 등 Transformer 구조를 안쓰는 분야가 없을 수준이지만 당시에는 아주 획기적이었습니다.</span>

                        
                        <br><br>먼저 저자는 Transformer의 장점을 아래처럼 언급하며, Transformer에 이미지를 적용하는 타당성을 부여합니다.
                        <ul>
                            <li>효율적인 연산(Computational Efficiency)</li>
                            <li>확장성(Scalability)</li>
                        </ul>
                        <span class="highlight" style="color: rgb(0, 3, 206);">실제로 자연어 분야에서 Transformer의 확장성은 매우 용이합니다.
                        Max length를 자원이 무한하다면 무한대로 늘릴 수도 있으며, 각 block별 i/o (input/output) 차원의 크기가 동일하기 때문이죠.</span>
                        따라서 저자는 이러한 이유로 이미지를 Transformer에 적용해보고자 했습니다. 그리고 실제로 학습 과정에서 CNN보다 자원의 소모가 적었다고 하죠.
                    </p>



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>ViT 구조 및 인풋</span><br>
                        <span>ViT Architecture and Input</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        아래는 ViT의 구조와 이미지의 인풋 처리 방법입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AK0iWDwY3fC2YaPbfYGVrURWUKk5pGQ_ACe6XYejDNWB-VAuARAXsuAGfDca-Y7Vv69jzrspZvs8Oq1Gd1zf4Nq1XG6DVZlaG1FuBf7fpfGCHo4P9ZMJ_W3RCNoQbAP2tsQNYy94ehpHqMwsUt_3Sg4ytU0j9rCnvwzBMbb6rvVVlL8uhmwdtk8ei8Tf7L2eY3qyhxHi6gzcuqSHDt4YvNpw_myq4VwGcYyUf-f0hT4kYOLsGmW7ZJGysuRmwxW3W0BMvpRH2DumFi-8W1cXLXNmuPK8PFcENzgqZAHDFKgT9WI8HVwDF5z3Kbh9pE98eMfU_n1Zb1SnBNgV1eN4Im3HHkV_nX5obcU80szr98ixD0qECwhfdhqWvza3E4VrIdMYd1Gnyi4C887MjHhueWsBW1u3M6WsN-XFKSd6S65nf-K-3p7OPmZCKW90foYhqDAb0nM8OgeW7SG8Xa1QDNK8OGHK8GsdFlhWv9_ZZ-Bxu4hGbhpMPpLXP8yHc4LUjXxUHBAXRy-8vxN0NTD3BB-6ZpWIPloJRgLHorsTP1f9a-b1JZDfSum_xz6JZmS-Bup4cNAEx73-BHkwkHKNBaKVniQNJYnviCF6KT8enniqImmy-zx6kCvqsi5wyUKSKrzwrA_8MOJ3mQPgjEYAVyYA6o66FMkczvb096sqe6it5pxOVNR_6ntcBizkn9mMIhcsm_mGGrWyiVSrSEMvK5biP2tHDevRCoPJVUwBERvl21kBC6e1o_Z_bS7pmI3NH7QP61tQ6egAfq6g_7Qir1J24qjyE9UbTcSDFesYcS-67up4l0asRS05qdevVHKaAqz-_77BSlBvpQ7oCGiH8h5gvOoWCCt-bfceuJd9ocZttEaW6Er4pNxVyQ1jFLf3paWl0AtCTwfiUsXGa_mpWNTF4LRMpzqxmse_um2eG3lBxfk7H6lfP3oT8NN1wg21yyj-7uRUaC9WwgrWLCUkf6Rkv9RLtOzDbZZUvH-HAoFPEGaRlDIOs1uCM_953K_r75hV1t6EN9Si1GHBSTo1dJsMTzLgG5Dxb4sp3kZpAz5kUYcCesPqDNJ66g3amGQefSKJbGbHZ-VLt-mSBYESpDFTRU5wXBfZUHkXLvPMLsz_dMKXoM2H0aNbEoXULARTXCss4xZXOvLQsj4Sf-Cpcm9P_MxJ1qqvwTWDw1laJFDWD6D20Ea1fpCbZyEPRjpOhPLUOeyWYLwCM-StVu-p9VNdc-3rU2w2qnCieBj51vEXJDD4UYsl9D_doToaE312yUadEaBkpfaDvteGWzbs5II4OW8THrPQKGXhdFM_xcFvKBfnERqqGIUH1E_UCpdQcpwxUibtm1ezFLAueKg4icc01FVCJ26WoJADGSjDREShndQgG9D0TpK9u8cqALfZXytL-t-8HSCP5PYj51Fkv86G71imwMcPS0oFnZi0ApSA18186ZlU3UoPGI_J_PlTyXsxom1q2IPOEwQB_qjGRMKCXbtw57BPd_r6S_CX_zM2rcrBcR72ClffPaa-nhsVACipS5bSL9F8s6U" style="width: 100%;">
                        <p class="caption">ViT 구조, 출처: ViT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>1. 모델 구조</b></span>
                        <br>ViT는 Transformer의 encoder를 사용합니다.
                        

                        <br><br><span style="font-size: 20px;"><b>2. Image Input</b></span>
                        <ol>
                            <li>Original Architecture</li>
                                이미지는 \(B \times C \times H \times W\) (batch x channel x height x width)의 이미지에서 \(B \times L \times (P * P * C)\) (batch x # of Patch x (Patch size * Patch size * Channel)) 크기로 변경하여 넣어줍니다.
                                <span class="highlight" style="color: rgb(0, 3, 206);">이는 자연어를 (batch x sequnece) 크기로 넣는 방식과 동일하며, 자연어가 임베딩되어 hidden dimension을 가져 최종적으로 (batch x sequence x hidden)의 데이터로 변하여 처리되는 것과 같은 컨셉이라고 볼 수 있습니다.</span>    
                            <li>Hybrid Architecture</li>
                                하이브리드 구조는 CNN 임베딩을 거친 이미지 결과를 위와 똑같이 (batch x sequence x hidden)의 차원으로 변화하여 Transformer 인코더 구조에 넣는 방식입니다.    
                        </ol>

                        <br><br><span style="font-size: 20px;"><b>3. 학습</b></span>
                        <br>ViT는 Transformer 기반의 모델이기 때문에 적은 데이터로 좋은 성능을 내기가 어렵다고 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">Transformer는 inductive bias가 부족하기 때문에 충분한 양의 데이터로 학습하지 않으면 일반화가 어렵기 때문입니다.</span>
                        따라서 구글은 6억장으로 pre-training한 모델을 공개하였으며, 추후 활용한다면 이 모델을 사용하는 것이 좋겠죠.


                    </p>


                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>ViT 결과</span><br>
                        <span>Results of ViT</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        저자들은 3종류의 크기가 다른 ViT 실험을 진행하였습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AK0iWDy9OqRF2OwCRUu3IJoQGnrm_OFHSURtNO_aleAyzNKQm-gPmuG_bS7S3jegFv70Z3g0wFVnXf4tNVSXhk_hTRGnMcvyvnNno6LI_c8dGvVxHfqEme-1M7T28_fDgzsixqlAlFvkz2A5ycqbawzgtri4y9aXg99-Bhn2wkMog2jxJxhxBDU52VmF-qsRmo3jvTXaVGDTPjJmodH3y--0F-3kcJN2JJ9iAbeVY3zisPWfeL0rWiw7OoATaSD8BO9THDKx6-Tfla07au4AbafIpmrDUj_gc2nKHMOozlWTjwuZ3t3wNZL9OozTUCMFtrsBKyZr254cslbmY5HD0jBuqrqj_SnYGHOL-qiOK1RHC_Hx9ATe1rxJANaZXWZQCccgJspXV0cOutWUI1haHVNICCURn0BTONLsjhDkorNj7ri0ldH6Cqq216U6mp6zqXB47cKjLuRaroenOH-syDm2HnMkargdK3L7uzcLlq1qPI29oKj5rCKZVLd3jRKeUPOajqHplm9NZem0n0GkDbIFz7-ewO0TeK_API0B82V5wbCYHOGkpdyN2up24LZq8nDzhoEzKBDBhI6Khmj5WzsonzuihjNh5M1tgWARI2apvidRV80qfaqxA_vtEIpjuqgNxcUy-Rnnrv00uCA_Ut8Vp9IAaCdF1_4FCXn6PNFqb6xFZ0U_1arlx7w_3Fy0JY9RmaHHVgLuWW3r-vhkCw0uGqDLbLq7EU45HyXfJiGM3P9Wr6lkOi-ewrb63H3lsUNGpCW3xAXU397-NelZfzS2F0s0b1succSaC3neYYUNBMVpiA5Fbr5Tqu_KxDuSNIUhJ_uoP5O0xOndpWEcr72WjDNyATMU3dQx-rh_yXFsuyvW9m7qr6dICq2Ut1PxuZXG5Q2E8q0QlcJVzfiWWnNXCnBKSUijYolyYqf_MqiECPiyYUuvIGUBb3m-jhFP3HMdt9YfhNtji5CQjdcElGkjMtFEjnA5m0eT3XGLYHF5nAz2Sm2dIU72zJC-gwK-O2tghaYOS27KHBl-RDsO7AflRJoFiTrw-0bCR4m9GhcApY6ZDjOny0oHgFqCSwMZ6lMinuYHup7FKuffbspIen8zZ8FumSJURDgQwJb6T1gfe54A9VTwbL2ZyAkXlwI8diApHQilp6Zfo5YIxU654Gd6LJEw2E90mrIlqwFzBUCfNpt_2vIW-D_FrAOWCEzyo9OLp5LIBBZ4kLoNAXQgtdwqmGJR0fHFMv0fWyMKy06mWZfzy_byhAn93NsHBdUE9FmzBFTQOpQI2LuBZyeHpdLhGnke1BpKMtRYCHgPH8OjNe_-4MmYgzKE0b-ajB8xcB7FEk1b-ENICaT77Hg9gCzvKQcYFp-51guHhfv5FN-HAtZvqMdP8TfvG9e96jByzxyUqy41_yx5Etxh8O5s-b-a2q-ZlZO5uk9OEYVTHhYSWgZeVEG-Uhewte9TB7yRJgQSxWg6m2tgpmp24XZn2rvBUF8Su5vGgZVV6MN5vqNq6c3cvrm_6ph8alU3uesxSiY9czSPh4lA8Go" style="width: 100%;">
                        <p class="caption">ViT 모델 종류, 출처: ViT 논문</p>
                    </div>
                    <p>
                        <br>그리고 Big Transformer (BiT, ResNet 기반 모델), EfficientNet과 비교한 결과 여러 이미지 classificiation benchmark에서 우수한 성능을 보여주었습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AK0iWDya7aS8BT5teQmOZVxHwXrVjHYuVhIISiDNqJseGsV-nq5j2iqr7sW1udrFm4Ohv59EYO1mZ_BmsexKAWKgbezO_A3oEJaCzNzIA79mA-4O39ogjR8Gsq1BICjqSP1vaxc2W0p9VZU19TEltRtZH8Jj1Cs2vp6Okh5qLK6xckgFBbZwCUlHUshHZlrUgt7aWP-g_K04RtXgi2PqmyMjJ2LuPGCAU5p6ZTbdaEU1VBx1hhp3xNaUBVouxpnIYUpx1EXbZJ434b_llfMIX58UsGl1AtaWBbwpx7aXgESP3KcORiwz7jhekkoTTQybq6UBQVhXVXC7b68ajbGxLbGKTu7KlS3PJ-ympallM-lWhy4fYuW89JNi0Ph59HPbL27Yq6EWMAKiLrq649aNaxbOvarvDwjO8Ep9wBffWNgyWxY7rTuwmU5R018IsiaapNsPuePSp4nlDIssFnDYkI1Aa4H_Q3oHAOf-dgKSsKe-bgtPdIptMH7qlI4gcspJEZP_4DjwPW1rkIGDUAWjKXHFAItcXcXXGMeErfz7Br09kPuDQP3Sapa2i-TwMIbxt7WbmUusNVUPOZNcEdlAqYtuyJCRs9BXJq7lpMPo4XwHf4co2ykT4ioET5YwNv1HoFq9K5FXsjF7x1MGCI2HItnjyrfFgmmNNJnITgzZM8GxbXkFLjXdzhDjtzljMK_jv4XbyX9-P-D_PjTBBD76x6xFRQeddGNDgFQRn7Usn7yp6CAoZmbLKQNAvReaAsE_YsLPnCwydE0Ko780u2NbihSLNtQfkOqONa6cK2e16HodZkGVQV-_4MbYcoaRcj4ob1ebmeYgM3E38paUzWQepO1TTiJEP1j12oUoWi86773ly366tAWwoUzUcNt5MmD-pJ1PY7dcLpHFZOwDvqhnjgF80p6imFLlBgPBwcAvOPz2txl7uafQvGnSkLw3GJS7v0xbixmwVihTKT5jsISkXgGGckYYMbT19rUky67THvrnRtOtj_Bsdl4kfUAEmYZJrDh4jpa1Mu9bbtrWOHcCG2FGCTyjjFetBJU4GH6aPJaZHQH8f931TffiEl0EZ_KUtUcNZE-FE7CtKKLrnGbC_3yNnzYz1ccpyYAtlwRNyx-N1sCddqQ9o79WfLsRTGctgIOE38istiZ1rBRpAq5aimHfhl2oTa9AfJTTvjEkxdbT8sWPJDB8yYsT1FR-exTBVL74AEA24jhwpX3cF3k_CBmP3OyPdMRHyvTeH9F5I95DiHolTa2dcpqFFcf1gL10rJUmK8JX9FTN9fhP3hvt6ZK-tJxetH0xGlYPJ49XvZbetwjkCUBui95VSpnQDhoDzcxxVLO3muHoL_lVkJFCFEoNBqxhF3pdhYk-1GZfUelCB59cJAkJ5e6zGsmiVqPt6uYSkYB-gADcXo_bn1zS0O_4xd1fXmEk4gqNrzKonLUEBv3zVDDGhAJkc4jn_Z6u2bFUd_RJNkZBOQe10isHfxVxXl5AFzh3SxbfCp7djglVKpJDM1g3VIXLkaHEl1AUJGtQ0R4JhpZvMQc" style="width: 100%;">
                        <p class="caption">ViT 이미지 classification 결과, 출처: ViT 논문</p>
                    </div>
                    <p>
                        <br>또한 위에서 언급한 것 처럼, 아래 결과는 pre-training 이미지 수가 많을수록 성능이 좋아지는 것을 보여줍니다.
                        또한 신기한 것이 적은 이미지로 pre-training을 수행한 모델은 CNN 기반의 ResNet보다 성능이 낮은 것을 볼 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AK0iWDwCPmqdv44WGTPYtiaJT5gogqQMbtnG2kNozwt8f8NH7Cf8Kp-69doOL6T-xzPuVnBvvmYDY5EUALlTdMbvB5CxmC2h4qH_xTISpf7Qj0BMH3Yd6DSwqq5Se2Mg96J00b0q42VZX1eg3VesJedxyPU1Fa9yy982TMjEJ6OHuc3lrWELqkOV1VWCTEiPn2MwY_1Vnq1Mr3eDf9L9b4BOVj_ZEq4LBEESK32ksUb3-NPr28Hunxv7rVYAHdXTeg4LB2KuBEOU0Gje-S9dee9wP0zCtr0q0i5vv5EYC7I8R9oHzT1YzkmPHDz5V2UTMGphOq2jYB1zXAuYyTvFDGz8KkhAal_5vdn4GuhXXSTfeQHQjDg-r7EFZImRD928cSLzZ5fJyF2ZfN61f76BUsg3wUXPzAa46kaJRIo4HxliSnuY3KmE31g5EpgX6RpeJf1uNy8WZWOyh5sicwOpO5SNyBkiiLkIDrpX_oFpGQFDmO8NqciMMqtFHdtqpkWGi6WR3wWYXb0xWeCwk9MJLHfLn5A1VxjWWtk-Ge2SlHJ8RpH9DF5bFIAZfvFDZDSpw71UIjhtamh6qJzJIQlJJEjz04AhrTj_6pZOk8vx0ew8g0cJiA1pDSPIsV1h7PC9l__CnSgUPJ6I3mTWHLKMHQrXqWAIMzxRX0075bB62xcL8Q7XaFfZ_fYvv-5mfJfuFkSGqLuokTFabhkXj21XfFMsg3Oyiwh-ilJ-JY9cY20LgLXaSfADgoxq2Qu7OkgiHm9vHsIySex7X2FhWx0mMoaxt7KwkuTGDatYMT_DlTjVkuwpRdyEIAaGqoVIJOtaUTSuFP7wrhCEXMXUKbzE0Vz89jrnz5TLhzjL7QDlw3kFmBz0BbF9s5lb7-Pj9lBlGR877v-vnlAj1CwT55Qqg8QTNq7TJbcssmgtNaVMicsUdVy88XQDo3N6C5LPf_w6dzFYVyBjJtTjIEAC2Yjscne19rhrl4Ola2FrsNL8MRaCz_SDlzLy7l34uH4cV2b6G7PorEX5Gq1N1KQTRcZYyYruCnRa6LX06paaVyMKMcULlimquMd5i587EgeanAUTC3o-Ng9E0RP-MrhUYF8es1S6OM-gK1rSUcQE8T8x3W4ASI3rm6ywqF7ywRVrhfPe-SpBlpnd9jS2RuniIs7L3fh6vOo9yRnuZict4706RFfB3jPaLKKoYGUCFz-Wr0D3OtwVtOAb8URtoec4Yla_ywZLaD2aU97jGo1IEHa_rJty1C6YMBUnQ2ZJTlp3ikKKmWcJyTDlCfDg9C6XXyc9iAYZ2Cq8dINMVWwejOsJBOFrg34dgP-L4jypoWiz-_ytfY0VnY-HWq40LLwSfzdhgtEimthPUMd_GJvL1aYq2RNMpYI-AwHCfGXJwSD2GbKVJq3TNP67Dfs1ByGXYSiA1Cb-aRjoSILkF1-JdxnHqeDBcZYF9aV5pRHnkJy5N0gbVSHxMk7kT0-wSlOqfa8SseCNwiu3Z5cdePHaLwekza1Bks8NdVWDpMLYbcAevMls9mBpz7-hN2zHU-4" style="width: 100%;">
                        <p class="caption">ViT 성능 비교, 출처: ViT 논문</p>
                    </div>
                    <p>
                        <br>그리고 모델의 attention 가시화 결과를 보면 모델이 의미있는 곳에 집중을 하는 것을 볼 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AK0iWDxFnav5sOUD3Hh51_LFZJ43RUdKN51YrP6yB266LRepRapWKDXUGSfRsKSkV1CoIezB8fmYJfaZSSxeN-uygUlYzMFyve5cuDNWpyIOIWdRRxTyajp8ndIsE9lYPae5kXPvrJw3YQXc1-dheIIKShW-K-oi6jfRbuQ55S7vATbUSPB6zqZcV-Mk_-3nE_jCQiXRLhfK6cHy6RFYs6KXnetQeK6CEOKsAMYti4iocARwdpGBu7SQ_hrfqVjc3oWA8DszLdo6WQUPo4UICFfBwV24UKoTcv48_a1taYoRMEuF1K3UK13HlekfpAnVPQRTARQtAORSVRhCHFgMnQCNM9xxVz252gDIVjZ-DJxhGdWZPkUCy_OavUIOks9qoVkczay5HSrJ5l3CR9ma4iPmUVD8RiQf6ITk2-Ok7iNGjFQNECqY8bz7uuDhbB0mEl0Xl1brJZi5tycX-1MVYfEWB_bloLp8HRTq1TVi1AaiEc0F9g2QY2ChT45N4lMM9nCi73v_SwtnQYsXOANeIy40GjI5v6JzqYDAVclA1aZdGX1onf3EHzWVTEA-lo6c0e9FdwNGQeasTt4DXbhVlhv6ieGOlPumQs_hyId7yPbEm-vtd683cs7e_TySyvW-fzxb_yIhgebvZ4uFKVPGOHovyIIHGqtxSKS9MpZmS0RR2DsZXUwMNEpN8--tEDpQ6xjCdjrOwgSHJC4jeytqqfOqhaMS1iJjWgsJ1bmBQfnPNIg_brIUD8FhcMpYoeZrlEAPVlNK7O7aqoculiqahcUbLng6T0lmHTanFD0OAoWj5_jCVu_RWLUF1K2QOQ4t_2lI1fmnraW6GH5R_RG_a1TJqTfQlN20jMAMK08-Zk0kWECb1YuKVXEOmoV8-T2wwpRiDDh4OD2QBM8fC1xpZCl-Hks4KTpNRZz6-7b83ftIWHId1BQn5GAL66e6cKfpxml2_k7Qha34hIDehXT5LVxGqkDWhhE3nUJ584hTT82S-c7IEUpAyocrFbRlkVEfxcsvqOMYK9S7MozoMxwt3Ot9wi9vUKLpgUOaiBeCH7xQRSKCVcIOwcIAEkgMAZrQc3f72gFtY8TRtj7jHWvinskZq3N02Ec_l6LYGBkRXxFLzDzae193fFIH2yK-91swMgpvqnCG2rsTJdQpo61GK_igP6a7jcGKKW_RpyuynmAav1a906nqjZ3BZDT2eTjJL-1_2gB5gJrCKyPuNY_lqhQsDlXHW-me6aWSPvN_7cG3n2dQdVr1-NFcJrOH0luRfHnNAyWkFgMOGR8wynzLu0Z8mKW906VyTeyprQVnJT-DZAl7VrD95srMKcvwrFuHhNBoiErMjmKamfaLxUSw35sbMELuxxn20WneYRxT5nJvZNJ9XzsMYP3vhWn4sLbBVOniBQOXjZMpLSFTe602XiMSpKUCSv_qUuqVgzZXqeuteAKcYnegWf6jZt3s6ANZ70oCRVXivJ-ZlKgUCAJR42OTo1gs5gRA4tpbCCxbWyYjPuDs0dQFru9J8zVUfB0xhd8x31hUfmUmVQo" style="width: 60%;">
                        <p class="caption">ViT attention visualization, 출처: ViT 논문</p>
                    </div>
                    


                    
                    <p>
                        <br><br><br>ViT는 현재 이미지 인코더 모델의 초석이 되었으며, 현재 LMM과 MLLM이 대두된 시점에서 CNN보다도 널리 쓰인는 것 같습니다.
                        
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#ViT
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('ViT 첫 게시물 입니다.\n\nThis is the first post of ViT.');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="alert('ViT 마지막 게시물 입니다.\n\nThis is the last post of ViT.');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br></span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>