<!DOCTYPE html>
<html>
    <head>
        <title>Vision Transformer (ViT)</title>
        <meta name="description" content="ViT 모델에 대해 설명합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/vit1.html" />
        <meta property="og:title" content="Vision Transformer (ViT)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="ViT 모델에 대해 설명합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/ALs6j_HGGLfmBIfSLZ1hwWNm2reJHMxU3Xk0T4nrpg65-igR9S_I0gdtqvYd5578qfdCAbjZtkSpZIuWsHbHAKBb7p1fAvYr0CsnnzWo50Ky4FcNNv7lLwL0wqRHVHLjGi_PyxaiMDynUx6iV-ikaeMpySgjISwqEWX_eS6gby5IweidiGqq3V9SRWO4L3L1PnPZB2x8qFkkLsynriiaXLeb8z769bEsgMGKixUdi5ZqdX2dbwX9Kt4QZojrtadjMxcOE5J59pq2wwP9FRGn66r8lC5nHaiHwPDhNE8iq3ZGFYeue9EL9qAKuIXXWHUu7txSAyQ8e1wReXALw3Fdx6kywHHn0thB7d3Wy0tFUbPyRP4FszyZpQcbqei6OUkim8eU4rjE07qTcQuL3-r-Qs7oIneJa2r7Q37boYUWqwf2Ts-PAFPiA9pH8xANFZ-ms97-AGRhaJ3mSXSIJStnC9ajDQGrtazPXs6TBkYt3KLVuzwFRH7dD94NDY94zCQOFkZuhLwO5QjYgy5DF8sn3jbz7p1bpu58i8PohZ8O99EDPAz7uNJDh57RSTYvcBr8brkJr8BuaDVZ4j7r9YkIfDqINTDfejixbYXrDf7DoTJu2cGgcoyxm07JBDBh7MiGQ3JQWw3XB_baPkdUbaHsZ0abztGdIj5ulzq3eAoAETLOljJt3TSWQ_UBu2bcc-RgX6r27kuoY27EAGRR5qZ4cMK47NCQ2TBv-PFnSeX_agCf5s8ScN5Ftnd6-Rw1P2inWq6C0dTgQvzFNnGnXOhxhyvitvpMUg9i8VLBZbaAgkkhmR8XJYIn8vHY62QBpWAwYy9Ziwcy72pFYO-wIUJjPtoN7Ig0Afybsmraub2KXcRUIiM4ENDS64zCu4mE-FBWWFzMAgDGRluYHq9kXn1f77Vl6HbZ8kaeHo4_T6cZGUS6iw6ZSzo_qixZG4nvGwsF4PfIDlTS-FIosUSx2okmyJezuY2WQoaE2sOWDk9Bv67FlAJ3j_10m_lDA6wHgo_NJjIgnn3_ITMEdn6oYNaXRJhFkizYHk6shvYhUxDIICS09SXjemNZMvgjLJk1hY3dna24R52tELck-FZyfoPBeeYX2yRc-nestnp7mbn12v6YRxTr9BfY5sarMvNoTkiK_1GQzQVaf5zbylKM3RL2hX6rMvOcAGQ7hYWIySsXbz9uMooj6j3cOFm9Sv241Ad2117e8bsKTvaeKyaQZIUlwnN91EjOpbV63XcPhjDaLCPYz0dW7HFweka9gvorf57X0JBWWo6Ltptehk2mUDKNpVJnFa0AMilJl-ulVyYIfa1nER31PisO5YBNcSFu7oJzfkpmA6jisVoNVMz4ER-_MTww_Lc6SnpUzXfQsGV7VnLI1I4rsXiuLP2BQ6bS6GVQQbysVZ6o9Hk9EsKVvkNgcoldLPRUNNOk25q7e41MSCfmKDB31RLnDfpp_39yvwAM0ClCno2QlYIhaE0Ump1IViT8aPrfkFcDNDxyJsyazydSG8PRG2dWZ0CouU9L1zDAFSqdqNWpJHW2kEr-" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Vision Transformer (ViT) / 1. Vision Transformer (ViT)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/ALs6j_HGGLfmBIfSLZ1hwWNm2reJHMxU3Xk0T4nrpg65-igR9S_I0gdtqvYd5578qfdCAbjZtkSpZIuWsHbHAKBb7p1fAvYr0CsnnzWo50Ky4FcNNv7lLwL0wqRHVHLjGi_PyxaiMDynUx6iV-ikaeMpySgjISwqEWX_eS6gby5IweidiGqq3V9SRWO4L3L1PnPZB2x8qFkkLsynriiaXLeb8z769bEsgMGKixUdi5ZqdX2dbwX9Kt4QZojrtadjMxcOE5J59pq2wwP9FRGn66r8lC5nHaiHwPDhNE8iq3ZGFYeue9EL9qAKuIXXWHUu7txSAyQ8e1wReXALw3Fdx6kywHHn0thB7d3Wy0tFUbPyRP4FszyZpQcbqei6OUkim8eU4rjE07qTcQuL3-r-Qs7oIneJa2r7Q37boYUWqwf2Ts-PAFPiA9pH8xANFZ-ms97-AGRhaJ3mSXSIJStnC9ajDQGrtazPXs6TBkYt3KLVuzwFRH7dD94NDY94zCQOFkZuhLwO5QjYgy5DF8sn3jbz7p1bpu58i8PohZ8O99EDPAz7uNJDh57RSTYvcBr8brkJr8BuaDVZ4j7r9YkIfDqINTDfejixbYXrDf7DoTJu2cGgcoyxm07JBDBh7MiGQ3JQWw3XB_baPkdUbaHsZ0abztGdIj5ulzq3eAoAETLOljJt3TSWQ_UBu2bcc-RgX6r27kuoY27EAGRR5qZ4cMK47NCQ2TBv-PFnSeX_agCf5s8ScN5Ftnd6-Rw1P2inWq6C0dTgQvzFNnGnXOhxhyvitvpMUg9i8VLBZbaAgkkhmR8XJYIn8vHY62QBpWAwYy9Ziwcy72pFYO-wIUJjPtoN7Ig0Afybsmraub2KXcRUIiM4ENDS64zCu4mE-FBWWFzMAgDGRluYHq9kXn1f77Vl6HbZ8kaeHo4_T6cZGUS6iw6ZSzo_qixZG4nvGwsF4PfIDlTS-FIosUSx2okmyJezuY2WQoaE2sOWDk9Bv67FlAJ3j_10m_lDA6wHgo_NJjIgnn3_ITMEdn6oYNaXRJhFkizYHk6shvYhUxDIICS09SXjemNZMvgjLJk1hY3dna24R52tELck-FZyfoPBeeYX2yRc-nestnp7mbn12v6YRxTr9BfY5sarMvNoTkiK_1GQzQVaf5zbylKM3RL2hX6rMvOcAGQ7hYWIySsXbz9uMooj6j3cOFm9Sv241Ad2117e8bsKTvaeKyaQZIUlwnN91EjOpbV63XcPhjDaLCPYz0dW7HFweka9gvorf57X0JBWWo6Ltptehk2mUDKNpVJnFa0AMilJl-ulVyYIfa1nER31PisO5YBNcSFu7oJzfkpmA6jisVoNVMz4ER-_MTww_Lc6SnpUzXfQsGV7VnLI1I4rsXiuLP2BQ6bS6GVQQbysVZ6o9Hk9EsKVvkNgcoldLPRUNNOk25q7e41MSCfmKDB31RLnDfpp_39yvwAM0ClCno2QlYIhaE0Ump1IViT8aPrfkFcDNDxyJsyazydSG8PRG2dWZ0CouU9L1zDAFSqdqNWpJHW2kEr-);">
                    <div>
                        <span class="mainTitle">Vision Transformer (ViT)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2023.11.03</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번에 소개할 논문은 바로 2021년 ICLR에 accept된 Google의 Vision Transformer (ViT)입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이미지를 Transformer 모델을 이용해서 task를 수행한 모델이며, 현재는 아주 일반적인 방법론이지만 당시에는 엄청난 주목을 받은 논문이었습니다.</span>
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/2010.11929.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">ViT 논문</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>ViT의 동기</li>
                            <li>ViT 구조 및 인풋</li>
                            <li>ViT 결과</li>
                        </ol>
                        <br><br>
                        그리고 아래는 Transformer 설명글입니다.
                    </p>
                    <div class="link">
                        <a onclick="pjaxPage('transformer1.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Transformer 설명글</a>
                    </div>



                    <h1 class="subHead">ViT</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>ViT의 동기</span><br>
                        <span>Motivation of the ViT</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        ViT는 이미지의 당시에 원칙이었던 이미지를 CNN으로 푸는 방법 대신 Transformer로 접근하는 방법에 대한 논문입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">ViT 이후 현재는 이미지 뿐 아니라 음성 등 Transformer 구조를 안쓰는 분야가 없을 수준이지만 당시에는 아주 획기적이었습니다.</span>

                        
                        <br><br>먼저 저자는 Transformer의 장점을 아래처럼 언급하며, Transformer에 이미지를 적용하는 타당성을 부여합니다.
                        <ul>
                            <li>효율적인 연산(Computational Efficiency)</li>
                            <li>확장성(Scalability)</li>
                        </ul>
                        <span class="highlight" style="color: rgb(0, 3, 206);">실제로 자연어 분야에서 Transformer의 확장성은 매우 용이합니다.
                        Max length를 자원이 무한하다면 무한대로 늘릴 수도 있으며, 각 block별 i/o (input/output) 차원의 크기가 동일하기 때문이죠.</span>
                        따라서 저자는 이러한 이유로 이미지를 Transformer에 적용해보고자 했습니다. 그리고 실제로 학습 과정에서 CNN보다 자원의 소모가 적었다고 하죠.
                    </p>



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>ViT 구조 및 인풋</span><br>
                        <span>ViT Architecture and Input</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        아래는 ViT의 구조와 이미지의 인풋 처리 방법입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HGGLfmBIfSLZ1hwWNm2reJHMxU3Xk0T4nrpg65-igR9S_I0gdtqvYd5578qfdCAbjZtkSpZIuWsHbHAKBb7p1fAvYr0CsnnzWo50Ky4FcNNv7lLwL0wqRHVHLjGi_PyxaiMDynUx6iV-ikaeMpySgjISwqEWX_eS6gby5IweidiGqq3V9SRWO4L3L1PnPZB2x8qFkkLsynriiaXLeb8z769bEsgMGKixUdi5ZqdX2dbwX9Kt4QZojrtadjMxcOE5J59pq2wwP9FRGn66r8lC5nHaiHwPDhNE8iq3ZGFYeue9EL9qAKuIXXWHUu7txSAyQ8e1wReXALw3Fdx6kywHHn0thB7d3Wy0tFUbPyRP4FszyZpQcbqei6OUkim8eU4rjE07qTcQuL3-r-Qs7oIneJa2r7Q37boYUWqwf2Ts-PAFPiA9pH8xANFZ-ms97-AGRhaJ3mSXSIJStnC9ajDQGrtazPXs6TBkYt3KLVuzwFRH7dD94NDY94zCQOFkZuhLwO5QjYgy5DF8sn3jbz7p1bpu58i8PohZ8O99EDPAz7uNJDh57RSTYvcBr8brkJr8BuaDVZ4j7r9YkIfDqINTDfejixbYXrDf7DoTJu2cGgcoyxm07JBDBh7MiGQ3JQWw3XB_baPkdUbaHsZ0abztGdIj5ulzq3eAoAETLOljJt3TSWQ_UBu2bcc-RgX6r27kuoY27EAGRR5qZ4cMK47NCQ2TBv-PFnSeX_agCf5s8ScN5Ftnd6-Rw1P2inWq6C0dTgQvzFNnGnXOhxhyvitvpMUg9i8VLBZbaAgkkhmR8XJYIn8vHY62QBpWAwYy9Ziwcy72pFYO-wIUJjPtoN7Ig0Afybsmraub2KXcRUIiM4ENDS64zCu4mE-FBWWFzMAgDGRluYHq9kXn1f77Vl6HbZ8kaeHo4_T6cZGUS6iw6ZSzo_qixZG4nvGwsF4PfIDlTS-FIosUSx2okmyJezuY2WQoaE2sOWDk9Bv67FlAJ3j_10m_lDA6wHgo_NJjIgnn3_ITMEdn6oYNaXRJhFkizYHk6shvYhUxDIICS09SXjemNZMvgjLJk1hY3dna24R52tELck-FZyfoPBeeYX2yRc-nestnp7mbn12v6YRxTr9BfY5sarMvNoTkiK_1GQzQVaf5zbylKM3RL2hX6rMvOcAGQ7hYWIySsXbz9uMooj6j3cOFm9Sv241Ad2117e8bsKTvaeKyaQZIUlwnN91EjOpbV63XcPhjDaLCPYz0dW7HFweka9gvorf57X0JBWWo6Ltptehk2mUDKNpVJnFa0AMilJl-ulVyYIfa1nER31PisO5YBNcSFu7oJzfkpmA6jisVoNVMz4ER-_MTww_Lc6SnpUzXfQsGV7VnLI1I4rsXiuLP2BQ6bS6GVQQbysVZ6o9Hk9EsKVvkNgcoldLPRUNNOk25q7e41MSCfmKDB31RLnDfpp_39yvwAM0ClCno2QlYIhaE0Ump1IViT8aPrfkFcDNDxyJsyazydSG8PRG2dWZ0CouU9L1zDAFSqdqNWpJHW2kEr-" style="width: 100%;">
                        <p class="caption">ViT 구조, 출처: ViT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>1. 모델 구조</b></span>
                        <br>ViT는 Transformer의 encoder를 사용합니다.
                        

                        <br><br><span style="font-size: 20px;"><b>2. Image Input</b></span>
                        <ol>
                            <li>Original Architecture</li>
                                이미지는 \(B \times C \times H \times W\) (batch x channel x height x width)의 이미지에서 \(B \times L \times (P * P * C)\) (batch x # of Patch x (Patch size * Patch size * Channel)) 크기로 변경하여 넣어줍니다.
                                <span class="highlight" style="color: rgb(0, 3, 206);">이는 자연어를 (batch x sequnece) 크기로 넣는 방식과 동일하며, 자연어가 임베딩되어 hidden dimension을 가져 최종적으로 (batch x sequence x hidden)의 데이터로 변하여 처리되는 것과 같은 컨셉이라고 볼 수 있습니다.</span>    
                            <li>Hybrid Architecture</li>
                                하이브리드 구조는 CNN 임베딩을 거친 이미지 결과를 위와 똑같이 (batch x sequence x hidden)의 차원으로 변화하여 Transformer 인코더 구조에 넣는 방식입니다.    
                        </ol>

                        <br><br><span style="font-size: 20px;"><b>3. 학습</b></span>
                        <br>ViT는 Transformer 기반의 모델이기 때문에 적은 데이터로 좋은 성능을 내기가 어렵다고 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">Transformer는 inductive bias가 부족하기 때문에 충분한 양의 데이터로 학습하지 않으면 일반화가 어렵기 때문입니다.</span>
                        따라서 구글은 6억장으로 pre-training한 모델을 공개하였으며, 추후 활용한다면 이 모델을 사용하는 것이 좋겠죠.


                    </p>


                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>ViT 결과</span><br>
                        <span>Results of ViT</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        저자들은 3종류의 크기가 다른 ViT 실험을 진행하였습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Gp4GToZGj-jPBqJ3-3AcWIeoGIZqnefYE-RFRDa2ff6ACDG1UZ6r81NQEW1E4GUJ3dYOdzTz_2xviCqla52_g3h4ZbMJP8BtoCSOiHQ1tTvxRWDNsQbmnS3uJ24Gx2WmefUQoRqNIZXzuQytwRI5NVQToXJEIalLYTNoQ24lxIvwFICPcS4x9Ar4gPhJs7QL9CAhCaea8_PPniUuNfAF_WtHcw0Q15pOm707JsK4BgINnnWWL5Vq4j_xA97KwAaIUBClmhWg_0PD49lyCZ3Fmp2tfh1RylNo2JCDO6RZr3imrDV5i9ACY0JU-1ZvI6ikEfu-P6J3hwBjXdMQLvGAZ_9xqlyFbUOFYxLuOwZtPJseLlmGPXdlMSBl8ANv4CCat62acAtsZ8nxC6JJ-GJZJ15ckAyS74dCl3peK84lEzIJ9gNuK6d_z-mR7pSPIfNBKoCeXIqn1HGrF8DtPVcppC0TxbuKDEdOzc2WsOW1gtR9tHPd-cYp6ygbVN1NWqO9gmG3W3tQ-lMEBbfzANXiZ1szwq4k8h6dIr_bLD7fq64V9SrWRyfR_Vb43PhZMWVB48HL9bp594XA7JdcY0vIEse0foWNqa2eCXpazYeLZ_OwR0Gu6YQghRjqvd_Vcq3jQUg4f_pDXYsa9I3u5qRkyzHlxsBSLYTtZ3OKpvmj-XQjV34pQIb49uLGysDysaQB4NrE9YjycM4qxQJxgNlYDacwBkXzxT-PIx5MBbg1AqFKBjdkJIHCkzjtGwwCwFmI8ODNQKBCSNa2U-kh0HkXQyrArV2DRuCUC0vKyRilD_o3yd3MsSGZJp7AJ0Vk03X1yMdze9IyOsv2t3Xj_yUSZRHVik0VNkye2wNPCU-ldy9_nDCgB84NaZ7GG7Djl604YgNOUknXBpCjAn7DFMfXcSG3ILo4f44eRRQtCvk2wkpcGAW9uSMR-Ry0Rk6KNFVth7qIzihgHA6RNufe2kMB4IZkKjRtmPmKpPuBDICWrmFjQe7012KTs3rOQNRQUfYOKzr9qGt-sLVQPPeDtXCPIR8TDpPuotK677DS9JFHDqwtt1sPdcSy6Kvljj22WGdyYd4zAUKL42tX23X-MO2T01Hjhq_3vnJMN4lWaKwktAiS3_arn_Wud3JnlepL1a_4pB5_aHrx-tEB8G0U6ietuLxCuc7IStrEdCgpLsD5NJxiRc9mUWRSIIKdME-Vo4fEPjDKQBI99pevPPbc3j32MqULUWcz7-GrD0Qr7j69r7TEU3bPRbVs5XLtWcH2A5j_WbSQDeUygjyKADxyHQeLISqz6nzUJYmD6LPDYag7TxqIYZVzmKOHyKg39yEtj9JsQFfLcPob13gkC6YR5mXWbWKJqnvFnd6yp2wSV8kp2iI3BwHwmsG73fT-bVZRCiW3FfUIWltyJZ8OdEzBGmr6qk5dqnD-cgd5B2-699Kt0zTkVFgx0l6zjYL-R_BMb_4K5ExfL19NbNt1nMbblwmYy4cB7Us8-SG83vmfRGrj1semQglRnUj-8ZYJNKahcMnqmL3fdz64El" style="width: 100%;">
                        <p class="caption">ViT 모델 종류, 출처: ViT 논문</p>
                    </div>
                    <p>
                        <br>그리고 Big Transformer (BiT, ResNet 기반 모델), EfficientNet과 비교한 결과 여러 이미지 classificiation benchmark에서 우수한 성능을 보여주었습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_ExS9ZmzlAgfXIwKbyiW_NOCT0WUKx5Wsy1GDuh3r8tiL_5068Sv2KkOYbXi4xORvYd7DO_HVqlwfJbMPNCqvIbnn-UUa9QNLily--PdcZze-Xuw68qwKLmpefOKD-0sRh-wcMLxniPBkusQ82aHoeuAZXvj5NGdxZXNkyvjj-RyOTypKBslX_EIc6IwPmn278GjTdHM_g1J0rCXxx0zkQMJ3xqnh3cEc6mR2Fis67wtnUd1807-BWVlsCOnaIrDLanmxT3gNzcU98E_DOpc0zfKWgUYnrMr-iUsRh6QwaXTe37eOguGDzHt2j2qMz6yJg0vaL4bjiwwuwYjhbSn8qBPtiRwa3OmYN3-UzqjiXsTG0Kak64deweti1xjxMK8IvoxMTjACr0xk1GrRW3Wa2LvINOzxId2a15M8Kn37_Jj-FV-QDsKWl6UDcRg8XYcouZvJL150UQJig2cqPJ8OnluhEKoRzOFP4oFzf3hn_p-3UP10-_mBq-qLt6RqLM4hhs4G0cZFwZMBGsupo9694oHeGjZHjrTdgJZ0KfzfA7D04EQPhQxE4bN1UsfDQPmDBAxJtdoYQiCYgGRq_xgz6PTqQE9EAJ0ToTs1TulBPhCrBIgSd4Z3zd12BqUDWd1gmTfmHizAUPUFVDHjcFD_mSvNFHJXXikoe1AcQnOtE35zsK2cOUUxNfSxLMXdqj2YKYNfFEeYHLhGSoqS23QEPwyUbrzQHQsOkeFHS4PjjsmBwBMEpLVGo2x-bb_AvWlTG07vTF4WrIaUyic5Pt8tUJ_VunjjGUeo9EJFipfbyvLyWK-kCmNtYOXNI4T46UFoRdMNLZmPLdy7-C5H0i9rGGZ-pF4XHy6Nx9Daa2rnoKCkuB6YkGYVXjWKKFsx9ruvs7D5q4wwHANHfFtgD3DbjjajZ_YxyqYOd-l5A3f0U-2B25Aln6HRoBxKiOKbTLMofRnByVjNvfnMjm3whA4sprIBviQ0DTFxIVXkGTluZ1kfJNO8hE9__3JKXE6aWP9BA_Ozf08FhT-eANptAQvNTjpqzX5kN6o0U5gHRM2bjZv9NUuVH8dsQqYe9-a0zqRGL-pYe0UcQ0gLHLE54Kbiy52LPWnn1u_d8IbMpeEtcR9IPfum7eXKU09zPbjNiifV7O-CzJImjHVpzKJmGOe4JjVfkm6xe_PEs27kMEH-LDTDMqJjLELPDnsz0iIgzv60ijsfbQoiSXNNZ_bHEweIAa_XE6fbuSbuktOUFRJ07EasPK31ipspSMtPbrGr7JVrCXvqkM5sWtQMGKkcBAvdkaQEhrwM9_YY_2g__EtzArgkhKz6whvUQImOhw8bs3rZwbV9Gy-NFqwbZG1tQgj5juX06e1jRmLZyMQ1o4ZmVZwYT9cuxNUxGbLB0MgvNRcOfOZJw92yaXqd2KToOT8WUQqBMBrhrmBBvLDIylieWQ2F1BWCUKrSEiGScMKBn_Yf-Rxqqkk-sQZD2SFw9XO-l4TLYMKwNJM_HClQPlDD4RWlUbLGgzo0eH-Clmwh4F6eX2c9XjbbFw" style="width: 100%;">
                        <p class="caption">ViT 이미지 classification 결과, 출처: ViT 논문</p>
                    </div>
                    <p>
                        <br>또한 위에서 언급한 것 처럼, 아래 결과는 pre-training 이미지 수가 많을수록 성능이 좋아지는 것을 보여줍니다.
                        또한 신기한 것이 적은 이미지로 pre-training을 수행한 모델은 CNN 기반의 ResNet보다 성능이 낮은 것을 볼 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Et2Wt7XbJPYxjiiEMPYuepKQFK5Jt9qTjlX6W_GkaXWtyeQnwqP48DHPxGbpcmzy-o_BJOBLKiizUAqqNK5_oPV2_s-9NjMfRNxeNbYpUaYraAXRmslH_2RGj7DmIO_el_U42xqjja0pN_3XMYI2CfyIL90Ofe5a6lVE2bLGxef6WlyF2jWdY4z8DJamkoaf612SJ-WGkIOxcvU4IuXKkXjml-N-La3B9MrUSkHabLgkXL1R2ai-Uy8-bUiuQNyJauRbcgYwQ-drNgVlFgOvn8ML8F6lWCMfrOcFsNvGbZb6SvdSICQ2q62jwIpNFQOrJb_4RPYtSxErXitc1DX67fm77hsOmbhoZU045kCzvM-70FBR7oSf_8jl_o3yblyxWHoqm7Xlu-C5Cs05f7Y_Fm7-6UcPl0O44XDu2k7Ve6Pw97mcmxDZdv0yiOEPgzdXR_IPy_ACWQVu5ROSxT0XLLHYVY3eu1xgv0tp5oCfrXwXt-E0Sg7DSIaD_EXcYCGMD6fx_yXl2eF5go6DQdJCwfaSEUuiy0nzqY8I4eyb9r_TXDsEGLaQxCkH7JdftPqRiwZUd77fyGgROv9kJKX38wkCRok9Xxp7URgOVUvI03FOdA0C6mWcrVulIkb3QtsEgPMk0QMfknVpQVCVVlVUs_gb0bjuE6JoKg5wMjc6gSfk_MGMDxllRF20QH23qyPAbjomULWeo6P1ayZ6E42xRrAsExIYD1gGQ7WXC40NeMc9-cgQNcbq8xJKLzzG1XYQDTlv6J-OpWCwIXT4EfI7k0G_2blEZvPtQuoXhQozrEzxdMXb16ii0I1xZlbS8lSy3jXkGNtDHSOATNB0_QpY3sOOZ9gKzxUHl7cnztQF7M87_BopebmSAYR5gmWjsu-NV79iUGY5Rv3yhUNBp13s2MswWX1aX-8FO21mlecHzsxmiWaUwSE-qimPhGZDF3aBsKzl4qga6YUk9v1oZJYs_MdkZM-CGgnyMYJvtWpIg8zy6UXC3Fbvw18oAuQaya8BkixPs6gEESDjObWEH7xFykHI-PX1xHzWLJ-JwOo52i_CZeKF0nUBPor6ARgFE0oXJA8Co8OldyV3Azo9vwLkJNL7grAHQDArMuLhqCRkvRLZ6RKMt3_xROLtE7RstQem8-Ip-xfOvDA5es6cNBXa2jQxYat7Q0q5-cPIK1_BurBr0xHCgdY3oEyHLgyPKrLIH1PF-Ic93fuK9nHK1sgR83Yw5DQmOXu7-MfouKJjXyihYY3JNjKQgrA7kXKse2ITlHaMk8k_b87Zhyv72jGzCVHJR5-9SdFHi-GIMrXJYkPp9QlFa8zhfVqP21ej7DlwLZuB68WyTbaMeqTXyCO1LGqd5lWQkq9542_E6ipBHEVNQkcsCxAELJX0FepyIbZS_xazkK2L5O45OUqu19m7N2qTBu5HwJxE6XNkBOhRQh2XEGtpt3A_QUrK93BDg_cZ3qifogU67hykvuiveEltFkvf7VAgHSrQ47LtkvNSGaqLP9WqOitXLNylZrWemyhwB7L9vg3X-R" style="width: 100%;">
                        <p class="caption">ViT 성능 비교, 출처: ViT 논문</p>
                    </div>
                    <p>
                        <br>그리고 모델의 attention 가시화 결과를 보면 모델이 의미있는 곳에 집중을 하는 것을 볼 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Evj2GJxy41hd7aXxIL73jnuKpbigkVufNxHMNmyzgPFbMK2LyQSBDd9oKBcSHaiZvBAO00L7wHZ6LGl26ssNbNpjJqIshFHeqFVMtqj4NYxcWBsIJL21p196LWUyJBWW_CPnl8_DBxii5-kKeoSauFv7wgxqaZEWzUTW7G0Skp4FBpIBr7fuEhy4hGN6m7m8OGepYsD6-gYO09rul4PTA_plyPcM7-oLn2EzmLuxahq9tkjiAkkS6uOPeRVCUYXYzDKHhKuI73yHPuEfzjzcH8WIYX4qG-1owxJvjLI4BpQe5eM5yY5_f7vntzmEChMwZixObA74itGBm7Fb_EBHKJGlgFanRZ0PmTyxklKVYpQsL-JsmwXZLSsa_regMElajjK5yPkhcNOpvxg7MM0N05rPsptw-LyXBovLjsglTm7ZamtgcF77mz8f5ntvNp0qdh16PJG61bobbH4kCLBpoKW9iEu0sfd1W_gUitrA06w9XgdEiZArWH1YqQcYN2gzKvGeItov2WZ7GXC54hXe47vW0H7Mu9gTGSJwLME9CRVIkHnkcldBr0fVQ-rzTGJ7xHmmhj3r7mD4EK3kEfLNgM5YbcgNzVT9QWx2PyYcxLndXH59CVtKLxX-gDh13aDMFDNQ65OVBLUwCDvX5Cd5FnZx3uBgNdVvSJW-1-RqZv0PLAGB67ofECbBPFYfrFqrpPmmxPX364E78z3FCAA9wZEEIFTM_PS3v-J363iHHC7OZTFOlugn4StdIsNnUdjYGyxbE3WDMAvTSCg8iNwVJU1vj49CV7bJCGjBiDKoXw5-6MzHPI4a71VIG3YvU8DGcnFGDwxKlihUJaC2NLO2Gh8EDGh3kii8NJ8jubvyfebTKDehNh2QjDVDCNIjH6bMpUFqhjROItMCQB0b2pD_cV5ZXziqLuX6FUUwObilmzlRAWzfekPlB6IuNYq2fz5x6CRGWjp66I-mRVWoZjj-AnbZJd0GcUjd52n0bnAu-cTvw9fj0pMd_tFEZoJe31l0Xwso9z0u8uJA-AfL6f8youqPObC2jkq-UA69mxbdKXrIBW_-XL3sEgLTxEMj6YRp3wNK8dWLcrltUUX0Fd3KedH96pRdQdKXGGw-mYuYCJGJrm5tgQRQx-64NNcA5AgbzAHNnStZmfYpKkTMRwuNmCCDTOS9wLiCRMyjJd26Iie_WM4nUrXeXvT90_Z-xqfLJAgavz3yIAdX2GKTIigAn9_Bf0oiCpSxN9TLNKuKRVeJgRLNjr3YCqmlS4l6ruh2l37GT55CMBV1Y7aqWhOFA3oMhs4DYMHxouSHfRnnMCH5osgMrzWcEA0cb4SfIUX8UmkufuIZESX_qB-2MOYITT8IwH_l894G9ZfXD1cVYcdjncLA3L-EPiC_Eak14ZeT5H3xHSUu92aylUTYKZtqZzf3OcyRuMv6MuxWHjWXIUjdav7MXzTSNhQah5Ak33P6iCGC3-MS4sqjPdSBm0GjsJ7Svd8qCf2znDevwyVvKILHFqfqze6MmC2D-s7ws_H1rQ2T3Spw" style="width: 60%;">
                        <p class="caption">ViT attention visualization, 출처: ViT 논문</p>
                    </div>
                    


                    
                    <p>
                        <br><br><br>ViT는 현재 이미지 인코더 모델의 초석이 되었으며, 현재 LMM과 MLLM이 대두된 시점에서 CNN보다도 널리 쓰인는 것 같습니다.
                        
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#ViT
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('ViT 첫 게시물 입니다.\n\nThis is the first post of ViT.');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="alert('ViT 마지막 게시물 입니다.\n\nThis is the last post of ViT.');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br></span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>