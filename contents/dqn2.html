<!DOCTYPE html>
<html>
    <head>
        <title>DQN을 이용한 Cart Pole 세우기</title>
        <meta name="description" content="DQN을 통해 cart pole 세우는 모델을 학습합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/dqn2.html" />
        <meta property="og:title" content="DQN을 이용한 Cart Pole 세우기" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="DQN을 통해 cart pole 세우는 모델을 학습합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/AMPSemeVPj8GotyzzoLf5ewWPlLx0aRrKGrhZBHQCE16mLKxRGRNJQ8SKLeS5npzN7MKwlMm2CqknMmT7W9h8rffYTzs8yO4Xodi5eh0jLW6aqTEJPvHeP_de899Mf7pYrIC8E_cLqBwym6Aha4pOf2d6CmR-AdlNBilIefEvg_v3mq6nO9_VuYEvNre00nPOtjHe2yy-d9dZI03ear3r6ybvdr1n5H9wZCm2a_rsW2ZSSHjHOpjdNFgmL-78XfgK83dQWdsTlwno2_MpeEUp4bkuK7AOHhbAdJ-0YXan477d7KW8-p-VwIZIJy3ntefI6poaVqCpbgLSqSsF4l29SZ18mFxVCtYyKf2AiQ5I-mncNMAWBJlqWE-9KGtW2zqkozjafjF9lbFE2UI-QMgUgcUuF1wgfmdeyMe1l8V7e4nOMY_xSYZyKosbwD9KJD2Pl7Kd7ynOmsm4E6mk3M5pwquOV2cCzj1lN0D1jQ8hNuiAeiVzbMW0rlbY_XQeX5U2z7L06Cy0hipH1DSAV4Q8JtGef3EyImCdpMEO0sdjIbyGCAjGVSmWH-maDOBNmrqU0JEytfZKlASQGXcdc2ZbTTxdYw6p7P3O_syFKEfGfLjI1V1kQOG91eEeBfmpoRM-RRv2Vc7HdU0Bjvot8LJYMDuOB4AU4nEkufIXJKDcAxIzvlqNx5XCa7qZ7NjdTNJQjCWWwUoNORLILyguTSWkH1Vwplem8aWedXvFvTAFzCy7zmm2jTltQ87xrQB-yL1cQFLoMPHWdIgcNBlyXQsRqYRzuIiRDuS3gKUQsSe9xRkWH9mJQqYQ8PTeaKwsqwehacZnpGpHwWX_GJPLQ3ouvXA7rvWyqKYJO_ob86491m-esEt2yS7U6DJ0ltgLJsIll_G17X5kdQ87iP7e4cP4X1SOq-GAgNeOxAIUfYthGyzT-EoljOmLx_x70OZ2I78z3qnvLR_aGhjlPtWaFgcUjdynsx6ts_TC0h4Mly7e9TR-z8zPSDMmSC2FkysWMauLuGjZnu0HsotwgDHrl7AlCis_CDWUIzmCJMInzrpn4BZ5SBpR5PBPGw59KbnwqbmFlyz4sxJy5ZKuLXl8RPZ6Cn0WKrbWCaHuK6QxMUxYf4kfJwsPxaLI7ChmolVVT_G0m5kAylmhkow470vJLIdZGbOYqs-TVClv2y1-oadZ7L5z1KrXHPWGu2Cs5wFSGMQXhfhw4CHgPx_Y5xm6Pe2YOKhLWdrD_hcWBS8YAg-nop0B1mMfuno6mCS0VuLFNiz4w6EXqXU5KHdJlS2XguK3IOJepLwXtRvBBLvJX7zqEUcvvBs3fdVjXwNr5tI-Y0Nsfu3VHrauAhzkgA3g6RsE9FkmZerLNdlF4K3WgG54tz3cxVuYRd3pjPTOIOjxeKktay6ZLVlt5mldInB2vxOxg6oOzHvX1DtbOnBUweOWYzGBEPwWXA2nqVBN_VWp28fbdyX9qHGCFF6EuNddOJJyozOT-s" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Deep Q Learning (DQN) / 2. DQN을 이용한 Cart Pole 세우기</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/AMPSemeVPj8GotyzzoLf5ewWPlLx0aRrKGrhZBHQCE16mLKxRGRNJQ8SKLeS5npzN7MKwlMm2CqknMmT7W9h8rffYTzs8yO4Xodi5eh0jLW6aqTEJPvHeP_de899Mf7pYrIC8E_cLqBwym6Aha4pOf2d6CmR-AdlNBilIefEvg_v3mq6nO9_VuYEvNre00nPOtjHe2yy-d9dZI03ear3r6ybvdr1n5H9wZCm2a_rsW2ZSSHjHOpjdNFgmL-78XfgK83dQWdsTlwno2_MpeEUp4bkuK7AOHhbAdJ-0YXan477d7KW8-p-VwIZIJy3ntefI6poaVqCpbgLSqSsF4l29SZ18mFxVCtYyKf2AiQ5I-mncNMAWBJlqWE-9KGtW2zqkozjafjF9lbFE2UI-QMgUgcUuF1wgfmdeyMe1l8V7e4nOMY_xSYZyKosbwD9KJD2Pl7Kd7ynOmsm4E6mk3M5pwquOV2cCzj1lN0D1jQ8hNuiAeiVzbMW0rlbY_XQeX5U2z7L06Cy0hipH1DSAV4Q8JtGef3EyImCdpMEO0sdjIbyGCAjGVSmWH-maDOBNmrqU0JEytfZKlASQGXcdc2ZbTTxdYw6p7P3O_syFKEfGfLjI1V1kQOG91eEeBfmpoRM-RRv2Vc7HdU0Bjvot8LJYMDuOB4AU4nEkufIXJKDcAxIzvlqNx5XCa7qZ7NjdTNJQjCWWwUoNORLILyguTSWkH1Vwplem8aWedXvFvTAFzCy7zmm2jTltQ87xrQB-yL1cQFLoMPHWdIgcNBlyXQsRqYRzuIiRDuS3gKUQsSe9xRkWH9mJQqYQ8PTeaKwsqwehacZnpGpHwWX_GJPLQ3ouvXA7rvWyqKYJO_ob86491m-esEt2yS7U6DJ0ltgLJsIll_G17X5kdQ87iP7e4cP4X1SOq-GAgNeOxAIUfYthGyzT-EoljOmLx_x70OZ2I78z3qnvLR_aGhjlPtWaFgcUjdynsx6ts_TC0h4Mly7e9TR-z8zPSDMmSC2FkysWMauLuGjZnu0HsotwgDHrl7AlCis_CDWUIzmCJMInzrpn4BZ5SBpR5PBPGw59KbnwqbmFlyz4sxJy5ZKuLXl8RPZ6Cn0WKrbWCaHuK6QxMUxYf4kfJwsPxaLI7ChmolVVT_G0m5kAylmhkow470vJLIdZGbOYqs-TVClv2y1-oadZ7L5z1KrXHPWGu2Cs5wFSGMQXhfhw4CHgPx_Y5xm6Pe2YOKhLWdrD_hcWBS8YAg-nop0B1mMfuno6mCS0VuLFNiz4w6EXqXU5KHdJlS2XguK3IOJepLwXtRvBBLvJX7zqEUcvvBs3fdVjXwNr5tI-Y0Nsfu3VHrauAhzkgA3g6RsE9FkmZerLNdlF4K3WgG54tz3cxVuYRd3pjPTOIOjxeKktay6ZLVlt5mldInB2vxOxg6oOzHvX1DtbOnBUweOWYzGBEPwWXA2nqVBN_VWp28fbdyX9qHGCFF6EuNddOJJyozOT-s);">
                    <div>
                        <span class="mainTitle">DQN을 이용한 Cart Pole 세우기</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2023.03.02</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br><a onclick="pjaxPage('dqn1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>에서는 강화학습 딥러닝 모델의 시초격인 Deep Q Learning (DQN)에 대해 설명하였습니다. 이번글에서는 DQN을 이용한 cart pole 세우는 모델을 학습해보도록 하겠습니다.
                        본 글에서는 DQN cart pole 학습 방법에 중점을 두었습니다.
                        실제 학습된 모델을 평가하여 gif로 이미지를 만들거나, scheduler 등의 전체 코드는 GitHub에 올려놓았으니 아래 링크를 참고하시기 바랍니다.
                        
                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">본 DQN의 코드는 PyTorch와 Gym 라이브러리를 이용하여 구현되었습니다.</span>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>재현 메모리</li>
                            <li>DQN 모델</li>
                            <li>DQN 학습</li>
                            <li>DQN 학습 결과</li>
                        </ol>
                    </p>
                    <div class="link">
                        <a href="https://github.com/ljm565/cartpole-DQN" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">DQN cart pole GitHub 코드</a>
                    </div>



                    <h1 class="subHead">DQN 구현</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>재현 메모리</span><br>
                        <span>Replay Memory</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        여기서는 DQN의 핵심 아이디어 중 하나인 재현 메모리를 살펴보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">기존 Q-table을 딥러닝으로 대체하려는 시도는 불안정한 경향이 있었기에 이를 해결하기 위해 도입한 것이 바로 재현 메모리 버퍼의 개념입니다.</span>
                        이렇게 즉각적으로 모델이 선택한 action과 그에 대한 next state를 학습에 반영하지 않고 재현 메모리라는 버퍼에 담아둔 후 랜덤으로 batch 만큼 추출하여 학습하기 위한 코드입니다.
                    </p>

<pre><code><span class="return">import</span> <span class="clazz">torch</span>
<span class="return">import</span> <span class="clazz">random</span>
<span class="return">from</span> <span class="clazz">collections</span> <span class="return">import</span> <span class="clazz">deque</span>


<span class="reserved">class</span> <span class="clazz">ReplayMemory</span>(<span class="clazz">object</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">capacity</span>):
        <span class="var">self</span>.<span class="var">memory</span> = <span class="clazz">deque</span>([], <span class="var">maxlen</span>=<span class="var">capacity</span>)

    <span class="reserved">def</span> <span class="method">push</span>(<span class="var">self</span>, <span class="var">state</span>, <span class="var">action</span>, <span class="var">reward</span>, <span class="var">next_state</span>):
        <span class="var">self</span>.<span class="var">memory</span>.<span class="method">append</span>((<span class="var">state</span>, <span class="var">action</span>, <span class="clazz">torch</span>.<span class="clazz">FloatTensor</span>([<span class="var">reward</span>]), <span class="clazz">torch</span>.<span class="var">FloatTensor</span>([<span class="var">next_state</span>])))

    <span class="reserved">def</span> <span class="method">sample</span>(<span class="var">self</span>, <span class="var"><span class="var">batch</span>_size</span>):
        <span class="return">return</span> <span class="clazz">random</span>.<span class="var">sample</span>(<span class="var">self</span>.<span class="var">memory</span>, <span class="var"><span class="var">batch</span>_size</span>)

    <span class="reserved">def</span> <span class="method">__len__</span>(<span class="var">self</span>):
        <span class="return">return</span> <span class="method">len</span>(<span class="var">self</span>.<span class="var">memory</span>)


    
<span class="var">memory</span> = <span class="clazz">ReplayMemory</span>(<span class="num">10000</span>)
</code></pre>
                    <p>
                        <ul>
                            <li>8번째 줄: 재현 메모리 버퍼의 크기 설정.</li>
                            <li>10 ~ 11번째 줄: 모델이 선택한 action, action을 선택한 시점의 state, 이로 인해 계산되는 next state, reward를 저장하는 함수.</li>
                            <li>13 ~ 14번째 줄: 학습을 위해 랜덤으로 데이터를 샘플링할 때 사용하는 함수.</li>
                            <li>21번째 줄: 재현 메모리 버퍼 설정.</li>
                        </ul>
                    </p>







                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px">&ldquo;</span>
                        <span>DQN 모델</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제 DQN의 모델을 구성하는 코드입니다.
                        Cart pole 세우는 task는 그리 복잡하지 않기때문에 아주 간단한 MLP 모델로 구성합니다.
                    </p>

<pre><code><span class="reserved">class</span> <span class="clazz">DQN</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">config</span>, <span class="var">case_num</span>, <span class="var">device</span>):
        <span class="clazz">super</span>(<span class="clazz">DQN</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">hidden_dim</span> = <span class="var">config</span>.hidden_dim
        <span class="var">self</span>.<span class="var">case_num</span> = <span class="var">case_num</span>
        <span class="var">self</span>.<span class="var">device</span> = <span class="var">device</span>
        <span class="var">self</span>.<span class="var">model</span> = <span class="clazz">nn</span>.<span class="clazz">Sequential</span>(
            <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="num">4</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>),
            <span class="clazz">nn</span>.<span class="clazz">ReLU</span>(),
            <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">case_num</span>)
        )


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">x</span>):
        <span class="var">x</span> = <span class="var">x</span>.<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
        <span class="var">x</span> = <span class="var">self</span>.<span class="var">model</span>(<span class="var">x</span>)
        <span class="return">return</span> <span class="var">x</span>
</code></pre>
                    <p>
                        <ul>
                            <li>4번째 줄: 모델의 hidden dimension.</li>
                            <li>5번째 줄: 모델이 취할 수 있는 action의 종류 수. Cart pole에서는 좌, 우 2개.</li>
                            <li>7 ~ 11번째 줄: DQN 모델의 MLP. <span class="highlight" style="color: rgb(0, 3, 206);">4라는 숫자는 모델이 현재 state [cart position, cart velocity, pole angle, pole angular velocity]의 4가지 종류의 상태를 받기 때문임.</span></li>
                            <li>14 ~ 17번째 줄: 모델이 학습할 때 거치는 부분.</li>
                        </ul>
                    </p>




                

                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>DQN 학습</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제 DQN을 학습하는 코드입니다.
                        아래 코드의 config.의 부분은 <a href="https://github.com/ljm565/cartpole-DQN" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">GitHub 코드</span></a>에 보면 src/config.json이라는 파일에 존재하는 변수 값들을 모델에 적용하여 초기화 하는 것입니다.
                        그리고 self. 이라고 나와있는 부분은 <a href="https://github.com/ljm565/cartpole-DQN" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">GitHub 코드</span></a>에 보면 알겠지만 학습하는 코드가 class 내부의 변수이기 때문에 있는 것입니다.
                    </p>

<pre><code><span class="reserved">def</span> <span class="method">select_action</span>(<span class="var">self</span>, <span class="var">state</span>, <span class="var"><span class="var">phase</span></span>=<span class="str">'train'</span>):
    <span class="var">eps_threshold</span> = <span class="var">self</span>.<span class="var">config</span>.eps_end + (<span class="var">self</span>.<span class="var">config</span>.eps_start - <span class="var">self</span>.<span class="var">config</span>.eps_end) * <span class="clazz">math</span>.<span class="method">exp</span>(<span class="var">-1</span> * <span class="var">self</span>.<span class="var">steps_done</span> / <span class="var">self</span>.<span class="var">config</span>.eps_decay)
    <span class="var">self</span>.<span class="var">steps_done</span> += <span class="num">1</span>

    <span class="annot"># choose bigger action between left and right</span>
    <span class="return">if</span> <span class="clazz">random</span>.<span class="var">random</span>() &gt; <span class="var">eps_threshold</span>:
        <span class="return">with</span> <span class="clazz">torch</span>.<span class="clazz">no_grad</span>():
            <span class="return">return</span> <span class="clazz">torch</span>.<span class="method">argmax</span>(<span class="var">self</span>.<span class="var">q_net</span>(<span class="var">state</span>), <span class="var">dim</span>=1, <span class="var">keepdim</span>=<span class="reserved">True</span>)
    
    <span class="annot"># random action between left and right</span>
    <span class="return">return</span> <span class="clazz">torch</span>.<span class="method">tensor</span>([[<span class="clazz">random</span>.<span class="var">randrange</span>(<span class="var">self</span>.<span class="var">case_num</span>)]], <span class="var">dtype</span>=<span class="clazz">torch</span>.<span class="var">long</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)


<span class="reserved">def</span> <span class="method">train</span>(<span class="var">self</span>):
    <span class="return">if</span> <span class="method">len</span>(<span class="var">self</span>.<span class="var">memory</span>) &gt;= <span class="var">self</span>.<span class="var"><span class="var">batch</span>_size</span>:
        <span class="var">batch</span> = <span class="var">self</span>.<span class="var">memory</span>.<span class="var">sample</span>(<span class="var">self</span>.<span class="var"><span class="var">batch</span>_size</span>)
        <span class="var"><span class="var">state</span>s</span>, <span class="var">actions</span>, <span class="var"><span class="var">reward</span>s</span>, <span class="var"><span class="var">next_<span class="var">state</span></span>s</span> = <span class="clazz">zip</span>(*<span class="var">batch</span>)

        <span class="annot"># batch data and current q</span>
        <span class="var"><span class="var">state</span>s</span> = <span class="clazz">torch</span>.<span class="method">cat</span>(<span class="var"><span class="var">state</span>s</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
        <span class="var">actions</span> = <span class="clazz">torch</span>.<span class="method">cat</span>(<span class="var">actions</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
        <span class="var"><span class="var">reward</span>s</span> = <span class="clazz">torch</span>.<span class="method">cat</span>(<span class="var"><span class="var">reward</span>s</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
        <span class="var"><span class="var">next_<span class="var">state</span></span>s</span> = <span class="clazz">torch</span>.<span class="method">cat</span>(<span class="var"><span class="var">next_<span class="var">state</span></span>s</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)

        <span class="annot"># finding current q and max q values</span>
        <span class="var">curr_q</span> = <span class="clazz">torch</span>.<span class="method">gather</span>(<span class="var">self</span>.<span class="var">q_net</span>(<span class="var"><span class="var">state</span>s</span>), <span class="var">dim</span>=<span class="num">1</span>, <span class="var">index</span>=<span class="var">actions</span>)
        <span class="var">max_next_q</span>, _ = <span class="clazz">torch</span>.<span class="method">max</span>(<span class="var">self</span>.<span class="var">target</span>(<span class="var"><span class="var">next_<span class="var">state</span></span>s</span>), <span class="var">dim</span>=<span class="num">1</span>)

        <span class="annot"># target q</span>
        <span class="var"><span class="var">target</span>_q</span> = <span class="var"><span class="var">reward</span>s</span> + <span class="var">max_next_q</span> * <span class="var">self</span>.<span class="var">config</span>.gamma
        <span class="var"><span class="var">target</span>_q</span> = <span class="var"><span class="var">target</span>_q</span>.detach()
        
        <span class="annot"># training</span>
        <span class="var">self</span>.<span class="var">optimizer</span>.<span class="method">zero_grad</span>()
        <span class="var">loss</span> = <span class="var">self</span>.<span class="var">criterion</span>(<span class="var">curr_q</span>.<span class="method">squeeze</span>(), <span class="var"><span class="var">target</span>_q</span>)
        <span class="var">loss</span>.backward()
        <span class="var">self</span>.<span class="var">optimizer</span>.<span class="method">step</span>()



<span class="annot"># deine memory class</span>
<span class="var">self</span>.<span class="var">memory</span> = <span class="clazz">ReplayMemory</span>(<span class="num">10000</span>)

<span class="annot"># environment define</span>
<span class="var">self</span>.<span class="var">env</span> = <span class="clazz">gym</span>.<span class="method">make</span>(<span class="str">'CartPole-v1'</span>)
<span class="var">self</span>.<span class="var">case_num</span> = <span class="var">self</span>.<span class="var">env</span>.<span class="var">action_space</span>.n

<span class="annot"># model define</span>
<span class="var">self</span>.<span class="var">q_net</span> = <span class="clazz">DQN</span>(<span class="var">self</span>.<span class="var">config</span>, <span class="var">self</span>.<span class="var">case_num</span>, <span class="var">self</span>.<span class="var">device</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
<span class="var">self</span>.<span class="var">target</span> = <span class="clazz">DQN</span>(<span class="var">self</span>.<span class="var">config</span>, <span class="var">self</span>.<span class="var">case_num</span>, <span class="var">self</span>.<span class="var">device</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
<span class="var">self</span>.<span class="var">target</span>.<span class="method">load_state_dict</span>(<span class="var">self</span>.<span class="var">q_net</span>.<span class="method">state_dict</span>())
<span class="var">self</span>.<span class="var">target</span>.<span class="method">eval</span>()

<span class="annot"># optimizer and loss function define</span>
<span class="var">self</span>.<span class="var">criterion</span> = <span class="clazz">nn</span>.<span class="clazz">SmoothL1Loss</span>()
<span class="var">self</span>.<span class="var">optimizer</span> = <span class="clazz">optim</span>.<span class="clazz">Adam</span>(<span class="var">self</span>.<span class="var">q_net</span>.<span class="var">parameters</span>(), <span class="var">lr</span>=<span class="var">self</span>.<span class="var">lr</span>)
<span class="var">self</span>.<span class="var">steps_done</span> = <span class="num">0</span>

<span class="annot"># training</span>
<span class="var">self</span>.<span class="var">q_net</span>.<span class="method">train</span>()
<span class="return">for</span> <span class="var">episode</span> <span class="return">in</span> <span class="clazz">range</span>(<span class="var">self</span>.<span class="var"><span class="var">episode</span>s</span>):
    <span class="var">state</span> = <span class="var">self</span>.<span class="var">env</span>.<span class="method">reset</span>()
    
    <span class="return">for</span> <span class="var">t</span> <span class="return">in</span> <span class="clazz">count</span>():
        <span class="var">state</span> = <span class="clazz">torch</span>.<span class="clazz">FloatTensor</span>([<span class="var">state</span>])
        <span class="var">action</span> = <span class="var">self</span>.<span class="method">select_action</span>(<span class="var">state</span>)

        <span class="var">next_<span class="var">state</span></span>, <span class="var">reward</span>, <span class="var">done</span>, <span class="var">_</span> = <span class="var">self</span>.<span class="var">env</span>.<span class="method">step</span>(<span class="var">action</span>.<span class="method">item</span>())

        <span class="return">if</span> <span class="var">done</span>:
            <span class="var">reward</span> = <span class="num">-1</span>
        
        <span class="annot"># push to memory</span>
        <span class="var">self</span>.<span class="var">memory</span>.<span class="method">push</span>(<span class="var">state</span>, <span class="var">action</span>, <span class="var">reward</span>, <span class="var">next_<span class="var">state</span></span>)

        <span class="annot"># update Q networks</span>
        <span class="var">self</span>.<span class="method">train</span>()

        <span class="annot"># update state</span>
        <span class="var">state</span> = <span class="var">next_<span class="var">state</span></span>                

        <span class="return">if</span> <span class="var">done</span>:
            <span class="return">break</span>
        
    <span class="return">if</span> <span class="var">episode</span> % <span class="var">self</span>.<span class="var">config</span>.target_update_duration == <span class="num">0</span>:
        <span class="var">self</span>.<span class="var">target</span>.<span class="method">load_state_dict</span>(<span class="var">self</span>.<span class="var">q_net</span>.<span class="method">state_dict</span>())
        <span class="var">self</span>.<span class="var">target</span>.<span class="method">eval</span>()


<span class="var">self</span>.<span class="var">env</span>.<span class="method">render</span>()
<span class="var">self</span>.<span class="var">env</span>.<span class="method">close</span>()
</code></pre>
                    <p>
                        <span style="font-size: 20px;"><b>Action 선택 함수</b></span>
                        <ul>
                            <li>1 ~ 11번째 줄: DQN 모델 결과로 action을 선택하는 코드.</li>
                            <li>2 ~ 8번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">steps_done이라는 변수를 바탕으로 학습이 많이 진행 되었을 때(학습이 많이 되어 DQN 모델의 신뢰도가 높을 때) threshold를 계산하여 모델이 선택한 action을 내어줌.</span></li>
                            <li>10 ~ 11번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">steps_done이라는 변수를 바탕으로 학습 초기일 때(학습이 덜 되어 DQN 모델의 신뢰도가 낮을 때) threshold를 계산하여 random 선택을 더 많이 하게 함. 즉 학습 초기에 random으로 여러가지 케이스 탐색을 하게 함.</span></li>
                        </ul>

                        <br><span style="font-size: 20px;"><b>DQN 모델 파라미터 업데이트 함수</b></span>
                        <ul>
                            <li>14 ~ 37번째 줄: 모델의 파라미터가 실질적으로 업데이트 되는 함수.</li>
                            <li>15 ~ 17번째 줄: 재현 메모리에 저장된 데이터들을 batch size만큼 랜덤으로 추출.</li>
                            <li>19 ~ 23번째 줄: 리스트로 된 데이터를 torch tensor로 변경.</li>
                            <li>26번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">추출된 데이터 중 그 당시의 state에 대한 action 값(float value)을 가져오기 위해 현재 0, 1로 이루어진 이산 action 값을 바탕으로 위치 파악 후 action value를 가져옴.</span></li>
                            <li>27번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Next state에 대한 action value를 가져옴.</span></li>
                            <li>30 ~ 31번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">학습에 사용하기 위한 target value를 만들어줌.</span></li>
                            <li>34 ~ 37번째 줄: Q network를 loss를 바탕으로 업데이트.</li>
                        </ul>
                        
                        <br><span style="font-size: 20px;"><b>DQN 에피소드 부분</b></span>
                        <ul>
                            <li>42번째 줄: 최대 10,000의 데이터를 저장할 수 있는 replay memory 클래서 정의(재현 메모리에서 설명한 class).</li>
                            <li>45 ~ 46번째 줄: gym 환경 정의 및 action case 개수 정의(좌우 2).</li>
                            <li>49 ~ 52번째 줄: Q network와 target network를 정의. <span class="highlight" style="color: rgb(0, 3, 206);">target network는 Q network와 동일하게 초기화</span></li>
                            <li>55 ~ 57번째 줄: Optimizer, loss function 정의.</li>
                            <li>60 ~ 91번째 줄: 학습 Episode가 일어나는 부분.</li>
                            <li>62번째 줄: 학습 환경 초기화. 어느 수준 랜덤으로 cart pole 위치 및 각도가 초기화 되는 부분.</li>
                            <li>65 ~ 66번째 줄: 현재 상태에 대한 다음 취할 action을 선택하는 부분.</li>
                            <li>68번째 줄: 선택한 action을 취했을 때 나타나는 next state, reward, done (cart pole이 넘어졌는지 여부) 데이터를 내어주는 부분.</li>
                            <li>70 ~ 71번째 줄: Cart pole이 넘어졌으면 reward를 -1로 설정(gym 라이브러리에서는 모든 reward가 1로 설정되어있기 때문).</li>
                            <li>74번째 줄: 이렇게 얻은 데이터를 재현 메모리에 저장.</li>
                            <li>77번째 줄: 재현 메모리에 쌓인 데이터를 바탕으로 Q network 학습.</li>
                            <li>82 ~ 83번째 줄: 만약 cart pole이 넘어졌으면 episode 중단.</li>
                            <li>85 ~ 87번째 줄: 일정 주기마다 target network를 최신화 해주기 위해 Q network의 파라미터로 clone.</li>
                        </ul>
                    </p>








                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>DQN 학습 결과</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        아래는 학습 episode별로 cart pole이 버틴 step (duration) 수입니다.
                        Episode가 50보다 작을 때는 20 duration도 버티기 버거웠지만 학습이 진행될 수록 오래 버티는 것을 알 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 gym 라이브러리의 최대 duration 상한선이 500으로 설정 되어있어서, 그 보다 더 버텼더라도 학습이 조기 종료 됩니다.
                        따라서 최대 duration이 500을 넘어가지 못하는 것을 아래 그래프에서 확인할 수 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemdl820mTU_8qpvplSYoTVNR6hSEaofiXFbPGFWJ8NIQTM2eWUYhK-i0u21qQ8WNOTSzvB35Vz_HmDdd4f41z4S7e82ZZikap6CRzWVmSL3qK2Ch3bv2-9CCNrDB_U8CLA5UiE1CJ9TmQhHGsi1k3GmTzZK03qL917kKe4axBHxftxWZur2DiwtvlJzKDY7khvzNeVqQw04i3ZIVzYBmJTtF3hKAik4lJVByeXkuqGm-KCeEfGVN6VnV_A_J89Z02UB9862Tt-tPRPXFcs65zX886jcxIZVDHMx2Z6DqudSBv8IZk1Yhf_9c14xYd9FFXDOtaVhSl5wkA-C21wt5bRdLKoO0wagE2nOM5ZwvMZnWvPwk097xRLqRCnwBWTD8AmNj01NSCaMROA84TAaqMfBVKaFAjTGg28QAYDOKC6yKn55suc_ryUbGNqiAS1nWp1j9_1AbW4H6pXM_yt3Oxz_WYKKYpMucwe4QN3b3YpyhZj6KLQrZCr5EyTm2-ddgE1NEXnxlUtg6UWsBPyRloWAelFAyVkAK8n0On12AjwQkBVAn0oZn-mXyBTMSZHd39zd0d5mBUoe5nUlp8-yvRbw6EgfhLVN824yC3PVln-doSJSOYyCLZMfaxHwXYiDMbWbye6LqKSgrHaYCfjSHhXANuQUtuLEFIlpV-SoWCLFepPdCz2B0Sd-dRFhG0jxBj4AaUy2p-31vwUbZBNAz6n2-DsWcI7uX7VX8YNMzJ_ckem8H1UqCTsdVzTaWbrQzEB0YX1_iXKBhkxkEV3LZVbq6Jn0TG3u7GA0ldQtnmIXW0WiY068t2Kq2MKkBWXCwOHAmuhrOY4U0FVf1RwglzSlAqqF2ei1TlwVYdqXHYGG9YNbc7v70CoPIPbhHiokRPKXe2RtFVzI2A0OqPt11FCUEQfUVainhYr7atkWXeHBtiwwrmEY-SJqT99fg60VIKfTxIaplwGDKK7U7pVdRhPj5l88VTTgHl8hAX-axCRsIfXuvKRtj0iA3SFNGMNwBgB_UXBxqoXQB0VA-etFEqzZaJKiuu-YD53dT_kq4OYktz8JAzFoy3yAmIeqCD8L_AKK8KzDjUkgOec6aETnWnL0U9trRdiVsBiDR40wuYVBwwJ3z74DwskKhQPKUqW2hlu_zqC4DVL0tM6uN8JuF2OBPzgZiHz8HUenr_zQqwO0jH76iEzCWz99mwLHkS_gwNRgHWFUtAv5fKPVefs_WvXqCjzlyhemdoSoCAZNd-htT4sbFSd_yquARyjGljYhF_7KRA-bFm27eLyS0cCU9kEcisn_gWeioqVDCVEoBXuTqNRXD-DT-Qs2DftT_dyGM6Vo2mQoMNzvIJINmh21l0f6lgZLMTejwxv7Mi_Q40Dy-HapyZVfKdGqPWdXL1ftfNy8HlI9Nz-yHCWy5h_y_g74m_LTtmJ_phP05Yv219jNGrTfAvgrjucX1VdA43jn1k_riplJ5VuopiA5Ysk-j78Q" style="width: 100%;">
                        <p class="caption">학습 에피소드별 cart pole 버틴 duration 수</p>
                    </div>
                    <p>
                        <br>아래는 episode 중 500 step을 버틴 cart pole 강화학습 결과입니다.
                        학습이 잘 된 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemeVPj8GotyzzoLf5ewWPlLx0aRrKGrhZBHQCE16mLKxRGRNJQ8SKLeS5npzN7MKwlMm2CqknMmT7W9h8rffYTzs8yO4Xodi5eh0jLW6aqTEJPvHeP_de899Mf7pYrIC8E_cLqBwym6Aha4pOf2d6CmR-AdlNBilIefEvg_v3mq6nO9_VuYEvNre00nPOtjHe2yy-d9dZI03ear3r6ybvdr1n5H9wZCm2a_rsW2ZSSHjHOpjdNFgmL-78XfgK83dQWdsTlwno2_MpeEUp4bkuK7AOHhbAdJ-0YXan477d7KW8-p-VwIZIJy3ntefI6poaVqCpbgLSqSsF4l29SZ18mFxVCtYyKf2AiQ5I-mncNMAWBJlqWE-9KGtW2zqkozjafjF9lbFE2UI-QMgUgcUuF1wgfmdeyMe1l8V7e4nOMY_xSYZyKosbwD9KJD2Pl7Kd7ynOmsm4E6mk3M5pwquOV2cCzj1lN0D1jQ8hNuiAeiVzbMW0rlbY_XQeX5U2z7L06Cy0hipH1DSAV4Q8JtGef3EyImCdpMEO0sdjIbyGCAjGVSmWH-maDOBNmrqU0JEytfZKlASQGXcdc2ZbTTxdYw6p7P3O_syFKEfGfLjI1V1kQOG91eEeBfmpoRM-RRv2Vc7HdU0Bjvot8LJYMDuOB4AU4nEkufIXJKDcAxIzvlqNx5XCa7qZ7NjdTNJQjCWWwUoNORLILyguTSWkH1Vwplem8aWedXvFvTAFzCy7zmm2jTltQ87xrQB-yL1cQFLoMPHWdIgcNBlyXQsRqYRzuIiRDuS3gKUQsSe9xRkWH9mJQqYQ8PTeaKwsqwehacZnpGpHwWX_GJPLQ3ouvXA7rvWyqKYJO_ob86491m-esEt2yS7U6DJ0ltgLJsIll_G17X5kdQ87iP7e4cP4X1SOq-GAgNeOxAIUfYthGyzT-EoljOmLx_x70OZ2I78z3qnvLR_aGhjlPtWaFgcUjdynsx6ts_TC0h4Mly7e9TR-z8zPSDMmSC2FkysWMauLuGjZnu0HsotwgDHrl7AlCis_CDWUIzmCJMInzrpn4BZ5SBpR5PBPGw59KbnwqbmFlyz4sxJy5ZKuLXl8RPZ6Cn0WKrbWCaHuK6QxMUxYf4kfJwsPxaLI7ChmolVVT_G0m5kAylmhkow470vJLIdZGbOYqs-TVClv2y1-oadZ7L5z1KrXHPWGu2Cs5wFSGMQXhfhw4CHgPx_Y5xm6Pe2YOKhLWdrD_hcWBS8YAg-nop0B1mMfuno6mCS0VuLFNiz4w6EXqXU5KHdJlS2XguK3IOJepLwXtRvBBLvJX7zqEUcvvBs3fdVjXwNr5tI-Y0Nsfu3VHrauAhzkgA3g6RsE9FkmZerLNdlF4K3WgG54tz3cxVuYRd3pjPTOIOjxeKktay6ZLVlt5mldInB2vxOxg6oOzHvX1DtbOnBUweOWYzGBEPwWXA2nqVBN_VWp28fbdyX9qHGCFF6EuNddOJJyozOT-s" style="width: 100%;">
                        <p class="caption">Cart pole 학습 결과, 500 duration</p>
                    </div>
                   

                    <p>
                        <br><br><br>지금까지 DQN을 이용하여 cart pole을 강화학습 해보았습니다.
                        이렇게 간단한 task는 DQN만 사용하더라도 상당히 잘 작동하지만 복잡한 task에 대해서는 여전히 불안정한 감이 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">여담으로 OpenAI에서 개발한 강화학습 알고리즘인 Proximal Policy Optimization (PPO)는 간단하고 성능도 좋아 최근에 가장 많이 쓰는 기법 중 하나입니다.</span>
                    </p>

                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#DQN&emsp;#CartPole&emsp;#gym
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="pjaxPage('dqn1.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br>Deep Q Learning (DQN)</span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="alert('DQN 마지막 게시물 입니다.\n\nThis is the last post of DQN.')" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br></span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>