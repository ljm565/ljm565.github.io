<!DOCTYPE html>
<html>
    <head>
        <title>딥러닝 강화학습의 시초, Deep Q Network (DQN)</title>
        <meta name="description" content="딥러닝을 접목한 강화학습 알고리즘인 DQN에 대해 설명합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/dqn1.html" />
        <meta property="og:title" content="딥러닝 강화학습의 시초, Deep Q Network (DQN)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="딥러닝을 접목한 강화학습 알고리즘인 DQN에 대해 설명합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/ALs6j_HgJsC5YdzS53KpJ1wJki6wSfNyuEO8upv47G9OnyF7lnsWnvRsd4nN95wxsYRiy6t1eztvvfCX6KnYiRPIGliCbfWyjrzUXocmqm7NOi3qbvhPE9WLBoK18JjZi1OGJXSY_SpIBFQdYFQI-S_im2nPk69OXm6HWdJHgJEaKusSkmwQNljswyuY8q57ykQ2qrnKpEehoC8t7SezlWGwqu51M7r1TdKEtHQBRvMTnU96dIwxmstI0QZQAFjJcYwg4ElwCRhn5j4IAv1cHxZfSWduuyGiWGNCHwkVoWnbb8o2LETgx_v-OpunZ4vKBCGJN4HA2PsWneKxfPErlfdBLZhSeaII9-CBbQvl7g3hxs816p_0yQkUasgYSCKBFvFKnltdblj2MxB-kTjJRAF347TqcZZGg0W47Q6ZYf__TuQR-GQeb9h99CH1iv7ED6qsXPoyJVVsP8mwKD_3jCcTW-YsxybtTgtury-GXOwymj1STvCUl_Qk8n_Z-PP8rgpU1YYuCKjLahN3ZUTj43wTnnt8nQkk_zdT2iAKKXdykihqYgpJ_HUC8Sh713-R5XRevzFeY-W6Rp-bk-Bc6sGY4f_iOW-yOFPUfX7ReW6uPl7V5SOuLrG6NCofCCQmJTm1Nfk2tS6sW3ROVKymuPU3_bq86aDNi6Eg1zjF3YtMGgJXLUuIuTfjuTg_iRWPKue5fNkHkIgNjgRgVjjNgXVhbG0NnxBPNWEUZlVh2WhdNp5hr9Rm4B1ERYuKGztM8RJn_MDHqUsO89d_V-2Nk_yF5Xzi1ocT6xZweVBxZ-efvJxM2De-YywDnPWjonFp73m6eqd2mbV7oNTlH6Vrs8uSm0RJmjtFtaBQR1o76B4mxsS63XIb2Q9GtrdACyqxSGSDCQeyWiSP05Kicju7DqaupgiqhjitgFNh9Rh1uaDsa1jt1-KCszF-xKdRGsSalFH2ZMzDK9h0la90k7dkqgnomjKAH_-WP4axP6wHIUiT007UT-6YbUl5sazUdH477zkCddHe33FNqsDBbLQNPnyOA7GKmBmTkYziV6Drr3ybB3ab3iP2QTTbndGrukWQNcNx9-Kz3roFhnSFwojVw2DGrsHkwmP77FcXd5-3oQa9rYVBxn2gyYKqV4CnWUAE3aGTcVHfybcEUf_WJV8NkhX2bdQN76dn2_qid-AMx7W2kI1X29HRcnksKVe4UdGpzlrzJUCS7NT2YYUJ-6IXqZSyP-PzCIli1K0p4C6ZZt6nHAyms7-uvLGCQ9CjU9z4oZQTZ4t-6rGuNCUXwwx3E49QMP9iu2a_urfjwRY2CDoyZpqnqDTCTZtwkIvu7Wa_x1NHl17jA4jLcwCdPixaFnLhvbuhUknEl0B4P1G30Jsgvc6MeB4eRRWvjuB9w_ewgPCYxv0Yzt0i8wV2QpmFgrg1fh_wAAeYqW38fxsfJXEb84VGtZWcxKLiT_iM0kgDSs_Xd2MUkS5__zOqmuX0Dyr8zvwt4ft74h4v8zQpSuTXNP64RC-C_2J8rGtNpYluhq7srYSWw61lKUoA" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Deep Q Learning (DQN) / 1. 딥러닝 강화학습의 시초, Deep Q Network (DQN)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/ALs6j_HgJsC5YdzS53KpJ1wJki6wSfNyuEO8upv47G9OnyF7lnsWnvRsd4nN95wxsYRiy6t1eztvvfCX6KnYiRPIGliCbfWyjrzUXocmqm7NOi3qbvhPE9WLBoK18JjZi1OGJXSY_SpIBFQdYFQI-S_im2nPk69OXm6HWdJHgJEaKusSkmwQNljswyuY8q57ykQ2qrnKpEehoC8t7SezlWGwqu51M7r1TdKEtHQBRvMTnU96dIwxmstI0QZQAFjJcYwg4ElwCRhn5j4IAv1cHxZfSWduuyGiWGNCHwkVoWnbb8o2LETgx_v-OpunZ4vKBCGJN4HA2PsWneKxfPErlfdBLZhSeaII9-CBbQvl7g3hxs816p_0yQkUasgYSCKBFvFKnltdblj2MxB-kTjJRAF347TqcZZGg0W47Q6ZYf__TuQR-GQeb9h99CH1iv7ED6qsXPoyJVVsP8mwKD_3jCcTW-YsxybtTgtury-GXOwymj1STvCUl_Qk8n_Z-PP8rgpU1YYuCKjLahN3ZUTj43wTnnt8nQkk_zdT2iAKKXdykihqYgpJ_HUC8Sh713-R5XRevzFeY-W6Rp-bk-Bc6sGY4f_iOW-yOFPUfX7ReW6uPl7V5SOuLrG6NCofCCQmJTm1Nfk2tS6sW3ROVKymuPU3_bq86aDNi6Eg1zjF3YtMGgJXLUuIuTfjuTg_iRWPKue5fNkHkIgNjgRgVjjNgXVhbG0NnxBPNWEUZlVh2WhdNp5hr9Rm4B1ERYuKGztM8RJn_MDHqUsO89d_V-2Nk_yF5Xzi1ocT6xZweVBxZ-efvJxM2De-YywDnPWjonFp73m6eqd2mbV7oNTlH6Vrs8uSm0RJmjtFtaBQR1o76B4mxsS63XIb2Q9GtrdACyqxSGSDCQeyWiSP05Kicju7DqaupgiqhjitgFNh9Rh1uaDsa1jt1-KCszF-xKdRGsSalFH2ZMzDK9h0la90k7dkqgnomjKAH_-WP4axP6wHIUiT007UT-6YbUl5sazUdH477zkCddHe33FNqsDBbLQNPnyOA7GKmBmTkYziV6Drr3ybB3ab3iP2QTTbndGrukWQNcNx9-Kz3roFhnSFwojVw2DGrsHkwmP77FcXd5-3oQa9rYVBxn2gyYKqV4CnWUAE3aGTcVHfybcEUf_WJV8NkhX2bdQN76dn2_qid-AMx7W2kI1X29HRcnksKVe4UdGpzlrzJUCS7NT2YYUJ-6IXqZSyP-PzCIli1K0p4C6ZZt6nHAyms7-uvLGCQ9CjU9z4oZQTZ4t-6rGuNCUXwwx3E49QMP9iu2a_urfjwRY2CDoyZpqnqDTCTZtwkIvu7Wa_x1NHl17jA4jLcwCdPixaFnLhvbuhUknEl0B4P1G30Jsgvc6MeB4eRRWvjuB9w_ewgPCYxv0Yzt0i8wV2QpmFgrg1fh_wAAeYqW38fxsfJXEb84VGtZWcxKLiT_iM0kgDSs_Xd2MUkS5__zOqmuX0Dyr8zvwt4ft74h4v8zQpSuTXNP64RC-C_2J8rGtNpYluhq7srYSWw61lKUoA);">
                    <div>
                        <span class="mainTitle">딥러닝 강화학습의 시초, Deep Q Network (DQN)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2023.03.11</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번에는 2013년 DeepMind에서 공개한 딥러닝을 접목한 강화학습(Reinforcement Learning, RL)의 대표적인 알고리즘인 DQN에 대해 설명해보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">본 글에서는 Deep Q Network (DQN)을 설명하기 앞서 강화학습의 간단한 분류 체계와 Q-learning에 대해 살펴보고, DQN에 대해 설명해보겠습니다.</span>
                        
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>강화학습의 분류 체계</li>
                            <li>Q-learning</li>
                            <li>DQN</li>
                            <li>DQN 결과</li>
                        </ol>
                    </p>
                    <p>
                        <br>그리고 아래는 DQN의 논문입니다.
                        DQN의 논문은 2가지인데 2013년에는 NIPS에 2015년에는 Nature에 낸 논문입니다.
                        그리고 본 글에서 설명하는 내용은 2015년에 나온 내용으로, 2013년도 연구를 더 개선한 방향에 대한 내용입니다. 
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/1312.5602.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">DQN 논문 (2013)</a>
                    </div><br>
                    <div class="link">
                        <a href="https://www.nature.com/articles/nature14236" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">DQN 논문 (2015)</a>
                    </div>



                    <h1 class="subHead">Deep Q Learning (DQN)</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>강화학습의 분류 체계</span><br>
                        <span>Replay Memory</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        먼저 강화학습을 위한 알고리즘의 분류 체계를 살펴보겠습니다.
                        아래 그림은 강화학습의 방법, 환경에 따라 분류한 아주 유명한 그림입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_F2AyIenQfIww7LtGdeY_-R3LVg1U9OBgLhxHX0mritjn5RUBjOCDE1TeXe191UOJ8gtvkrCUK4PxWXxndREGCh4Ill8lGMV64svE6ZDMYFKMTLrYppJXV9z7p3Z2_IiU2sUwHX-H1Avro9NkvgWdHi9CK-pd5buMTBGuiYLsR99lqix_OM-wH5IH_ktAggK1SXeD5VH_iF7RBqjp0JnbOD2atKpTVwJg40IessiGkGQHOgKlWhVY4tnfdZpUWo0R1r_F90u97p10EbSOXbmqmUfDl2WcQPvuuoEXZh3j4w0xwXP7NUo6_uCO7kMjcHzWMQdumYXS1NqiUUm4Y-Muy7Ox0VlHqdTHjPnFsmNVauIxmywUnGQsYmwGBBLbSSvmR53cG16mYFwzhiZxvqwfNlT1VcFg2HAGMK8umMLkm0Vctfn1ZcXpG9qqlsSjh16SeoDN3qhdM7IX3HcBLstB2CkT6GVxOkwfBshFemomJCE3GbYLfcqhekIZqAq1FCzd7NexAHNofkOl3Rf7AS4rwP2Dx-EgbfOdR4mRFgbwy-oEdrRcoVZIuYWhmE0vESH8K2UZryywC9aZraiQmBrjtQgpt3zygzzXpV5wP38JDyuDn1z_fSLycpMn5S5Bz-z-amnBLMPyvnBOUOI9ls-Zp6vwkqd0e8khFrvoFqJnEQOnQSsFPJjsLjBw-oreVESMmsygo6z-c7DQ6f16fX5_xOU0RROX-EBDsvnMkKwxrLHcvI9hjQFXCTbZn1y4LPPoNEK6xqnOtyxhvxSp7iNFq0afTgrT68yW3VFUVfUTXIA1p9xIz7MXpRckbYz6XgXfE-MJqe9FmFZXX7sn_ruebJmCu0blL2MssFr-lCLpPvwt2KjGvLjin4OCiV-UnkrDcOT7r9pK3ZQf0-xiM_VsIZGHRQaij332Vj8TZU7XgXMz_dkYqL5ltKecpsupDjlEekQTgm_VHy3nPts0M5So6EtBLT_18uEhjIUoM9JzRQqU4PqX7mxVhH_AOHXdhiOWfWbhA92nE4vBZbN2EOcm-EQymR1EuKXwm0DmiyS6trqyLnXrRNHpG63rFd9C7pFUATSgBokLCVDUUStU3vL2Bzf7g7GfF3jqH9wLQqQwhd4FJ6GK_lKCq7QNODeSUXz0ar18HAZzN5Y7k1w-vQrW9eSaHG_SWBU8ZmM-DMYPO0XU0odlSvAUsS4MCEHxNSrOJ6eRUSY8q9vaN1WExuqkcY66YjEMGLSH3h2yu7tqOijJvfN37vv1QBzzf7GYLZAYuLyFR0E7mG8KvqCOo2zsuVf3NxDVVDoPklQfFRRePtTkI5x0dozeKP-1V5PYZ5jFnWk39v1iSgFM7v9lEI2-BSy71SNfYmFM0kJ7J-Roh0BbzMz8kRy1lEiPqqQ4eziENbjh3OpQG9uIeLKzHVRwgqY6JrQZibIc-AXutA7VDSzajOsS8yA4DlQzFvC9iFedP-1D2zfFa-OsYW6BoHIKXlhd0s9QAEXto0Q2k-8LggIStC02W_0kviq4quNEtk6pk7zBHxlLFB" style="width: 100%;">
                        <p class="caption">강화학습 분류 체계</p>
                    </div>
                    <p>
                        <br>이렇게 강화학습의 종류는 다양한데 어떻게 구분할 수 있는지 좀 더 자세히 살펴보겠습니다.
                        
                        <br><br><span style="font-size: 20px;"><b>Model-based RL VS Model-free RL</b></span>
                        <br>먼저 강화학습 종류는 환경 모델(environment model)에 agent가 접근가능한지 여부에 따라 나눌 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">Agent가 환경 모델에 접근 가능한 경우는 model-based RL, 불가능한 경우는 model-free RL이라고 부릅니다.</span>
                        
                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">Model-free RL은 Markov Decision Process (MDP) 와 연관된 전이 확률 분포를 사용하지 않습니다.</span>
                        쉽게 생각해서 trial-and-error 방법으로 문제를 풀어간다고 생각하면 됩니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">반면에 model-based RL은 MDP의 정보를 알고 있다는 가정하에 진행됩니다.</span>

                        <br><br>이 두 종류의 모델은 각각의 장단점이 존재합니다.
                        만약 정확한 환경 모델을 알고 있다면 agent가 보다 효율적으로 정확하게 최적의 행동을 계획하여 움직일 수 있습니다. 따라서 학습의 효율도 증가합니다.
                        하지만 실제로 환경 모델을 정확히 파악하는 것은 어려우며 불가능한 경우도 존재합니다.
                        또한 환경 모델을 구축하는 것은 어렵고 가성비가 안 좋을 수 있을 뿐더러, 부정확한 환경 모델을 구축했다면 agent의 학습 또한 제대로 이루어지지 않을 것입니다.
                        Model-free RL은 이러한 model-based RL의 특성에 비해 구현하기 쉽다는 장점이 있습니다.
                        

                        <br><br><br><span style="font-size: 20px;"><b>Value-based RL VS Policy-based RL</b></span>
                        <br>환경 모델 여부에 따라 강화학습을 나눌 수 있지만 어떤 것을 학습하는지, 즉 학습 대상에 따라서도 종류를 구분할 수 있습니다.
                        바로 value function과 policy를 학습하는 value-based RL (위 그림에서 Q-Learning), policy-based RL (위 그림에서 Policy Optimization) 입니다.
                        
                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">먼저 value-based RL은 일반적으로 Bellman Equation에 기초하여 Q function이라고 하는 value function을 근사하고, 이를 통해 policy를 구하는 방식입니다.</span>
                        이때 value function은 agent의 현재 state와 선택한 action에 따른 미래의 reward를 예측하는 함수입니다. 즉 action을 잘 할 수록 높은 value (reward)를 내어주는 함수라고 생각하면 됩니다.
                        만약 value function이 완벽하면 자연스레 agent는 가장 높은 value를 주는 방향으로 action을 하게 되고, 이렇게 되면 우리가 policy를 명시적으로 구하지 않아도 agent는 자연스레 policy 반영하게 됩니다.
                        이러한 방식을 우리는 off-policy 혹은 implicit-policy라고 부르고 대표적인 알고리즘으로 DQN이 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">반면 policy-based RL은 value를 구하는 중간과정 없이 바로 policy를 근사하는 방법으로 대표적인 알고리즘으로 Proximal Policy Optimization (PPO)가 있습니다.</span>
                        만약 policy가 완벽하면 policy를 구하기 위한 중간 과정이던 value function을 찾을 필요가 없습니다.
                        이러한 방법을 우리는 on-policy라고 부릅니다.
                        
                        <br><br>이 두 종류의 모델도 각각의 장단점이 존재합니다.
                        Policy-based RL은 다양한 agent의 action이 다양하거나 연속적인 경우(로봇 등)에 효과적이며 수렴이 value-bassed RL에 비해 안정적으로 잘됩니다.
                        다만 sampling 하는 policy와 학습하는 policy가 같기 때문에 계속해서 바뀌는 policy에서 sampling을 할 수밖에 없습니다.
                        따라서 value-based RL에 비해 sampling이 매우 비효율적이다는 단점이 있습니다.
                        마지막으로 위의 value-based, policy-based RL 방법을 모두 고려한 알고리즘도 있으며, 이를 Actor-critic 모델이라고 부릅니다. 
                    </p>








                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px">&ldquo;</span>
                        <span>Q-learning</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        DQN을 설명하기 앞서 간단하게 Q-learning에 대해 살펴보겠습니다.

                        <br><br><span style="font-size: 20px;"><b>개요</b></span>
                        <br>Q-learning은 위에서 보았듯이 model-free RL을 위한 알고리즘입니다.
                        Q-learning의 목표는 유한 마르코프 결정 과정(Finite Markov Decision Process, FMDP)에서 agent가 특정 상황에서 최적의 행동을 할 수 있는 policy를 찾기 위한 방법입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">Agent는 단순히 바로 다음 단계만을 고려하지 않고, 현재 상태부터 미래의 여러 단계들을 거쳤을 때의 결과 상태를 반영한 보상을 극대화 시키는 것을 목표로 합니다.</span>
                        즉 최적의 action을 하면 높은 보상을 받게 될 것이며, 미래의 잠재 보상도 고려하여 총 보상을 극대화 하도록 현재 action을 취할 것입니다.
                        이렇듯 표면적으로는 보상을 극대화하는 것이기 때문에 보상의 quality를 의미하는 'Q'라는 명칭이 붙게 된 것입니다.

                        <br><br><br><span style="font-size: 20px;"><b>Q-value and Bellman Equation</b></span>
                        <br>먼저 Q-value에 대해 알아보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">위에서 agent는 현재의 state에서 action을 취했을 때 reward와 미래의 잠재적인 reward를 모두 고려하여 선택한다고 했습니다.
                        그럼 이때 agent가 action을 취했을 때 예상하는 값이 바로 Q-value라고 부릅니다.</span>
                        Q-value는 말그대로 어떠한 값이며, 그 값은 아래처럼 계산합니다.
                    </p>
                    <div class="equation">
                        \[Q(s_t, a_t)=r(s_t, a_t)+\gamma r(s_{t+1}, a_{t+1}) + \gamma^2 r(s_{t+2}, a_{t+2}) + \cdots\]
                        \[=r(s_t, a_t)+\gamma\big(r(s_{t+1}, a_{t+1}) + \gamma r(s_{t+2}, a_{t+2}) + \cdots\big)\]
                        \[=r(s_t, a_t)+\gamma Q(s_{t+1}, a_{t+1})\]
                        \[\]
                        \[\therefore Q(s_t, a_t)=r(s_t, a_t)+\gamma Q(s_{t+1}, a_{t+1})\]
                    </div>
                    <p>
                        <br>위의 수식을 설명하면 이렇습니다.
                        <ul>
                            <li>\(s_t\): current state</li>
                            <li>\(s_{t+1}\): next state</li>
                            <li>\(a_{t}\): \(s_t\)에서 택한 action</li>
                            <li>\(r\): action에 대한 reward</li>
                            <li>\(\gamma\): 미래 reward discount factor (0 ~ 1 값)</li>
                        </ul>
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 Q-value는 바로 다음 action을 취했을 때 들어오는 reward와 discount 된 미래의 reward 값을 합친 전체 reward라고 볼 수 있습니다.</span>
                        그러면 간단하게 현재 state, action을 \(s\), \(a\)라고 하고 다음 state와 action을 \(s'\), \(a'\)라고 하면 아래와같이 다시 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[Q(s, a)=r(s, a)+\gamma Q(s', a')\]
                    </div>
                    <p>
                        <br>그리고 Q-value, 즉 Q-function은 항상 선택한 action에 대해 최대의 보장을 내어주어야 하므로 아래와 같이 다시 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[Q(s, a)=r(s, a)+\gamma \max_{a}Q(s', a')\]
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">즉 agent는 위의 Q-value를 최대화 하는 action을 선택하도록 학습 하는 것이 목표인 것입니다.
                        그리고 위의 식이 바로 벨만 방정식(Bellman equation)을 기본으로 하는 식입니다.</span>

                        <br><br>초기에는 agent가 random으로 action을 취하면서 새로운 곳을 탐험하도록 하여 여러 경험을 하게 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 점점 학습이 진행될수록 random으로 취하는 것보다 greedy하게 자기가 최적이라고 선택하는 action을 하게끔 조정하면서 학습합니다.
                        즉 학습 초기에는 random으로 여러 경우를 살펴보게 random으로 선택할 확률을 높여주다가 이후에는 random으로 선택할 확률을 줄여서 학습하는 것입니다.</span>
                        이렇게 완전한 greedy로 agent를 움직이게 하지 않고, 특정 random 난수를 하나 추출했는데 threshold 값을 넘지 못하면 random하게 움직이게 해주는 여지를 남겨두는 greedy를 우리는 \(\epsilon-greedy\) 기법이라고 합니다.

                        <br><br>그리고 다시 위의 Q-value를 구하는 Q-functino의 식을 살펴보면 뭔가 재귀적인 느낌이 납니다.
                        그러면 학습 초기에 agent가 아무것도 모르고 완전히 이상한 action만을 할 때 재귀로 Q-value를 구해서 업데이트하면 학습이 잘 될까라는 의문이 듭니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">하지만 이는 걱정할 필요 없는것이, Markov state라고 가정하면 벨만 방정시에서 유도된 식은 미래의 보상을 반영할 수 있다고 합니다.
                        뿐만아니라 초기 Q-value가 부정확하고 알 수 없다해도 무수히 반복을 하다보면 결국 마지막에는 정답의 Q-value에 가까워질 것이라고 합니다.</span>

                        <br><br><br><span style="font-size: 20px;"><b>Q-table</b></span>
                        <br>그럼 이제 Q-value를 구하는 방법을 알았으니 실제로 저 식을 어떻게 활용하여 agent를 학습시키는지 살펴보겠습니다.
                        먼저 위의 Q-function을 다시 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[Q(s, a)&larr;r(s, a)+\gamma \max_{a}Q(s', a')\]
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">위 식의 화살표는 현재 state에서 어떤 action을 취했을 때 받은 reward와 next state의 Q-value의 값을 현재 state의 Q-value로 업데이트 해준다는 의미입니다.</span>
                        그리고 이렇게 무수히 많이 업데이트를 하면서 각각의 state에서 구한 Q-value들을 기록하고 업데이트 해주는 곳이 바로 Q-table 입니다.
                        <br><br>아래에 예시를 들어보겠습니다.
                        아래 Q-table에서 각각의 state에서는 좌, 우, 위, 아래 총 4가지의 action을 할 수 있으며, 각각의 방향에 해당하는 action에 대한 Q-value를 보여줍니다.
                        그리고 하얀칸은 reward가 0, goal 지점만 reward가 1이라고 가정하면 아래와 같은 step으로 Q-value가 업데이트 됩니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FXmwA7g0E1j-1cHpqlXbLIPuGDAd0sTOwMcqghf_eiZhY-d-ivbKhXxAqf2VRonWuSQS2_2XvS0SWG8hu81f_xfYivl26LSvLeXd7_KWts6odyGw6Lz_wZHLlSsiSNMO99fQu86iIUP6DoJVo1xtI9WJY1hDiOPfefAk408paA7J3s4Gq2NecewRAMa3_T3FK2m7Dbi28JpAc3ZpbYN1hiCY6F2wZzT7OUf71P9MqtUfHxBilgn1M_nlneN5RP7H97_R0D73TUbeE5s1qNRTLNOfv6GAKToja8xHbeBt1m8yey0XJMgu2RKPqlvM5HxnNJjHZHy3pQcjNHujdFW5OIbxBh77UJDau2-teNDZrYD-yYOkquQNKQiJHFI6fIeN-QBPSFyB0k2l_FxyeY38R2JiOdMrDqmr-YRf_FZg7OZ2_C7KuT-CnV2jCC8aZTqjo3k0VyZN6KYxUESaKkDZNFOfYEdubY6tFsCGtXRY5Ki-WK5fF86IHV__KnHyI1J0LE_zg4cpCoKYSJtX1AUlvvSJdvvB9-F93XfRKhIfhz6jics6F36U8Iyg16twml8Trx-PKCpACxX76liwaI7K-Ax3b72GEU0INuz2GmcKH77ctbDO1kO0HIkgUzb1N2Jg5XHJJnfM2ihUQZgNV5tUMYnJnqXS-OFhvZjU42No4cIfv343LwAiYekeEMlD_fOi8YW-eYs1PeWsYhQH8j0vFOGnJ19J4MH4FTEQu7SEuFQ8hidWSz0u8_fht_As0fFCaUYzZFUusQ83oU9McWXbDseGvnL8cqEkhFuhqbfL3MePd1WBCijFRbNrz5tz4dpFkMQQtizLC_ssP1rlYHPxqhmk0-dlCwV1luxl77FzN58BM0H34pnMDGQUYPFj9fSjkPzlyBllJUGDBrmCb-R1dtX7V_eGreec9gFztl1nExjgh2lzc8vgP7DBGLHzNVXg9I_uMbvKJa-iA5WHZxL1aHH-nxt7Ps6twL1LVWZMZ8qzpAt-vIlA69SrkLvP6W9LSFSFDutJe3PN6s_EMpZzZd0yq_zUoG3VghvB-cjA4M7a_scTWjvBYi-05rLy_mkToY1stFhJ0oDMZ0AYklMZjanzaH7Bo7POPB32ZSgenZmBHqeqZuaAgCD2YSToecZ2i1bqID_2L-ppGgBTtfjc4iSdez5BDHD-8apeyyaDjKHetrQ2C73__FdmEozMLraXM1yCUSjePwjHOQjFCC650oi2nOd7y8iq2qyfc9Tdrb2qEJQIq3RwaozgF_n0-2RZNuSrkHtVjrFN17L-ouoKr_oByu4Dp0WVfXBZt-1hwrbXSK2MRGnzPUDFRCl1gF8SKG15NSlcEUXE2zPjXBgrfuMNePfNbl53G8PgLEGfXOgdl72YKtwGboClqerszbNhp4sNtqG2cMhLDk3BtYyW45dpp0ekyf6v9rcjLqotePeFtAU_5btTQZW_4sR-MN06EpHNc1T1pLVuKIOQPom9UVolqFqX3FUoLecgbV3QZOb6D2u9MylQ6us64qd0KCE3k7Z_Yz8Q" style="width: 60%;">
                        <p class="caption">Q-table을 이용한 Q-learning 예시</p>
                    </div>

                    <p>
                        <br>그리고 아래와같이 Q-value를 업데이트할 때 좀 더 smooth하게 업데이트 해주는 방법도 존재하며, 실제로는 아래의 방법을 더 많이 씁니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래는 기존 값을 새로운 값으로 완전히 덮어 씌우는 것이 아닌 기존의 값도 조금 반영하여 업데이트 해주는 방식입니다.</span>
                        이때 \(\alpha\) 값을 통해서 현재의 값을 과거의 값과 비교하여 얼마나 반영할 것인지 비율을 정할 수 있습니다.
                    </p>
                    <div class="equation">
                        \[Q(s, a)&larr;(1-\alpha)Q(s, a) + \alpha\big(r(s, a)+\gamma \max_{a}Q(s', a')\big)\]
                    </div>
                    




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>DQN</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        위에서 Q-learning을 살펴보았습니다.
                        그럼 이제 DQN은 무엇이고 Q-learning과 어떤 부분이 다른지 확인해보겠습니다.

                        <br><br>먼저 아래의 Tic-Tac-Toe의 게임같은 경우 O/X가 놓아진 상태에 따라 엄청나게 많은 state가 존재하며, 최대 9개까지의 action이 존재합니다.
                        이렇게 간단한 게임조차도 강화학습을 위해서는 많은 것을 고려해야하고, 이를 Q-table로 만드는 것도 간단한 일이 아닐 것 입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HxqN1Lt6hwf3OgbHrAOSZyK37yZjL6HSNUpR_G0xKTidngs3aPwF7NFMkLd973cEtr-NqZpC-FJPGyvnuzjF5tyTJrNgoh1v2-U3zA21Wl-LzruCYlQKsGWTNnl2fhHA6ikR0v-mmDkG3kcT5nUwJ6YljnMUm18QunlYX7zsLnwfxvPRZyl87eCIYJaJbDvHyIe7rrUNv-_6gA41EfWtx4wSA22muvu04lyvUHx8uT_P2K-bTF3xnXpqKicIFbYJCkU1EE4RuZqhIcRZXRw5dy9zIBv5gQ8PgFwzkSekmmuWlV_mkF1HdxqVjF4X1dhZiGeoD9qtLsLlARFvnlhp5FvGAaylJrLchNvCLi468GJNbUnv8yAWWNiI-rFFmNFiqMOxkX7ADKwfw9xoyfGmJP9HC5L-fga7U2odf9pDs13WllQ3R6ZXbQ3nSXsk_2je-KJg_zq0By76SxT14yJ5xj9IR05iVVqkuhRYRJb0dyW5bxVX5_ZtYdDECtTFyupejwupBpy3B5TtWknbcaRkjQARJ89baVJ7t1sh_pxYZQGwOuiwl-NQTDuHsDWGVTtwFW6wkfno7l1Z-i9jIWRk0x61FrOhF1lq9g2-FOqk-X6V5Y5luWMcuqt9XIsk1psQPE4ej7PW5BO1K6vPVZQH3gkFIYamTf_qL_drUp06dJs8g9BRmYly9iUh0_1DiLCTgKaRxgLbonPtHqF1XiHsMA0wZ8kFrh4cemJyPtQ1aQ8SE5DOs-GBBenzvIYMbgEcH5TP5CgT1CSy386AGgxmumjl8jo4r4BmMW1yYfO4iPSe8JhAAPObjPDCCu0A9SnkVoPb3ddMKn-QhNFe0a-CTuwzIB4V92WONt0ekDB7BstBzLTH7aeFT09ilXjTR1Pe2_J1FxQLBkWzUpSCiRTNTdZZGVsm3ufwJooQ5XFqVeQg2Kwij1c3qcWOBt26Yd11NsYtXPNAvLo3QSbuTGK6VN7kZqFpa5VSpgYSgWm1LS1a0VOL9JBon0F3FEIydQGCs9XfzZOiwdGD5Vd6UKCZ8l_GCq6GW8eRXOqDFpu4-LiEOcesimqWFS3zvFUaljckn-1LFs2UrDhN8EyQk8xUQyE4ejRWHTS7ToKkoo3mg6P35Ayh-ESXh3SMFkv_7lhv2wJj46sVns1cKsojjMvIkblqTte13ah7Hz3TUPr3qDYHPDxIAY8RGpkSVGA69d3v-IIRDAjjuP4Li2Xq2rxtl7lqlUSEu6I7HGA6Ur_zYGl1SFpWC34wmd85-7yecCcFjC0mY8VF211W_EoBiEEVGeU60GT_FIR5ncpHwwT2igLGUjbl73XhZdQ2yAgrEWta9rc3-uq2I2K2guESgksulMjW8qVIzvOCojrN-nwvtkapdZ9wEzhDJQLQIBlrKP-thiHgmwVENsddELtSWkQB5ZlHJkdxlVGnxxOMe1LKNn_bOzWplZXIZ4pm0bZ68zVAmWacmBg_KlH6-VFdDQfHsyzAQzhFcYG5NqbkfRFxkUtbvwi08nulqW3J36dqlQFm3L_5wS5ewU" style="width: 60%;">
                        <p class="caption">Tic-Tac-Toe</p>
                    </div>
                    <p>
                        <br>이렇게 간단한 게임도 고려해야할 것이 많은데 복잡한 게임은 오죽할까요.
                        <span class="highlight" style="color: rgb(0, 3, 206);">따라서 DQN의 아이디어는 Q-table을 딥러닝 모델로 대체하자는 것입니다.
                        즉 주어진 state에 대해 Q-value를 근사할 수 있도록 하는 딥러닝 모델로 Q-table을 대체하는 것이죠.</span>

                        <br><br>그렇다면 모델 학습을 위한 loss는 어떻게 계산할까요?
                        다시 위에서 봤던 Bellman equation을 가져와보겠습니다. 대신 이제 \(Q(s,a)\)가 \(\theta\)로 parameter로 구성되어있기 때문에 \(Q_{\theta}(s,a)\)로 나타내겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">만약 Q-value를 한치의 오차도 없이 정확하게 내어주는 딥러닝 모델이 있다면 위에서 봤던 Bellman equation이 완벽하게 성립할 것입니다.</span>
                    </p>
                    <div class="equation">
                        \[Q_{\theta}(s, a)=r(s, a)+\gamma \max_{a}Q_{\theta}(s', a')\]
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">다시 말해 위 식에서 모델이 예측한 현재 state에서 기대하는 Q-value 값인 \(Q_{\theta}(s,a)\)와 즉각적인 reward를 포함한 \(r(s,a)\)와 action을 바로 직후 state를 모델에 넣어서 예측한 Q-value인 \(\gamma Q_{\theta}(s',a')\)의 합이 같아질 것입니다.</span>
                        즉 loss는 Mean Squared Error (MSE) 식을 사용해서 아래와 같이 쓸 수 있고, 모델이 Q-value를 정확하게 내어주는 모델이라면 아래 loss는 0이 될 것입니다.
                    </p>
                    <div class="equation">
                        \[loss\,=\,\Big[\big(r(s, a)+\gamma \max_{a}Q_{\theta}(s', a')-Q_{\theta}(s, a)\big)\Big]^2\]
                        \[target\,=\,r(s, a)+\gamma \max_{a}Q_{\theta}(s', a')\]
                        \[predict\,=\,Q_{\theta}(s, a)\]
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">근데 loss 식에서 보면 target value도 어쨌든 \(\theta\)로 이루어진 딥러닝 모델에서 얻어지는 값입니다.
                        보통 일반적인 딥러닝 모델의 target은 이미지, 문장처럼 고정된 값인데, DQN에서의 target은 Q-model이 학습되어 파라미터가 바뀌면 같은 경우에 대해서 모델이 업데이트 될 때마다 다른 target 값이 나오게 될 것입니다.</span>
                        이는 학습의 불안정성을 야기하게 되는 것이지요.
                        서술한 target이 변하는 문제점 외에도 기존 DQN(2013년 DQN을 포함, 다른 딥러닝을 이용한 Q-learning 모델들)은 불안정한 부분이 몇가지 있었는데, 문제점을 알아보고 어떻게 해결했는지 살펴보겠습니다.

                        <br><br><br><span style="font-size: 20px;"><b>기존 DQN의 문제점</b></span>
                        <br>논문에서는 딥러닝을 이용하여 Q-learning을 시도한 많은 모델들이 수렴하지 않거나 불안정한 모습을 보였다고 합니다.
                        저자는 이러한 원인을 크게 3가지를 꼽습니다.
                        <ol>
                            <li><b>Data Correlation</b>: 일반적인 딥러닝은 각 데이터별로 서로 영향을 주지 않고 independent 합니다(e.g. 서로 다른 두 사진은 서로 영향을 주지 않음). 
                                하지만 강화학습은 이전 데이터가 다음 action, state 등에 영향을 줍니다.
                                다른 말로 data간의 dependency가 존재하는 것이지요.
                                <span class="highlight" style="color: rgb(0, 3, 206);">즉 Q 모델을 학습할 때 서로 상관 관계에 있는 데이터들로만 그 순간 학습하면 잘못된 방향으로 학습될 수 있다는 문제점이 있습니다.</span>
                                
                                <br><br>아래 그림을 보면 특정 구간에서 얻은 state, action만 가지고 Q-model을 근사하려한다면, 그 구간에만 overfitting이 되는 것을 확인할 수 있습니다.
                                <span class="highlight" style="color: rgb(0, 3, 206);">따라서 여러 구간의, 즉 상관 관계가 없는 여러 state, action을 뽑아서 Q-model을 예측한다면 올바른 Q-model을 만들 수 있다는 것입니다.</span>
                            </li>
                        </ol>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Gr5igziH-8KeLBLjNUBr_wsfNEpS3I4Rr-ras9-VfMc-gPfZ53y_7TbAsc4q7Q_6qKcnq6yDsy3rl0gqL11con0bWqPx9H7b0nMtdZxakfWqJf2onvjBdIMAy00ISrhNB5sV27h_veSQBTJV2tXzoXazewkcIk3ZspSr8uHedPt64asfhDydRIlbNGc0G2UHv72zldVC5jxbKGtGw2Sd2xrmVKi0CVyIy0XvAaJc3qFTirYq2GGQcKuUEJB8i119lvPxL60G2DxtZO7N4LVWPNkcWkc8Nltii8IRoKyTnZttIrUk8HpEYS3ir4byc5pxpzxkPbkDbvwfsPrAkMVvWQrfTzHX6lGaqyHPvayq63T33EA6-DUk_-WVy4zIKgSskOFTVtm4jwhdFJmPwIPw3ZMbqYA1xDG52ZcSbk1J6PLtnjwICWyIgdmskW4qKN_VxLbFv1nfyNfCx0T-wvpg6_kJ0jtFntYSnKAi8i9EtO34wkKMm88QaSOV_MU4S3FiMLRPCZcaSKslRwEZ2kXkpzSjwFIQggw7xt7dxqd6e56WTSOTxwJReC5F_ggeak7cf8uy1iLwxe786B4XAkclzg8bUbv1SPhQDkyAOuEAi0NaZyIl-6Z2SM6swb6KgK378vtZUEIX3WGgb4HMPIQlskQcvHem2l9vyQjh_0x0PVAzuZt93GXb5Oif7DaiCmMUjCeTeaQWCHtarvMN3CX4m1q8vO_ZHv5A6e7XZqaEdiYbNvSuQNHhQwZnw2735hX9eFjE9AADnelqT5-AfXH1DIQAERa0Rz9OXJVYux72peSi-e7ZH75wJ94RsWk-hpx5I6lIQPQrEejnZe2N-UL9p3VWP2ZWD5ad3M-N5jHc4GbkOihiDj2k8ZE70NuBLYdBSjNciY1dmE16bdCf_CHCwqH6HFOJLgYiBJ50ZrakeGSls-ZQofqxzc3VPFMFfOUpiTCsgEa_vvCiUCba2bT5BrsFNUb8ELuVfmpMk2JHNwVltLG7SrUyKb6QkY25oZGFzuj73165wjUIdLMwpKoJOSHEi3wTqfpRcuJKPxlbOo0wd8iFqCu-A9JLVZO8FQQ4ZwF9QBkHqRWrNcEcZJ8n4xpHY8ZHnyyyrLbhWZJPGArWdydMKmUsQBA9Zmnky2QWCe15iuvNrNqUYjM8zPaIHgAwwzalMWlBOL-TunUWxP-v4zM4e6vXQolTJJ4PyCpb9TSIoZcSQaNQg5cWqwJ5J1A-AkRYBZoeQFgDE5DSxa4b_YFhyq7U1hTKFXnEXE9bELxCW4M5cxhi1nsvc7OAcp-4GcrGcw2dCLEs64AntRAln9qtKrSa5Vu2GGE1J27odMpzSZmscp4sMsbPxufnqLKB4Jx__8258fgUY8nZdThXhu0ocIHNt0nAtSaU-YUdLURHRriTarnVwCxrT0itC9aZnp4s8nNdNcc1ibmiGy1pOpL-Zy6YaINN4gy6KbEhVMb2JKO4nsYwezffEwFmHfEPyHFEr5XNP2DQuYzHeAberWOzQ5oaLnAm4fIllEsOXs15JOgg" style="width: 100%;">
                        <p class="caption">Data Correlation 예시</p>
                    </div>
                    <p>
                        <br>
                        <ol start="2">
                        <li><b>Data Distribution</b>: 강화학습의 데이터는 Q-model에서 추출됩니다.
                            그렇다면 parameter가 업데이트된 Q-model에서 추출된 데이터의 분포는 분명 업데이트 되기 전에 추출된 데이터의 분포와 다를 것이니다.
                            <span class="highlight" style="color: rgb(0, 3, 206);">즉 이렇게 모델이 업데이트 되면서 추출되는 데이터의 분포가 달라지니 학습이 불안정해진다는 문제점이 있습니다.</span><br><br>
                        </li>
                        <li><b>변하는 Target</b>: 이 부분은 위의 loss를 이야기할 때 언급한 부분입니다.
                            <span class="highlight" style="color: rgb(0, 3, 206);">Target 또한 Q-model에서 얻으므로, 같은 값을 넣었을 때 target 값은 같아야함에도 불구하고 Q-model이 업데이트 될 때마다 추출되는 target 값이 변하는 문제가 있었습니다.</span>
                        </li>
                    </ol>

                        그럼 이제 어떻게 위의 문제점들을 해결했는지 살펴보겠습니다.

                        <br><br><br><span style="font-size: 20px;"><b>1. Experience Replay (Replay memory)</b></span>
                        <br>먼저 replay memory를 통해 위에서 언급한 1, 2번의 문제를 해결했습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">학습이 진행되면서 얻은 모든 state, action을 replay memory에 저장합니다.
                        그리고 학습할 때 랜덤으로 batch 크기만큼 추출하여 학습을 진행하는 것이지요.
                        즉 현재 state에서 얻은 action과 여러 값들을 바로 학습에 사용하지 않고 저장을 해두는 것입니다. 그런 후 학습에 사용하는 것이지요.</span>
                        이러한 방식을 채택함으로써 서로 상관 관계가 없는 데이터를 학습에 사용하게 되고 학습 불안정성이 해소 되는 것입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_GDU-1IGzRPOOxsRyg4qHwldlKGFdL7kvCuyDFvEuKglsOIA4eG1RXYz93cM2FRdfDrzWTA_UISJV1dGPK10l5-kEqnBRiYtNDpJvZFk7lrjF3DMC6T3gBBkJXI6t1W3o6HwyWHoyhYPG_Yx15yBFPoWfagthBLPRnWBDcKNnuNGzztTA7-uze-KFw3LUvRdX99YB2Jnr1WGA43eFPvRJwYx0i9dumLBdN7abJvNvw422Aj9i5kIeVwaocLexxm1xNfkTspTFfNgPV4bN4h-Kv_zgEZGzmycYBqFal5FZDhQQrQZ69aFYgNzvj8hirJOzpJU_c7Lslpsmdj_TffOF_n0E1d8sm1Pmtp9mTk3mCG4fmX4uM3qAWWMK_ZQB5qZll7D4AvKdwuZPyWG7aunFpGRNO1liVNRP0umHyCKpTSmDQrYwqGBhwdYleXHVpSoxVt1_OuXByab8op-m-j8KzQ-uMaVOMCHfPzsaO1GY1y3jBX-yRW73eq_HGgfgK2dZysE_eEZGG-PV3R-R1zw58WkfSdnpEkFwAfXUbIkUCzyQtdQO-pJKMd235TsvOzHXFCOSrpvzyQ1-kMYfB8tEgnI4rt3YyuJJpcnYB5vKIaBr5ZSV0jJEgEsgMieuI1ZAVdI9bLFdDdoWutyvI1wffTcRg2c7XHAq-ZXOgYKnDttZuV8NAN10_iyFMsCE5_mtfKkGZ22tq2NxtrFVxdKiwxKhnr0hWqDqgRPNcZsap_hhVKX-gzm8NZUrxe4-_OOG5hEIm00ClY6ssNDTAMAJ3igTaUCznQazTj30KOpI3K2ddsJcXoaxqhoLBMDFziYfGgGU6l9ziNCUZm8YY5SdguF9ubcP9yvJac5jcnU1OiUNFN_9IBAmNsu7jighvbcdZsotoYGrQkmHhBReFM6afigq6dzIPM3GAve0kJntpMZ6Gd4hXUCQkayFJmNhlbxdOmTvYFlqmpz-zMHVeYBYlDtkzRsj8PZhYmo0tC0cAqGcBJLIjxyJm7UUVejFxTmeiKRzYq5Gl3dlEI1wqZ0Vi0L6mWYzDVdNvxcEhWK9KywLzVwP-ko4G48rLiwFCEeblJ3p1_b9FxVXt5nedIzSb-V0bluxLWPSz5iQhFl3-GoNySekRYHRl77zjBaXSOUHwiQpXAVbtM73NlUI3aiGOm0fCNmPwOhqjHQWeVJMPrF4NTHojD9-pNS2u5z5scGCBegt19gb0YSoS_2kblMtzNzxv0hCAbNk2N5msjuRhUvGsrAtSFd0lL_hWpYcYVjc9elH0gRHF8MUHBB9oWh5ww2YLCg0Z_f6DrowDYA74UbcM1Hltn1ZXMXqFHQ0j-xb7XwWnm9XEQeBd_T-YYCxDQh23RBW6iY9PNFENbwsadcyydn6LVwL-N_IsMWh4MrruRBU57cwje18ubpwDZi7Qy7s8CJR6t9DfuqS4YDB7XXRcpX8SP5LDIULamUuM-rxf8tV60DRfk_WNzSJT0TYenILdMpMF1T8CleEQuyxpYRvEkTLxX6fn6fr1F7Kv9icQ2ptwrfIkB" style="width: 100%;">
                        <p class="caption">Replay Memory</p>
                    </div>
                    <p>
                        <br>이렇게 replay memory를 사용함으로써 얻는 장점을 정리해보면 아래와 같습니다.
                        <ul>
                            <li>랜덤 추출을 통해 각각 다른 시점에서 얻은 데이터로 학습을 진행하므로 <b>data correlation 문제 해결</b></li>
                            <li>여러 action과 state를 고려할 수 있어서 <b>data distribution 문제 해결</b></li>
                            <li><b>학습의 안정성 증가</b></li>
                            <li>이전에 얻은 데이터를 여러번 재사용 가능하여 <b>데이터 효율성 증가</b></li>
                        </ul>

                        <br><br><br><span style="font-size: 20px;"><b>2. Target Network 사용</b></span>
                        <br>위에서 언급한 data correlation, data distribution 문제는 replay memory를 사용하여 해결했습니다.
                        그렇다면 마지막 문제인 움직이는 target value는 어떻게 해결할까요?
                        일반적인 딥러닝 모델의 target은 이미지, 자연어, 특정 label 등 모두 고정값이지만 아래 식처럼 DQN의 target은 Q-model에서 나왔기 때문에 고정값이 아닙니다(Q-model이 계속 업데이트 되기 때문).
                    </p>
                    <div class="equation">
                        \[loss\,=\,\Big[\big(r(s, a)+\gamma \max_{a}Q_{\theta}(s', a')-Q_{\theta}(s, a)\big)\Big]^2\]
                        \[target\,=\,r(s, a)+\gamma \max_{a}Q_{\theta}(s', a')\]
                        \[predict\,=\,Q_{\theta}(s, a)\]
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">이러한 문제를 해결하기 위해 저자는 Q-model의 parameter를 똑같이 clone (복사)하여 target network를 만듭니다.
                        대신 Q-model와 다르게 gradient를 끊어서 target network의 파라미터는 업데이트 되지 않고 고정상태로 유지하는 부분이 다릅니다.
                        그리고 완전히 target network를 처음부터 고정하는 것이 아니라 target network도 최신으로 유지하기 위해 10 episode마다 clone 하는 등, 주기적으로 업데이트된 Q-model로 clone을 합니다.</span>
                        아래는 DQN의 전체적인 도식도입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HgJsC5YdzS53KpJ1wJki6wSfNyuEO8upv47G9OnyF7lnsWnvRsd4nN95wxsYRiy6t1eztvvfCX6KnYiRPIGliCbfWyjrzUXocmqm7NOi3qbvhPE9WLBoK18JjZi1OGJXSY_SpIBFQdYFQI-S_im2nPk69OXm6HWdJHgJEaKusSkmwQNljswyuY8q57ykQ2qrnKpEehoC8t7SezlWGwqu51M7r1TdKEtHQBRvMTnU96dIwxmstI0QZQAFjJcYwg4ElwCRhn5j4IAv1cHxZfSWduuyGiWGNCHwkVoWnbb8o2LETgx_v-OpunZ4vKBCGJN4HA2PsWneKxfPErlfdBLZhSeaII9-CBbQvl7g3hxs816p_0yQkUasgYSCKBFvFKnltdblj2MxB-kTjJRAF347TqcZZGg0W47Q6ZYf__TuQR-GQeb9h99CH1iv7ED6qsXPoyJVVsP8mwKD_3jCcTW-YsxybtTgtury-GXOwymj1STvCUl_Qk8n_Z-PP8rgpU1YYuCKjLahN3ZUTj43wTnnt8nQkk_zdT2iAKKXdykihqYgpJ_HUC8Sh713-R5XRevzFeY-W6Rp-bk-Bc6sGY4f_iOW-yOFPUfX7ReW6uPl7V5SOuLrG6NCofCCQmJTm1Nfk2tS6sW3ROVKymuPU3_bq86aDNi6Eg1zjF3YtMGgJXLUuIuTfjuTg_iRWPKue5fNkHkIgNjgRgVjjNgXVhbG0NnxBPNWEUZlVh2WhdNp5hr9Rm4B1ERYuKGztM8RJn_MDHqUsO89d_V-2Nk_yF5Xzi1ocT6xZweVBxZ-efvJxM2De-YywDnPWjonFp73m6eqd2mbV7oNTlH6Vrs8uSm0RJmjtFtaBQR1o76B4mxsS63XIb2Q9GtrdACyqxSGSDCQeyWiSP05Kicju7DqaupgiqhjitgFNh9Rh1uaDsa1jt1-KCszF-xKdRGsSalFH2ZMzDK9h0la90k7dkqgnomjKAH_-WP4axP6wHIUiT007UT-6YbUl5sazUdH477zkCddHe33FNqsDBbLQNPnyOA7GKmBmTkYziV6Drr3ybB3ab3iP2QTTbndGrukWQNcNx9-Kz3roFhnSFwojVw2DGrsHkwmP77FcXd5-3oQa9rYVBxn2gyYKqV4CnWUAE3aGTcVHfybcEUf_WJV8NkhX2bdQN76dn2_qid-AMx7W2kI1X29HRcnksKVe4UdGpzlrzJUCS7NT2YYUJ-6IXqZSyP-PzCIli1K0p4C6ZZt6nHAyms7-uvLGCQ9CjU9z4oZQTZ4t-6rGuNCUXwwx3E49QMP9iu2a_urfjwRY2CDoyZpqnqDTCTZtwkIvu7Wa_x1NHl17jA4jLcwCdPixaFnLhvbuhUknEl0B4P1G30Jsgvc6MeB4eRRWvjuB9w_ewgPCYxv0Yzt0i8wV2QpmFgrg1fh_wAAeYqW38fxsfJXEb84VGtZWcxKLiT_iM0kgDSs_Xd2MUkS5__zOqmuX0Dyr8zvwt4ft74h4v8zQpSuTXNP64RC-C_2J8rGtNpYluhq7srYSWw61lKUoA" style="width: 100%;">
                        <p class="caption">DQN 도식도</p>
                    </div>
                    <p>
                        <br>위의 DQN 학습 도식도를 보면 순서도 같이 나와있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그중 7번의 clone 부분은 매 iteration마다 하는 것이 아니라 특정 주기마다(e.g. 10 episode 마다) 업데이트 하는 것을 참고하시기 바랍니다.</span>

                        <br><br><br><span style="font-size: 20px;"><b>3. CNN Architecutre</b></span>
                        <br>그리고 저자들은 CNN 구조의 모델을 Q-model로 사용했습니다.
                        Q-model에게 화면의 정보를 사람과 비슷하게 바라보도록 CNN 구조를 선택한 것입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 게임 화면(현재 state \(s\))를 CNN Q-model에 넣어주고 action을 내어주게 하는 것입니다.
                        예를 들어 action의 종류가 위, 아래, 좌, 우 총 4가지의 선택지가 있다고 가정하면, 우리 일반적인 label classification 모델처럼 4가지 action에 대한 확률(Q-value)을 내어주는 것입니다.</span>
                        그 후 agent는 가장 높은 확률(Q-value)의 action을 취하는 것이죠. <span class="highlight" style="color: rgb(0, 3, 206);">이 과정이 바로 위의 도식도의 1, 2번 과정입니다.</span>
                        <ol>
                            <li><b>도식도의 1번 과정</b>: 현재 게임 화면(current \(s\))을 CNN Q-model에 보내줌.</li>
                            <li>Q-model은 action의 종류마다 Q-value를 내어줌(일반적인 분류 모델과 동일).</li>
                            <li><b>도식도의 2번 과정</b>: Q-model의 결과에 \(argmax\)를 적용하여 가장 Q-value가 높은 action을 선택.</li>
                        </ol>
                    </p>


                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>DQN의 결과</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제 위에서 언급한 기법들의 유무에 따른 DQN의 성능 결과를 살펴보겠습니다.
                        결과를 보았을 때 target network와 replay memory를 사용했을 때 더 좋은 성과(더 긴 duration)를 얻는 것을 볼 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HkCmkxSJbfs0pXvqL79G7M7kkaxcd6LFwxJ-ZEEQRXe73N3NnpxelOObDLZRy19GvfRCeGHz1V9B3Bef7xCuaRxgVdUOJsG8rCpM0KwJN6g9LonNeOW8Mf6cWov0jC5Vs542esChb3WCTnSboHaNXs-ec8F8KW0rZikNmfUKClSLA-lPzFGNhIR0J2OxtuuoPPTQ0WTuIDoXTugJZMRuP1605PuNRPrM8em-N5DXkJN4K8F5y-WUONGwTl5Mm_GOvQ5W0gSPs47vMjbyZ2-TQETf85VgLk1av6rixk5gNuniaIQyzm419Jt3l3Arc4HsD2n3duqDgoRmAxem_JP9f-WyYrt6wnPBgXCyiTvHuRZ4uZCf2ga1AwrfBDthn34cwdLQC5JC6wQP2tJiixqO_r9wHvoJIMw4E5J0SWgUUBr_3M_h5jAClXTyEZMzmN8LHJNLc3vTqN7G2LW7B1vDVw9v1ZxL2ucuT4GHCg2iAMipvOzH2XLW242ATUIbobl6vQwWcXTuSaxpE2NtC7b8bDnKRn3JgoxRJv3EiTrf7nQ5V69o31y-Jp2uEYjf8uKFq7hEykIrKayE27w1cdA7SU6qb3K8oXM6RAUid-10p6GYrEDtDhngsN23Su5lsc1EZsJEICSTtm91TPL3wt-LQO_IDOP4If_-G0krFM23LBIu-eNtdn5DlwfSMLmNk-tf-d9UGgdzLXldpb0h3kxbTw2pF8SfffrqJ9ZW_ohz-8nwR7iudQGkszIjwiLqZNEDU2WaNxzsyP_02Uy2Oz586B0G0sSL4jokevIhFY1pZv9JlKPReHjpbVSqKgBoeDuUSfzW3YTDXPJBasIUKVixQMGuLabfezbqvBznKOIJ4Rl7eNAqiYEc--uVa_A0w2YIGUQYdeSqpkTMx6fKD59GAFDaKtGPySYlDbgtj0lEvoQk16zh_VuImclhLqJJoy1rKXPIm-uAJ9IvWrQZI0ZpDSGFQpZZT9CNrNUOWrqUwTZUny2DSmN2zmnTQOD-qVthdZp7S_U_1-_iU-xSdk-873cE_da8X3ZbVAjtadnAMB5tVhOcUWfkpwW2XparcdP0d_VLW0z9CbVFIVDIt9j8eaLeZsjngG2KWU85mTKVDoXhGhxSzGz6Cw-dZc4MfJLh_mqOgb30k0Xmj99PZtEMQr29FK0cJtvf431sM_zQlJfBv_8DlH7A8YZpLCIvQ2kx6aAqfuiqpsc2p7KqGKDY9Mi4gFtAYgTXH0kO-R7HhB3TAOtEIEX37fvxdKb4Nmpi4DAXfhramA4SB7ECtcZ31lEWGR7DjcnPuyq7rywCohfPzurZNiPsS8ajtNoxfXAGvHPg2rsvZP0H6ucE8DBDqcwsCFCF-vzPCBJQZed1zzp2uuPlVBykQFnZZBxs4io7WgusNdXgE_rahPR63sxoqAHb4Gs6hZlXZmybeGc4x3wk-Pqo-t7mDX8axIVnGan1w1yvhJ8W_IjM35VMNzhew3UIfMRbe_uUYUGcEyvkuP6cs6TN_WolY2MDnNrje7zIf8HUJbn3KF" style="width: 100%;">
                        <p class="caption">DQN의 성능 결과, 출처: 2015 DQN 논문</p>
                    </div>




                   

                    <p>
                        <br><br><br>지금까지 DQN에 대해 설명하였습니다.
                        다음에는 DQN을 구현하여 간단한 cart pole 세우는 모델을 구현해보겠습니다.
                    </p>

                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#DQN&emsp;#Q-learning
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('DQN 첫 게시물 입니다.\n\nThis is the first post of DQN.');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('dqn2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>DQN을 이용한 Cart Pole 세우기</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>