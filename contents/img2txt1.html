<!DOCTYPE html>
<html>
    <head>
        <title>Image Captioning (Show, Attend and Tell)</title>
        <meta name="description" content="Image captioning 모델 중 하나인 show, attend and tell 논문에서 구현한 모델을 설명합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/img2txt1.html" />
        <meta property="og:title" content="Image Captioning (Show, Attend and Tell)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="Image captioning 모델 중 하나인 show, attend and tell 논문에서 구현한 모델을 설명합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/ALs6j_HYvQZFLezym9CINcoh30lUoblxZ7m_eN8ad70cllu-ZLJ7vPV5YeOEfn2Xs99Wf5kr7x9n-mkGy771S_CQNeVTd7_JeQ9bFyha5sVlVxbkY62Rb8K5ny7vf3M0zm8wM9FeesqhjIBFGJrIgGNSpabQFWgf0yOQjjHXKVU8L5XLGVks4AJc2DAE9M2BSjJoPAJ52mT5wulT6YX6ybZS6ylE778_aJcNvEKckUh0e_5HFuOzPxilbgTOqIOuQhXIFY1FW-5c2-prYJhLoa39G4pnC58JqHeytdV-HYS3j3ykANCIBNHpcbRxP0xMGrWzT3eA5kACkh9965zM8Tm3dJuS39fy8cYznKHtHLVTXfZ2cCgWRKyD1zOwOuM6eSir0M7lP1cbky2iTTmq3I2rkln1RSgsM66E5-kElYyieneAbJqDAVHuO9tkzxzQInujCa63iZhHoPUmqiI1ofPWitXTtPfI5XTWQzV0_T5icnLznrNiyo8n5HMpirlqmF4NRHJ4OxFeU2kTaNtKB3wbSQKqmaPi17bMNxR2fqtVXyyvNzdA19rj7Fi-KmgKocEGXHpIv03vxX3cWeKP0Ky6gN7-mkpzQSpFOj0NiPSTcRnxQTUxzRHPQV_W3v-hKw-EVFR5nNMvSzcmmRricMgUY_CzFqijvicJy1J5rl2I873EK8WhNYF08_h3oFOA-loCdVhrt8n0auQkdDyRLean0ufsAjE7LboKeWH4qegWROyNUlXfbMantnZwr_CjBXgQtAYses0BNxJf9LQ6k3AT5kT2f3WWVF1lppiZTncilvGREsViaqYdXrkIA2mvId6Wjw48nx5R52-pBHKchsFSUeiOflYL7E9PPdhpuJFlOk9qQXRoW7sxne-ts9dMwr5yRj2lujQ-wcXi4OF7qztD-Jho7f9IXLKLx4FCmIZ2DyM7nKhR5R2xXldOxx3g27sXvzZvbYEohlduH6C10CmbpZRDhdAsLuQIV5O9HAy5wcFByFrfXF7czMlNXmYTk8qvlX89K3BacK6zjfEz6y5NHNisnd_1pd2birGaP9xuPlbEoCexE6XBOY8FGLDt2iPzyzybJ1bS5-Hee8LRaVRsI_UJwTgQ6NbP8IkXDHQhvG5nOAdJQ-tyHI48GcAZAAe0CPz3sbGYZ46sXVfM__yzgY0VZnnsJ0t-y7c986C3_Ygp3XH8xGz8LMiWCYCJHzKoIrU1hhZ74HQDrWWWNb8_h5ds4Hr1FuYSODxVyE0RfXYDYF1ZUM_R3YH-XVlq9iOYvbRHP0liFi6vaim8FMxWwhHBiUk5HllrkW-JliwoovrLiCRlzTr_7_0TwNwUiPmyJSKnr0XygCxjSV7_INJPwVaB2vlxWw8ZWn17VBljomENNKCfotkzyqf-gMq6PxFZNwZKdAAHATvchv7k0aCuWJ4moSZyPDjwKmHGkHN-46bYSLf5diKCR25Fp-9TtVTeWqtY5VinmHl71pnad40vyEgPuDFQ9M0BXodVGrSAaVFQFNUTkpUgXt8arTEuBcIT_RWiU33MuwBu" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Image Captioning / 1. Image Captioning (Show, Attend and Tell)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/ALs6j_HYvQZFLezym9CINcoh30lUoblxZ7m_eN8ad70cllu-ZLJ7vPV5YeOEfn2Xs99Wf5kr7x9n-mkGy771S_CQNeVTd7_JeQ9bFyha5sVlVxbkY62Rb8K5ny7vf3M0zm8wM9FeesqhjIBFGJrIgGNSpabQFWgf0yOQjjHXKVU8L5XLGVks4AJc2DAE9M2BSjJoPAJ52mT5wulT6YX6ybZS6ylE778_aJcNvEKckUh0e_5HFuOzPxilbgTOqIOuQhXIFY1FW-5c2-prYJhLoa39G4pnC58JqHeytdV-HYS3j3ykANCIBNHpcbRxP0xMGrWzT3eA5kACkh9965zM8Tm3dJuS39fy8cYznKHtHLVTXfZ2cCgWRKyD1zOwOuM6eSir0M7lP1cbky2iTTmq3I2rkln1RSgsM66E5-kElYyieneAbJqDAVHuO9tkzxzQInujCa63iZhHoPUmqiI1ofPWitXTtPfI5XTWQzV0_T5icnLznrNiyo8n5HMpirlqmF4NRHJ4OxFeU2kTaNtKB3wbSQKqmaPi17bMNxR2fqtVXyyvNzdA19rj7Fi-KmgKocEGXHpIv03vxX3cWeKP0Ky6gN7-mkpzQSpFOj0NiPSTcRnxQTUxzRHPQV_W3v-hKw-EVFR5nNMvSzcmmRricMgUY_CzFqijvicJy1J5rl2I873EK8WhNYF08_h3oFOA-loCdVhrt8n0auQkdDyRLean0ufsAjE7LboKeWH4qegWROyNUlXfbMantnZwr_CjBXgQtAYses0BNxJf9LQ6k3AT5kT2f3WWVF1lppiZTncilvGREsViaqYdXrkIA2mvId6Wjw48nx5R52-pBHKchsFSUeiOflYL7E9PPdhpuJFlOk9qQXRoW7sxne-ts9dMwr5yRj2lujQ-wcXi4OF7qztD-Jho7f9IXLKLx4FCmIZ2DyM7nKhR5R2xXldOxx3g27sXvzZvbYEohlduH6C10CmbpZRDhdAsLuQIV5O9HAy5wcFByFrfXF7czMlNXmYTk8qvlX89K3BacK6zjfEz6y5NHNisnd_1pd2birGaP9xuPlbEoCexE6XBOY8FGLDt2iPzyzybJ1bS5-Hee8LRaVRsI_UJwTgQ6NbP8IkXDHQhvG5nOAdJQ-tyHI48GcAZAAe0CPz3sbGYZ46sXVfM__yzgY0VZnnsJ0t-y7c986C3_Ygp3XH8xGz8LMiWCYCJHzKoIrU1hhZ74HQDrWWWNb8_h5ds4Hr1FuYSODxVyE0RfXYDYF1ZUM_R3YH-XVlq9iOYvbRHP0liFi6vaim8FMxWwhHBiUk5HllrkW-JliwoovrLiCRlzTr_7_0TwNwUiPmyJSKnr0XygCxjSV7_INJPwVaB2vlxWw8ZWn17VBljomENNKCfotkzyqf-gMq6PxFZNwZKdAAHATvchv7k0aCuWJ4moSZyPDjwKmHGkHN-46bYSLf5diKCR25Fp-9TtVTeWqtY5VinmHl71pnad40vyEgPuDFQ9M0BXodVGrSAaVFQFNUTkpUgXt8arTEuBcIT_RWiU33MuwBu);">
                    <div>
                        <span class="mainTitle">Image Captioning (Show, Attend and Tell)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2022.09.17</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번 글에서는 multimodal learning의 일종인 image captioning 모델에 대해 설명하겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">먼저 multimodal learning은 다른 타입의 데이터를 가지고 학습을 하는 것을 의미합니다.
                        예를 들어 image captioning같은 경우는 이미지와 텍스트, Speech-to-text (STT) 모델같은 경우는 음성, 텍스트를 가지고 학습을 진행하며, 이렇게 다양한 타입의 데이터를 바탕으로 학습을 진행하는 것을 multimodal learning이라고 합니다.</span>
                        즉 modality는 데이터의 타입이라고 생각할 수 있습니다.

                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">본 글에서는 image captioning을 최초로 시도한 논문인 "Show and Tell: A Neural Image Caption Generator"을 간단히 살펴보고, attention 기법을 도입한 "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention" 논문도 살펴보겠습니다.</span>
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/1411.4555.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Show and Tell: A Neural Image Caption Generator 논문</a>
                    </div><br>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/1502.03044.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention 논문</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>Show and Tell</li>
                            <ul><li>모델 구조</li></ul>
                            <li>Show, Attend and Tell</li>
                            <ul>
                                <li>모델 구조</li>
                                <li>Soft Attention</li>
                                <li>Hard Attention</li>
                            </ul>
                        </ol>
                    </p>



                    <h1 class="subHead">Show and Tell</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>모델 구조</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        먼저 show and tell 논문입니다. Image captioning을 위한 딥러닝 모델은 2014년에 이 논문에서 최초로 제안되었습니다.
                        이미지를 텍스트로 변환하는 원리를 대략 떠올려본다면 encoder를 통해 이미지를 representation한 후, 이 결과를 decoder로 넣어 문장을 생성하는 방식이 떠오를 것입니다.
                        너무 당연하게도 위 논문도 encoder-decoder로 구성된 모델이며, encoder는 CNN 기반, decoder는 LSTM 모델로 제작됩니다.

                        <br><br>아래 그림은 실제 이 논문에서 설명하고 있는 모델 구조입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">Pre-trained CNN 모델을 불러와서 encoder로 사용하고, LSTM decoder를 학습하는 구조입니다.</span>
                        이때 encoder의 feature는 LSTM의 첫 번째 input으로 들어가게 됩니다.
                        그리고 우리가 잘 아는 autoregressive 방식으로 decoder를 학습하는 것입니다.
                        이 모델은 당시에 image captioning 모델 중 state of the art (SOTA)를 달성하게 됩니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Fqf-jQq5TzpskxtWSsLmb1HUeKtT5DHUY7c0GV2Aa5_K1xTgTAiWBfp6DMP60q5oUGlRlNAgcCbtPS4QCdM4himrTcdLHhQfedkUdF-81sgksbFlOVlXaSZPu0-QfF0DarIt2n9YKbl6DxAN5y_ItmVT4DhZ8d6nYNHg-Q_ZFAuJ5N4umN9nmknCiPM77UTwFNT4E0kViLD2iW3Vo_r-V26eOaONmIZPGq7tqqn8fyda5xVehR9XqnimXbX6Lmh2p6sNBzZk8-IHPLM_-EymFqdZjoRXV2JWwppx1F10L5wa8T0YTTjfXPUmMFAYPQaCx8ohPk6SM01Pz6AS6liY5Y7dQVzewa5gckUYyoCuDqJrxGAfKOczzxNrJv0d2x7nt9Ix7VffzNnHLNR-WpI_3xWK17n-5Mn4EVdsouon-ZuaBzvoNjI1N8Seo_FwGeqg1gZq6TuDrL93Q0syTGsfB4gzbhgmUKAx1s_-_rj1Y0o0QAjdp9d42TKP3jT7XRXwdOJ307qts9g4EL5Ae6YG6Rxixs_8wbywgkeWqjCU9Cx0optArAUsR7wlb73QFf6tMIK0bMMmEeVdsgmthRbbp6W8Jmpj-L_S5-2ta0rK2DgDeQeYdcOCvm-Aqy9jCSZp9zuxsV6RtT7edMrsgiEEb99rrNxnpRera1vZytWnGRPv8iMmBWDH9IFGKJpcap3GT6NsZ2ZNdj5__UxSKIuLLstVLtSDTp9LMgFp-Zw6DwI0_1GBeAJ4X11AjS_G4nis-8FxSUICCW8LX0PZtDvbtdZpMGO0fImYjGUuJoB9NJ89nXUHdVHJ6Dofn54YwyCD21XX5V5Rr_TFrshOsdY6tkPskYik8ra9qHK97wpNd3HKz6c6RE7PH_TA4JRcnM_pBShPlPxDUf6qfTkCUTX0zBH9YIxRw1iefOiYUOnQxy57Qw4tT_davVR7p_YFdkSlaiOU8e_6wQhzrd1P61hW2COtZjYCVQ712wMWcbkSiWCt6J9aBGPJi_PcsdDV_NscbRE4z24Ifu_Pa5Mfo6FVqDr2J_YPW9d1t0budt2UG7cHVC2ig937erGxqRnrP5aCAAugV0YfEo3TStDzb8SRcYdi99pBauqYsXoVZCMf8GG9SOJnwygt_mVlNH1yRRhhmJ49lETRlu2xmhs5bvRioLRiFxdtto2SrkmwjzNgoJnBr4hr0N4y4KPeB4dGgffIxEv1V8skfia8kLt_GRyrgXcHUlTen3B131IbrfYmKTakEXafqeP8C0D-dXRtitukI6WquqPtJ9GtLm_Np_fFQyC1xicppc_I6yJGUBBwxQ-bJyA23swfMTepcri2FMso-PsRPJoPjZeejK2EZiB2GoosS9QjTSsCvJyIrh90VC0y4k5oMoX8_EQzV8nA0b0gwaOWQcKPFhkWdgumXC_PM_6YZJREO5xo_AVEjLZOcpKibGgqenmgJ-HXtyNdq_2twW4rRPKjSCXaW2yK9TK3Gu2dAVRvGN92y03vIz4fVyGX9A8JaZdaA61pBrY77jMr9W0pq_mFn1" style="width: 100%;">
                        <p class="caption">모델 구조, 출처: Show and tell 논문</p>
                    </div>
                    <p>
                        <br>아래 그림은 좀 더 자세한 구조를 보여주는 그림입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HTpvRmNLMjqf0SN2ygnBjn54ZtG2O4yr_MFibT7WxTECKl6h0_0i_oKZ8A2WqoIEwEwdH3FymXKM_PR0thwzgETobdob0GwSkHPUFs_xSv6XEy6OCSU4XHYEiJWe2VP3S0156eVJ1o0swBjwjPO9J1bbLGX_HSI_eFREhvMN9LDM4IoeUFjoR9dHpwIW24-MhAdQkFwwsFAoOhcnfKi_YFo1N3Ug4ZIKlx8FwjzUrSZqrAGn85oHwl_3rQp2Ue_M52hU9Odu6vr7nZK5_mPTBfhG12mhktBBaTbJh1oPEiO9Cblo6nXjAZAA6kVMGt7CfsCvxvuNBt306cnr6Cd4Ko4sIXVpk-gRay2v6IjCQT9990IUexOJWXgd49BLDMEy3DuiDMRGrF8YhxZxs6ylsdugU5PQgkLsVrTi7F2CN1Y59ofJ7AOklXtw14MKxsUWO8tTCRYRrpPylqMORcu_GEkvh0kXOZYHdOxKCT8cR6_HiW0woHdVy_ofXpThGBQXQ1ApHBc2G5vrpdzVxhf7E57z22RTrmJh8V4hIhQs7W-TmoFhwAmbjnXJN4_WAezYMDrT6f7_CRDuaQ9JfrkeP-6b5Fxej3tL805CwGiBOMFbg1u9wm8uZEbrWwXcC8SkkfkhcMVlTaI7HMxu5YvBo-LK6IpqQUbyYdfR67se3nwi31jmWttkurLhmvICBjHDwjQoCglJHP82JEEClCVAlEmXjPMv1jHrUM8PDXe8F2GrBVgeMW7aC34lF5kkPfbNrDKYLqSK2eU4sb31ltt47vEfbQB7X0oDoSSzdZ9sv7vLvE8AFB1ys6rKFKsYfDXz7e9ibFLP8hiRcr6X1AaDKm4g07pIdBhC-ohxe1gpCOf9-rpLpoA1KT-FTAS2ZyyUByo50lHvqKLzypqW9jX27ka6-AbBEzhd1I6mrrY5tqUIWT-DoBR8KCHZuGNiCxPsoL_QAZMkwcvCK36m-gq00u52foaacxOYSoBC4VApCniSqdI5pFVefUnnSzV82BZaMjBe_QIDVh4kjuoOFUBwPdiRUxT4xsT39igY48m8ADK5e1jBsbIVX3o1nxisXA5EHPkNQfvrwk4mxPwdOFbWKqiiF0HizdGolFlVO7QnbV7Jew5kvuFvYKblpIbQU6OvypFtuwTMey-9ZyXCoV3ppqb2hiaLO4icFPgsrFynW2_zQ2lO5U-B2j6kUMwedU3PqoVpylMSa63QrsdNUDunPDxGiHdLSvhn-bKtGFBRXM_V2in23neWwtiRnPbYvqvq7ob8XLrZl7CvO5P-Pej0otl4DEHPfY_5U9SF-7GSkvtHMvP54CBV9SI8uiRAF9nNOOQis1eAbWcnJ6J_MUCjVPStKSqQqjw-mF3xvI9RkyJM4tLAIrFfkrSzzrdSk8SCatv_yWu3h3Rl88fxLVlXapJeV2Dk0wPv7ObSjiEtFJfPUIo5NvVlNS1Fcohcvt8Hap7HIaAc4RFMEzGj--KyFpYdDEVCQdsZjXXhOA945OvvUtdtMdjEzTPk3uc-__Xmnv5CfeVXRr" style="width: 100%;">
                        <p class="caption">모델 구조, 출처: https://github.com/reyllama/paper-reviews/issues/5</p>
                    </div>
                    


                    <h1 class="subHead">Show, Attend and Tell</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>모델 구조</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제는 show and tell 논문 이후에 attention을 추가하여 나온 show, attend and tell 모델에 대해 알아보겠습니다.
                        이 모델도 위에서 설명한 모델과 동일하게 encoder-decoder 구조이며, encoder와 decoder는 각각 CNN, LSTM 기반의 모델입니다(LSTM layer는 1개만 사용).
                        <span class="highlight" style="color: rgb(0, 3, 206);">세부적인 훈련 방식도 다르긴하지만 가장 큰 차이점은 attention을 이용한 것입니다.</span>
                        기존 text-to-text를 위한 encoder-decoder 구조의 모델에서도 attention을 사용했었습니다(<a onclick="pjaxPage('RNN2.html');"><span class="highlight" style="color: rgb(0, 3, 206);">seq-to-seq 설명 글</span></a>, <a onclick="pjaxPage('RNN4.html');"><span class="highlight" style="color: rgb(0, 3, 206);">encoder-decoder attention 구현 글</span></a>).
                        이러한 attention을 CNN 기반의 encoder를 사용한 모델에 적용한 것입니다.

                        <br><br>아래 그림은 본 논문에서 사용한 구조를 자세하게 보여줍니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림은 soft attention에 대한 모델 구조이며, 논문에서 소개한 두 가지의 attention인 soft attention과 hard attention에 대한 설명은 아래에서 하겠습니다.</span>
                        
                        <br><br>이 구조도 attention을 사용하는데, 이전 neural machine translator의 Bahdanau attention의 구조와 동일합니다(<a onclick="pjaxPage('RNN2.html');"><span class="highlight" style="color: rgb(0, 3, 206);">seq-to-seq 설명 글</span></a> 참고).
                        <span class="highlight" style="color: rgb(0, 3, 206);">다만 이전 번역 모델과 다른점은 encoder입니다. RNN 계열의 encoder를 번역 모델을 학습할 때 사용했었는데, 본 연구에서는 ImageNet으로 pre-trained 된 CNN 모델(e.g. VGGnet)을 encoder로 사용합니다.</span>
                        하지만 기존 번역 모델의 encoder output은 (batch x length x hidden)의 크기로 나왔다면, 여기서는 encoder의 output이 이미지를 모델에 태운 결과이기 때문에 (batch x 512 x 14 x 14)의 크기로 나옵니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">하지만 (batch x 512 x 14 x 14) 차원을 permutation 하면 (batch x 196 x 512)로 만들 수 있으며, 이는 기존 번역 모델 encoder의 output 크기인 (batch x length x hidden)과 일맥상 통합니다.
                        왜냐하면 196은 14 x 14의 이미지를 일렬로 펼쳐놓은 것이므로, 이는 sequence length라고 볼 수 있고, 512은 hidden dimension이라고 볼 수 있기 때문입니다.</span>
                        즉 이 모델의 encoder의 결과는 이미지에서 파생되지만, attention을 사용하는 데 전혀 무리가 없습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HYvQZFLezym9CINcoh30lUoblxZ7m_eN8ad70cllu-ZLJ7vPV5YeOEfn2Xs99Wf5kr7x9n-mkGy771S_CQNeVTd7_JeQ9bFyha5sVlVxbkY62Rb8K5ny7vf3M0zm8wM9FeesqhjIBFGJrIgGNSpabQFWgf0yOQjjHXKVU8L5XLGVks4AJc2DAE9M2BSjJoPAJ52mT5wulT6YX6ybZS6ylE778_aJcNvEKckUh0e_5HFuOzPxilbgTOqIOuQhXIFY1FW-5c2-prYJhLoa39G4pnC58JqHeytdV-HYS3j3ykANCIBNHpcbRxP0xMGrWzT3eA5kACkh9965zM8Tm3dJuS39fy8cYznKHtHLVTXfZ2cCgWRKyD1zOwOuM6eSir0M7lP1cbky2iTTmq3I2rkln1RSgsM66E5-kElYyieneAbJqDAVHuO9tkzxzQInujCa63iZhHoPUmqiI1ofPWitXTtPfI5XTWQzV0_T5icnLznrNiyo8n5HMpirlqmF4NRHJ4OxFeU2kTaNtKB3wbSQKqmaPi17bMNxR2fqtVXyyvNzdA19rj7Fi-KmgKocEGXHpIv03vxX3cWeKP0Ky6gN7-mkpzQSpFOj0NiPSTcRnxQTUxzRHPQV_W3v-hKw-EVFR5nNMvSzcmmRricMgUY_CzFqijvicJy1J5rl2I873EK8WhNYF08_h3oFOA-loCdVhrt8n0auQkdDyRLean0ufsAjE7LboKeWH4qegWROyNUlXfbMantnZwr_CjBXgQtAYses0BNxJf9LQ6k3AT5kT2f3WWVF1lppiZTncilvGREsViaqYdXrkIA2mvId6Wjw48nx5R52-pBHKchsFSUeiOflYL7E9PPdhpuJFlOk9qQXRoW7sxne-ts9dMwr5yRj2lujQ-wcXi4OF7qztD-Jho7f9IXLKLx4FCmIZ2DyM7nKhR5R2xXldOxx3g27sXvzZvbYEohlduH6C10CmbpZRDhdAsLuQIV5O9HAy5wcFByFrfXF7czMlNXmYTk8qvlX89K3BacK6zjfEz6y5NHNisnd_1pd2birGaP9xuPlbEoCexE6XBOY8FGLDt2iPzyzybJ1bS5-Hee8LRaVRsI_UJwTgQ6NbP8IkXDHQhvG5nOAdJQ-tyHI48GcAZAAe0CPz3sbGYZ46sXVfM__yzgY0VZnnsJ0t-y7c986C3_Ygp3XH8xGz8LMiWCYCJHzKoIrU1hhZ74HQDrWWWNb8_h5ds4Hr1FuYSODxVyE0RfXYDYF1ZUM_R3YH-XVlq9iOYvbRHP0liFi6vaim8FMxWwhHBiUk5HllrkW-JliwoovrLiCRlzTr_7_0TwNwUiPmyJSKnr0XygCxjSV7_INJPwVaB2vlxWw8ZWn17VBljomENNKCfotkzyqf-gMq6PxFZNwZKdAAHATvchv7k0aCuWJ4moSZyPDjwKmHGkHN-46bYSLf5diKCR25Fp-9TtVTeWqtY5VinmHl71pnad40vyEgPuDFQ9M0BXodVGrSAaVFQFNUTkpUgXt8arTEuBcIT_RWiU33MuwBu" style="width: 100%;">
                        <p class="caption">모델 구조(soft attention)</p>
                    </div>
                    <p>
                        <br>참고로 위 그림의 size 중 B는 batch size, 초반 그림의 H는 height, 이후 decoder 부분의 H는 decoder의 hidden dimension을 의미합니다.
                        <ol>
                            <li>Captioning 하고싶은 이미지를 encoder에 넣고 encoder output을 구함.</li>
                            <li>Encoder output을 두 종류의 linear layer를 이용하여 LSTM initial hidden, cell state를 만들어준 후 넣어줌.</li>
                            <li>위의 방법대로 만든 hidden state를 \(beta\) linear layer를 태워서 가중치 값을 구함.</li>
                            <li>Decoder에서 현재 예측 해야하는 step의 이전 step의 hidden state를 가져옴.</li>
                            <li>위의 encoder output은 B x 196 x 512(B x 512 x 14 x 14 변형)의 크기를 가지고 decoder hidden state는 B x 1 x H의 크기를 가지며, <span class="highlight" style="color: rgb(0, 3, 206);">두 개의 데이터가 decder hidden dimension H의 크기를 가지게끔 linear layer를 태움.</span></li>
                            <li>이렇게 나온 두 개의 데이터를 element-wise로 더한 후, hyperbolic tangent (tanh) 함수에 적용.</li>
                            <li><span class="highlight" style="color: rgb(0, 3, 206);">이렇게 나온 데이터에 encoder 데이터 길이별 현재 decoder step에서 중요하게 여기는 점수를 알기 위해서 H x 1의 linear layer를 태워 B x 196 x 1의 score 결과를 구함.</span></li>
                            <li>Score에 softmax 함수를 적용하여 길이 별 score의 합이 1이 되도록 맞춤.</li>
                            <li>이렇게 확률로써 얻어진 score를 encoder output을 그대로 가져와서 행렬곱 수행하여 context vector를 구함.</li>
                            <li>\(beta\) layer를 통해 구해놨던 가중치 값을 context vector에 곱해주어 최종 context vector를 구함.</li>
                            <li>이렇게 나온 context vector를 다음 step의 input과 concatenate하여 모델에 적용.</li>
                        </ol>

                        <br><br>이렇게 attention이 잘 학습이 된다면 decoder의 step별로 특정 단어를 예측하기 위해서 이미지의 적절한 영역에 attention을 하는 것을 확인할 수 있습니다.
                        예를 들어 captioning 할 때 baby를 예측해야 한다면 이미지의 영역 중, 애기가 있는 영역에 attention을 더 하게 되는 방식이죠.
                        
                        <br><br>아래는 실제 논문의 결과입니다. 바로 아래 이미지는 잘 된 attention의 결과를 보여주고, 더 아래 이미지는 틀린 attention의 결과를 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Htj3S5uKgxNI1EKC1Rx6vTqa412FPWXEu3bhDXYV9KVRTtaBcfH3XwU7WyeIGiNm2kcGVp7q5ppi9RTknn8mrNiwtBJC50Kstb4D6LoRHH2eV69hcnBOtFVe_aXwlfGNrPX1Lz8VcjEn_X2O1-Gx3szBRKUnCBjWmibrkz1C5Ktvsv40PEnIOFGqNoTKulDnWsep0UNZtCmx4zrsjujpLV-Kz9zOi7jb0mxNC02cajkbVrH3ZY9Gmk7YoHXVcSuoNkHiDRsQAn3CiZDidzKmceF4lsQcHHjTIxsPT5aPacUZA2OYN6ZuBFVuGkverS1Ne0RC6MrYgKR3H0smRQCmZ2h_8LSgbV8UmZnnP-f2ft0OuBc5uWcPi8n-5mBfnCB7gZBZdIAuT0RHWQMPF_ckdWptgB02iZvSyia0uNNtc1yP0o3DcTMKWfSjJT4BCmCeJQnV0FF947aJKT1Hr_33fuOZJbR-d-VNkMZecplQyYf1BTDhkkBXyu5CXPeo1NWmshGpL1KGlccPvLaOw52RLst-BjxUTdyoso9ahYB6E0_20OKow09x22HGyl1MP6zgqPtjGvS18PCOa94En_wgTPWUv7USUuT75aWj1YGBdfyLjsUwejw7B16v-ORKDSQXqMQjDBfrPSw0HjvxtWnh30jUo6AF7_CIylBw8SWaIHQoagB0bf7gGtg8u8rAAcS7Bey-yCSVZncTJB34qiCNEBwk13Bs78TCQO02Mc6yebHmkI2vwJUiBoZlb6u3uBGn3E2f-k-NhuuqYJPvePOvvGj_WxAnoTv-9oF8FkWFXd7dnpCWzckpeUwGNfsukVK8fjo6RRcG2JbOc-dyygGnNmGjlMwb_sXr5t-g_Iqwqfx2Up9t3-XYWlq9_8rnhPaQ4401lsKFEXg46ocDbakK0ii_qL3JgLM4hiN91Ja6UZFxsufDSzE16dPkYvXffJsnm2zhhW2gtPDb7UTKnjM15dTrpAPBBTuhzz_Kunz5SXvsBngiIOEAe-1K8lL3dOk9ef5C2sDFxVgFb34cnK9QgKbhosmA6hnbk2EhjWoWsDCRO1hcOxmOU6DFHxH5zDcd7jqkvaFKcjFA6gcwIH818Qda8G_MBHuiS9GEfTnxVgfwgzcFWDqOwMVRTV6EBkCdsI9As9lg-RRX-xTlkWg7FXbc2oQCNNsunoUDOGt3whmF6yj3PGIz_5t-YJeI6MiZymN3W3XZh3vQWgPwUl7wFqTiU16n3xiQqQaB5WVmtr-aVxZHshx1E4zAFHVQa03ipTQam6TpPHgsc_kPH8MesaFtESFuvoaLXQwYZo8AGr31KNFF1KnOJyEfi79VqodYJmhBis6D5OW9svGyPbfb2P_Ol_VRFansjEm05TrtzPdG_8hW6ScctlgjzLhUfAwybWNCpDfICTaz2dFAnhp56F6Rg7RH2lPSztXweK9zep2DRsp2VdWGL9zGshf9qF_GZ2LEuoYZM5T-W43iGM_Jg4PyxiMh0I7ky6tfHqxm0i9w6ZRGl8P56eew2bDIyZhM0xBCn4NEh7" style="width: 100%;">
                        <p class="caption">올바른 Attention 결과, 출처: Show, Attend and Tell 논문</p>
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">아래 예시는 틀리게 예측한 경우이긴 하지만, 이미지의 전체 모습을 대략적으로 잘 파악하고 있다는 것을 보여줍니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EelV3QAm1zEzxQSM_Z5TggEUoiEDmKcnkMCmKcWAmALY0pvfpf0zOQ0PiQz0_JE-z1i__x419dS5goO94PRtkq5lXfnDeR1X6o9-EZTpRbLfZy_HmQTt8aIntqzAdvZ5j1DV9TWvC4AmvMKc1_y34N0WDuHDFfwMOch4eVW1Li4ZkIsKNbAINKoqE_0m-ODY5Hys7-Vvxtpngz-NHIgJrtVkzIN3lJMWEaaruJo_YoYGx4e4p9k12NTxzgC6bVl9Cs9X_2TsPQTuMyNdQ6NRwHuZWBMGyCCFyOOXKDc-i2o-kvYoCG_p-jVSjujTGHkbccyO_11fEKWYX28nu0BgeJv0W1F_XkG9v9FkfwHrWSgvExG0tP8qkCF-F0sSZk0HFyI8ZXZP8qbrxA7dg69LwdM_9VOoFU43rJDNSIUnz1r_kRmWdYYv-M4Gxnjb04GZft_No7at0ZDTSXnSUn-124hk4PjCtuAtcqI15hJRK7aJTtdvJE3io58Pu_l5U0OqtGXZ57HDJgdTD7cJ0eswbg3GqYREEMsIFWMuW4zha4u4D_E2PYjruKpRNhIkmjdYVhxfMxG6s8b6F0dyGFfB0Rv4PSYNEkpKBvPom4DBYlIWIKmwJeH12mxPEA-B9EYsxFYqumRNxtCwGVKxE8hZTcjWa4gCFpkf42Cm7vaEvxVYpSjOMwwFu1oInbVxdfgzgshVJbzysnG1E12x6kE_QGgrfU1cUDL8oE87l71Sls_VjutTpJS31D_EIksYoyQAaO23wYkPLMV-ctv93m9W7d94Qxc3wNyKwV95JJYnaAKuJwI8mmbBk_EbtnZUuVeOilaUq70Ejzn8EGDKHsOCBqtyh_IGtzJUiZ5FXsB7FmqTahWjrc_8nY5bi0-1vTHYUjk3-NoHFL5r3N9l61mdwU5tAvek_8VrzocClp7wBlQsMbworZi65bH_IZXteYgd7LT-R-aezfThGKRvBdBOutnfILycaTaGQ3wfxbvr68iYQ6fAA-oypLWvVNgeYN2khsOGAMrdCv41cAHuoaRKcoQJdQrU54dJRElh-ZBTLIIB0obUr-wgB1mODUaJHStjoW_DjgNldOemGZe-d1duibtLhFY5d99PfLECRfG3pGjzOl6cPUEf_2pTQEw9WMfhP2zGkZdD55VOHbhKv-Fc1pEvJkyHYg-jH9uhVcYjNWfiN1vxtnkEhp2Ntt_15z58-iB-7RwqtCNDfPu7goTKe91qyeZYHb6418uOhY9w4EhmWAL6vv5MLUyQdvJhWS9IPglQMnsYZXQqZjgQN8lmeoTEkvZIGdWsWGtGQnIhFHNb97WOWj3eFwP9oOXXBfAUeub0WkgyVuc1R--cqYDnWH5OYHZLA7g0YWCfDQjyeacZww6gdmqvWak__MoVfiPYQgbq8ocbZT-fGwslFEMWEi90Kel1z5lkTtTqro66eESwLZwmHws1ErmseAo6oaem0NvHBAa5TWsZlFujv1rpLWmsmQEqcxYK_2fpvOG4CdXHgYDVzobnl7_uwMWBKlPh-vHxFNCqBx" style="width: 100%;">
                        <p class="caption">틀린 Attention 결과, 출처: Show, Attend and Tell 논문</p>
                    </div>



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>Soft Attention</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <br>Soft attention은 위의 결과 이미지처럼, 실제 이미지 픽셀별로 softmax를 통해 모델이 어떤 픽셀에 얼마나 집중하고 있는지 계산하는 방식입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이러한 방식은 당연히 미분이 가능하며 gradient backpropagation (기울기 역전파)를 통해 학습이 가능합니다.</span>
                        따라서 soft attention을 사용할 경우 end-to-end로 학습이 가능하며, 우리가 흔히 사용하는 backpropagation으로 학습이 가능한 것이지요.
                        이러한 경우 사용하는 loss function을 살펴보겠습니다.

                        <br><br><br><br><span style="font-size: 20px;"><b>Negative Log-likelihood Loss</b></span>
                        <br>Image captioning task는 당연히 들어오는 이미지에 대해 캡션을 다는 모델이기 때문에, decoder의 결과와 실제 target의 캡션과 loss를 계산합니다.
                        실제 이미지를 \(x\), 캡션을 \(y\)라고 했을 때, loss 식은 아래와 같이 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[-log(P(y|x))\]
                    </div>
                    <p>
                        <br>즉 위의 식은 들어온 이미지 \(x\)에 대해서 ground truth 캡션 \(y\)의 우도를 최대화하는 maximum likelihood estimation (최대 우도 추정)을 하는 것입니다.
                        논문에서는 위의 loss 말고도 regularization loss를 추가합니다.

                        <br><br><br><br><span style="font-size: 20px;"><b>Regularization Loss</b></span>
                        <br>위에서 사용한 attention은 하나의 step마다 이미지로 가서 attention을 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">예를 들어 t step에 진행된 196(14 x 14)개의 픽셀의 attention score를 \(\alpha_{t,1}, ..., \alpha_{t,196}\)이라고 가정해보겠습니다.
                        이때 196개의 score의 합은 softmax를 통해 나온 결과이므로 그 합이 1이 됩니다.</span>
                    </p>
                    <div class="equation">
                        \[\sum_{i=1}^{196}{\alpha_{t,i}}=1\]
                    </div>
                    <p>
                        <br>하지만 regularization loss는 각 픽셀별 attention score가 1이 되도록 하는 것과 비슷하게, 각각의 픽셀의 time step별 attention score 합이 1이 되도록 loss를 추가해줍니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">예를 들어 캡션 길이가 32라면, 32번의 decoding step이 있고, 각각의 step마다 attention 결과가 있을 것입니다.
                        이때 \(\alpha_{1,i} + \alpha_{2,i} + ... + \alpha_{32,i}=1\)이 되도록 규제를 한다는 것이지요.</span>
                        어떻게 보면 위의 수식과 비슷하지만 1로 만드는 방향이 다른 것이지요.
                    </p>
                    <div class="equation">
                        \[\sum_{t=1}^{32}{\alpha_{t,i}}=1\]
                    </div>
                    <p>
                        <br>하지만 여기서 의문이 듭니다. 각 step마다 전체 픽셀별 attention score는 softmax를 통해 그 합이 1인데, 합이 1인 attention score들을 step마다 모아서 하나의 픽셀을 선택하여 step별로 합을 다 했을 때 이론적으로 1이 나올 수 있을까요?
                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">아래 그림에서 빨간 화살표는 attention score를 계산했을 때 softmax를 사용하였기에 무조건 모든 attention score 합이 1이 나옵니다.
                        하지만 규제하고 있는 내용은 파란색 화살표처럼 각 픽셀별로 모든 step의 score 합이 1이 나와야한다는 내용인데, 이 가정에 따르면 모든 attention score는 196이 나와야겠지요.
                        하지만 실제 합은 32이고, 규제하고 있는 내용은 196이 나와야하는데 time step이 196인 특별한 상황 빼고는 이론적으로 불가능합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_F46kuBi5Ics31Mky7KnjVssgLgLTyQTmlLHiouCNIzZ3lOsObB8J3aVcJpv6AkqOEWRWA9HwFZxOMDflnYqvlp88YXLf0B7foDbBR1PdyekYxQ8xJLj808SN2yo9DB0EwjyycyPwYeNzRPZjX9cvbjHqpz-0YTiNDugC3Wi27b3xJQkUbkXm0a9uybu54VX3PKXVVfbm5noumacc7QG0VL1fUPmH3-a2LX9km_GbiuvCWCDQNqiDlGleGnjyraBTJ9CbmwSIL13bLa35tYVCcUqCpWf6MuN1jm5K9k7VYnFrYe-qMUePgg5pFaeC7reGD1bR8mzePqE4kJZ1Wid-WYorAq_5pCntHCl8aXdg1opQmhaE2KXvqm7F4NvoBBTA50TD363I9L0EMFPF2GHnU1duXfs-RO-AtBNovPmwSRpQJBab4aHb3wQSqxMeXIU7Geb-QHXJggSsk9LcsZKXVxXU9bqg_JE0TGmlwn2JdA2B8WNA_Y9ZTOslMlM1qWh7yG9OiUARZBNTkzKQS8Yngv5s9gJEToK7X9vp580EN8WswFmn4jxBdqB6d16JnD7ERZyi6c1QfC17QWiK0gP2FdJ-E-54zhwHwB_mcIEm9yo8Oe-Lya7lINrMPkowOar7glHBi1joobHviNdY98Pc5_YgZAA-fWwWg14Lf5BGAQ3MK2xgqAjJJ5FP-S_4tNaVClCEJ-Aev8SqOrsQwARaL1DIpAxkhMXvrDf7qTp_Rp6H5G56W3f6dDC1YUOjb32aBJejoyfqNtl8N5uXTPDXRgFBo20Fs__fK6QpxOAU2qSpQoZa58xIi_KeKVxWCr_LBtn3V7YpgScqnxoC138qAqEczZgh0QSKD7zUU6p1LRfXRepPiYbrRmDdBkFoMM2XUQs9pGd5z1NuubHCGS9pyWlgyOcXKwwrR9GL8B3f3W796AzXaZH1DueXWLddKLm5DUHt6o8cl_txUkXUDi222HrNbiWoswrCXVKbrKfd3Bk5Ij0GEuIrlZHksDb7f57rVXZqlrFj-yPiELnPheUVVmadvgLonZS5U7-jHDTtz0yXiIg8xB9OgEW4IXZCQKvHvVfEacxOJlALel3qbiqbTTwBnQTuYgYXtuePTA5XKlm0eGYC-8Kn6KNnjekFeoONtQPLOKtnvA2o51pRFXCi6D2WrG2dnLi6641eyrRs5BA_r_Upqb2tj2oOAP22iK0H4xmD97Gl2_F1jYUzZ2zblfJSMjz9tQWRAG6DSCtSLfZU9DEyw33hnGlNw7lYErbqJGH2sMo5Hh081HbXAT5FVLOy6-_so2_FWsbPWkFXZKi1oXRwk5upMlXV3HUdvuErurWM_KYFDq-Tirph34CAewGqBG1ZxDRr2bq52WrbLJv_qiX-ZM6dP053DgYeIAqCZI8_5nd_BP8jrmQA9WwWlm5jt7kmiwv36mLklITEPjvpBeAcqI-3dgIxGqRmG4grQygN-mOyfJlBjYdSarAdLigsA7UlrDMAZIx-QA5B151dzmwV4erLNDPoDOtf0eCOeqeef_105u" style="width: 100%;">
                        <p class="caption">모든 time step의 attention</p>
                    </div>
                    <p>
                        <br>따라서 실제로는 모든 time step의 각 픽셀의 score 합이 1이 될 수는 없지만 그 방향으로 학습이 되도록 "규제"하는 것이지요.
                        <span class="highlight" style="color: rgb(0, 3, 206);">논문의 저자는 이 규제가 캡션 생성 과정에서 각 이미지의 모든 영역에 골고루 집중할 수 있도록 도와주는 역할을 할 수 있다고 주장합니다.</span>
                        즉 이 방법을 통해 모든 픽셀이 캡션을 생성하는 데 중요한 기여를 할 수 있도록 유도하는 것이지요. 그리고 전체 time-step 동안 모든 이미지의 부분을 똑같은 score로 집중하는 것도 방지할 수 있습니다.
                        최종적으로 regularization loss는 아래와 같이 나타낼 수 있습니다.
                    </p>
                    <div class="equation">
                        \[\lambda\sum_{i}^{P}{(1-\sum_{t}^{T}{\alpha_{t,i}})^2}\]
                        \[\lambda:\,regularization\,constant,\,P:\,전체\,픽셀\,수,\,T:\,전체\,time\,step\]
                    </div>
                    <p>
                        <br>즉 위 식을 이때까지 들어왔던 예시를 통해 설명하자면, P: 196, T: 32가 되는 것이지요.
                        그리고 우리는 기존 \(\sum_{i=1}^{P}{\alpha_{t,i}}=1\)만 사용하는 것이 아니라 이제 규제 term을 추가하여 \(\sum_{t=1}^{T}{\alpha_{t,i}}=1\)까지 고려하기 때문에 저자는 이를 doubly stochastic attention이라고 부릅니다.
                        따라서 최종 loss는 아래처럼 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[Loss=-log(P(y|x)) + \lambda\sum_{i}^{P}{(1-\sum_{t}^{T}{\alpha_{t,i}})^2}\]
                    </div>


                    




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>Hard Attention</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>Hard Attention</b></span>
                        <br>Hard attention은 각 픽셀별로 얼마나 집중하고 있는지 계산하는 방식이 아닙니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 attention score를 직접 계산하지 않고 확률적 sampling을 통해 영역을 선택하는 방식입니다.
                        즉 soft attention은 각 픽셀 196개의 attention score의 합이 1이였지만, hard attention은 하나의 픽셀만 1의 값을 가지고 나머지는 0의 값을 가집니다.</span>
                        즉 one-hot encoding의 결과가 나오는 것입니다.

                        <br><br>좀 더 수식적으로 접근해보겠습니다.
                        먼저 \(a\)는 이미지가 encoding 된 feature, 즉 B x 2048 x 196의 결과라고 하겠습니다.
                        그리고 \(\alpha_{t,i}\)는 t-step의 각 픽셀에 대한 attention score라고 가정해보겠습니다. 이 score는 soft attention에서 softmax로 구한 픽셀별 attention score입니다.
                        즉 \(\alpha_{t,1} + ... + \alpha_{t,i}=1\)이 된다는 뜻입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">마지막으로 \(s_{t,i}\)가 t-step의 i(위 예시에서는 최대 196)번째 픽셀에 대해 one-hot encoding으로 나타낸 값이라고 하겠습니다. 즉 \(s_{t,i}\)는 0, 1 중 하나의 값이고, 하나의 값만 1이 될 것입니다.
                        그리고 모든 \(s_{t,i}\) 대해 동일한 확률로 1을 줄 순 없겠죠. 그렇다면 \(s_{t,i}\)가 1이 될 확률은 어떻게 구할까요?  바로 이 확률 값을 soft attention에서는 score로 사용했던 \(\alpha_{t,i}\)가 됩니다.</span>
                        그리고 \(s_{t,i}\)를 아래와 같이 나타낼 수 있습니다.
                    </p>
                    <div class="equation">
                        \[P(s_{t,i}=1|s_{j &lt; t},a)=\alpha_{t,i}\]
                    </div>
                    <p>
                        <br>위 식에서 '\(j&lt;t\)'라고 적은 이유는 이전 step들을 고려하여 나온 hidden state를 attention에 사용하기 때문입니다. 그리고 attention을 할 때 \(a\)도 사용 되기때문에 조건부 확률에 \(a\)가 있는 것입니다.
                        예를 들어 '개가 잔디 위에서 뒹굴고 있습니다'라는 캡션을 생성하기 위해서 현재 step이 '잔디'를 예측해야한다면, hard attention을 위해 잔디에 해당하는 픽셀의 \(\alpha_{t,i}\)(1이 될 확률)가 높을 것이고, 반면에 '개'에 해당하는 픽셀의 \(\alpha_{t,i}\)(1이 될 확률)가 낮을 것입니다.

                        <br><br>우리는 일상생활에서 날씨 확률을 계산할 수 있습니다. 
                        예를 들어 오늘이 맑은 날이었을 때, 내일이 <span class="highlight" style="color: rgb(0, 3, 206);">맑을 확률: 0.5, 비 확률: 0.2, 후보 C 눈 확률: 0.2, 태풍 확률: 0.1</span> 이런식으로 나타낼 수 있습니다.
                        <br><br>위의 \(s_{t,i}\)도 동일한 맥락입니다. 
                        <span class="highlight" style="color: rgb(0, 3, 206);">\(s_{t,1}\)가 1이 될 확률: 0.4, \(s_{t,2}\)가 1이 될 확률: 0.05, ... , \(s_{t,196}\)이 1이 될 확률: 0.1</span> 이런식인 것입니다. 이때 이 확률이 \(\alpha_{t,i}\)가 되는 것이고요.
                        즉 \(s_{t,i}\)의 확률은 일상생활에서 볼 수 있는 multinomial distribution (다항 분포)와 그 모습이 비슷합니다. 하지만 이 확률의 숨은 의미는 1이 될 확률이기 때문에, 즉 1개의 값만 1이 될 수 있기 때문에 <span class="highlight" style="color: rgb(0, 3, 206);">multinoulli distribution이라고 볼 수 있습니다.</span>
                        
                        <br><br>이렇게 구한 \(s_{t,i}\)는 특정 1개의 값만 1이 되고 나머지는 다 0으로 이루어져있습니다. 이렇게 구한 \(s_{t,i}\)를 \(a\)와 곱하면 아래와 같이 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[\widehat z_t = \sum_{i}{s_{t,i}a_{i}}\]
                    </div>
                    <p>
                        <br>이렇게 구한 \(\widehat z_t\)를 attention의 결과로 사용하는 것입니다.
                        즉 soft attention과의 차이는, soft attention은 \(\alpha_{t,i}\)를 그대로 사용했다면, hard attention은 \(\alpha_{t,i}\)를 바탕으로 하나의 값만 1로 바꿔줘서 사용했다는 점입니다.

                        <br><br>이제 loss function을 생각해보겠습니다.
                        우리는 이미지 feature \(a\)에 대해 캡션 \(y\)의 log likelihood를 최대화 하면 됩니다. 그리고 이 log likelihood 식은 위에서 구한 \(s_{t,i}\)를 이용하여 아래처럼 나타낼 수 있습니다.
                    </p>
                    <div class="equation">
                        \[\log{p(y|a)}=\log{\sum_{s}{p(s|a)p(y|s,a)}} \geq \sum_{s}{p(s|a)\log{p(y|s,a)}}\]
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">즉 우리가 구하고싶은 log likelihood의 lower bound는 \(\sum_{s}{p(s|a)\log{p(y|s,a)}}\)가 되고 이를 loss function으로 사용합니다.
                        즉 저자는 lower bound의 log liklihood를 최대화하면 원래의 식도 같이 최대화 된다고 주장하고 있습니다.</span>
                        즉 최종적인 loss는 아래처럼 정의됩니다.
                    </p>
                    <div class="equation">
                        \[L_{s}=\sum_{s}{p(s|a)\log{p(y|s,a)}}\]
                    </div>
                    <p>
                        <br>따라서 위의 loss를 모델의 weight \(W\)에 대해 미분하게 되면 아래처럼 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[\frac{\partial L}{\partial W} = \sum_s \left[p(s|a)\frac{\partial \log{p(y|s,a)}}{\partial W} + \log p(y|s,a)\frac{\partial p(s|a)}{\partial W} \right]\]
                        \[= \sum_s \left[p(s|a)\frac{\partial \log{p(y|s,a)}}{\partial W} + \log p(y|s,a) \cdot p(s|a)\frac{\partial \log{p(s|a)}}{\partial W} \right]\,(\because f' = f \cdot (\log{f})') \]
                        \[= \sum_s p(s|a) \left[ \frac{\partial \log{p(y| s,a)}}{\partial W} + \log p(y|s,a)\frac{\partial \log{p(s|a)}}{\partial W} \right]\]
                    </div>
                    <p>
                        <br>하지만 위의 결과처럼 모든 s에 대해 경우의 수를 구해서 parameter를 업데이트 하는 방식은 오래걸리고 비효율적이기 때문에 Monte Carlo Estimation을 통해 충분히 많은 N개의 데이터를 추출하여 이들의 평균값을 이용합니다.
                        충분히 많은 N개의 데이터를 이용하여 전체의 데이터를 사용한 효과를 도모하는 것이 바로 Monte Carlo Estimation이고, <a onclick="pjaxPage('VAE1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">VAE 글</span></a>에서도 언급한 바 있습니다.
                        따라서 위의 미분 값을 Monte Carlo Estimation을 적용하여 아래와 같이 다시 적을 수 있습니다.
                    </p>
                    <div class="equation">
                        \[\frac{\partial L}{\partial W} \approx \frac{1}{N} \sum_n^N \left[ \frac{\partial \log p(y| \tilde{s^n},a)}{\partial W} + \log p(y| \tilde{s^n},a)\frac{\partial \log{p(\tilde{s^n} |a)}}{\partial W} \right]\]
                        \[\tilde{s_t} \sim \mbox{Multinoulli}_L (\alpha)\]
                    </div>
                    <p>
                        <br>이때 \(\tilde{s_t}\)는 multinoulli distribution에서 N개를 sampling 한다는 뜻입니다.
                        하지만 Monte Carlo Estimation을 하게 되면 분산이 크게 나오는 문제점이 있습니다. 우리는 전체를 고려하는 것이 아니라 일부 데이터를 고려합니다.
                        따라서 매 추출마다 무작위로 서로 다른 영역에서 선택되면 분산이 커지는 것은 당연한 것입니다.
                        이를 방지하기 위해 저자는 아래와 같이 exponential moving average를 사용합니다.
                    </p>
                    <div class="equation">
                        \[b_k = 0.9\times b_{k-1} + 0.1\times\log p (y | \tilde s_k, a)\]
                    </div>
                    <p>
                        <br>위 식은 이전 정보를 90 %, 지금 정보를 10 %만 이용하여 업데이트 하겠다는 뜻입니다.
                        또한 분산이 커지는 것을 막기 위해 entropy term \(H\)도 추가로 넣어주고나서 최종 loss는 아래와 같이 표현할 수 있습니다.
                    </p>
                    <div class="equation">
                        \[\frac{\partial L}{\partial W} \approx \frac{1}{N} \sum_n^N \left[ \frac{\partial \log p(y|\tilde{s^n},a)}{\partial W} + \lambda_r \big( \log{p(y|\tilde{s^n}, a)} - b \big) \frac{\partial \log{p(\tilde{s^n} |a)}}{\partial W} + \lambda_e \frac{H[\partial \tilde{s^n}]}{\partial W} \right]\]
                    </div>
                    <p>
                        <br>위 식에서 \(\lambda_r, \lambda_e\)는 각각 exponential moving average와 entropy term의 적용 비율을 나타내는 파라미터가 됩니다.
                        뿐만 아니라 분산을 줄이기 위해 주어진 이미지에 0.5의 확률로 s̃의 값을 s̃ 대신 \(\alpha\)로 넣기도 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 이렇게 구한 최종 loss 식은 강화학습과 비슷합니다. 왜냐하면 1이 될 위치를 고르는 action에 따라 \(L_{s}\)라는 reward를 받게 되는 방식인 것이지요.</span>                    
                        본 논문에서는 이 두 방식을 사용하여 나온 attention의 결과를 비교합니다.
                        아래 이미지 중 위의 결과가 soft attention의 결과이고, 아래의 결과가 hard attention의 결과입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_ElPqxsy9EYq4qJhxpwBOHNehDbtJeSJazaQHJrwXsh2qSuuDoBjTYQzcXMOEB-D5anBmPQttML9ztovWjdPqsWun5X9LPoZqkAkF6JATFdP2zuIi9Iqa7_ZERfPYUpapQma-HOm2mDUoNqsKhgtg4FGoQflhXXuBkI-WeCb3iVU_3FgDKeFsenygSlgZ_UFz-j7JnYU5E3w6oDC6MUNW0UUZlpNrgefQf03faThyx1yOkYDc-mz9sxWCCRgLtrE5Ucprgi-9-ug6FWImJjqPlWiCRq_CPT981F4mli4rsrn1eegOigFSzPEAOv8HypglxPMY_dJfktUU-29P2fNiDncbn3L_kj5WMT-v3RMPlX4rrpHWrozeDLJMQipT_Plfhq1pWwyf-6w-xGjDuRrFRN5s-guS2ZzoVcSTSoNVRFRDF-2f7bSHUw0pNgCKd2oHBkNUzbQ8cOPlwGBc5eE2NvhKYf0nbSj21Ujl71yNOXOeDCK1dt9Q2pji7WvlX9wQPmQOcY8IyiPGAehcDiAajGO1Huha6Ah_4x5yNg6METEtkbMkHFhupm6aYdcs0yKSSNGVJ2UsM3PYTvPPghCVCMvFf3goCGjjMS-92fIBV4aSwHaESjl0_6JJidbMNkQGNj-LuXlu2oCJwKMh2gOMiL1XP10Vh6Wr7kLj_OSGhTKvpLeSGIKcbm5ypR7al7_CHCAe6k9Bx4VF-hcUxbNeFOtSQYWYpeza54fSUic2Zn8lPPuAaQGOR2m_DKhUHQC2mqMv4g2huvEihic_VikHDT8q0vtfFvSw44qTzO4OqLNt0B3HblF6mHINd_p1GP7qKeUjrZiPEAXr8yAPIPBgJkAi01nXbmA7ioud2asCM9S09M4XyCK2JX091j9UURwFsNJgJqdP4eUjpGLQrVoMLiVIwJnY5_SlC70atsE7sOjtRHIUxSbyZg3Ggq44QdsG6NlcvI5Su9KaZV1qjqxAyfTHxPg19A-RMoc7asbnsWgk3eTiU42gDl1Xg7GRQChB28-IrT8g5c4I_mTSDOUVGr1Wmg94hfmSPYjZ50V_QIZLvOm9XGrBlsolemkqhKdeO17CsxMusTwwjHPFZidOkd1C0P2xB8ZabR1ccY-34u9RvmduFW4N38GFv4lxqUME_OepHwEEndD1j84VcuDGAdQ4lU77g4sjmWy8L18wHbREVJQ7gtp4pMyCzncvSuRYmDzohAtwnTuJud6Rwr31wSq3rRBQBT-DKFypYaDTOqYUuc-fhemN-Rb1rFtP83Q4rH7sJvQwg7IyxX3w4DqEyZUiiHY8dolRuASpfpH877-FPJL-IMTMK-RFoPyW8WS-63oX2VQy4vqrEpfbCHPbZ0z6vAAr4CqTY9XnDkFrT2-ADMgjLOlHGNsI3PS6BYhi4789ITplFqXzbyaLru3Z4RrrLFSyzORC2F_7JVsmdVMNQeQp5e_g5gWsvYYijmUONK2t6r173fSYkMu3zkgtfZbpnjokU4xmPgtuRvLP4N-7yU8ycS9GEojFNjTfZBVY7AoH2Dnw" style="width: 100%;">
                        <p class="caption">Soft, hard attention 결과, 출처: Show, Attend and Tell 논문</p>
                    </div>
                    <p>
                        <br>논문에서는 캡션 생성 모델 성능 평가를 위해 BLEU, METEOR 지표를 사용했습니다.
                        COCO, Flickr8k, Flickr30k 데이터에 대해 실험을 진행한 결과 hard attention을 사용했을 때가 soft attention을 사용했을 때보다 더 결과가 좋았다고 합니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EL7FIHYdjTbJJXEJkQ12M3SIAXc-AWk-EwNVb3iclHAViSCaVzwYx-Be5nVNS0muaJ90gkUu3PIrcR6l28LXKZNNBvh26vY0Xuj-Jo4XUPqcqQJnNZiEm3_bdn2cC-8QvHIaG_GfW4Z0qQzlNV4mK5DY5inbIVu9QKPpDUkyMXlVsw-KQFrVcz32v7wgVEb1UtT3XtQhTqyZcmYI-7MWOYSpZjZhe1WSxtTdzCPC28gymoCG_m6y_SNvzEPH26lppHY2LoqIoiSdsKLQP-h_QIIanFxJalHxgrgz0vC__oRXtZ3QiJIQArhmJfekX5nu5aJ6N5hOqYqbxqxGqw4w3hek7J3pz8pxoFZU-d9UPlYdlEEsG3Fk32xowydCD3czdkkSpcaW0Yxx2qx7LIjienM8Koo-OWsuijjfPBzZ1s1z54CjUHPPxAfZfNlYouOHTtF5UxHR6627ryiYoRhnA8DQ1hN4dc7StHqoncUg1ulLxJZAOmhKvm9_dn_RUkpon2s46hKLfJwIdt87NLGKGRhGbgZst3rOJXAu94OgENo5jM9RWlPiwBKyNBgEVFlKDMEFxK5kssl6kodmFtPq5wMf2xbCibBrQ4Mqz39hR4fEV117-xLkuw54Iuo1XV5LF32i1JeXYsLYlKsylaBrnQ-ehIl17mmAvp2RZ7C_fajP0QbRRnXGn059y6hmOH9_3m_D2NMLO6JbrPw2oMVkGyjGSL_EEqNt7NRF0ee3DPMZmTGkSwOehtVm40O5J6Tv48aaOMN_12ltm1whVWV-NTBXxh8_ws5bHAv9wJf4-dGbq5AOt0rv_LLhxoBYNfTpu9mymA1SLtwwzHdI80DPGGkLVDHDfsqD_SIBtOkImYGvPtWtwJwlWX4Kmb4TNd13cFFouXEIHvsWG_oIHks1rbx7jkFjWCMFzlW08oYAOkM2fvIGeuYPNYJF4WHp_JwzZNqQadvwbnS94Ru6_YW1PmTDOJHdqXwgq3adZTvBHu-8iy1d1Pjvs0pCgopwQ5wbFdRg6z9xuppGqzEqJOPCvLintcxSAMJAbQK7z1SQYUteY62LkksrDlv44UZqDzve6X_QJG-HbRFdGRa1zBLVGntr-Q--y7m-1oC-9e7GrSioxsX0yu7q51C_IGliq3qXIHn-fz9IRad5BzUGGRlSdNqLNzJo8oNT_XBNliQDvrhhsEDac-caaIzO5CSPTTeiXNdwKLue25NwJ7EH3ODTho3dHG8ukb9kSw_NdYqtMO_aCnLuKZGEOAvbM0gykAuXvssYvTgVNlRCJkcje-l8mzvSNaRAv9gPRDcvNeHDaUenGpCWPWQ5t1fDWXR7911kKKOs358aB2EZ9K-cwQWknjJo4CUQov8whYJnvyBVxG1u4yb7db3z1Y_WjqKmcIx9yJ9d0Yr-Waql5vzGDIffRFZYaSndCw81AkhV14Xu12HyAcrRyde2vLviGei6MDwFgFwrnP9nYP_j68o2ysvd5BvUuLGE2w8KLmAZirAk6eeU1Pemj_8p3Tpl2oiMmAEcZwhoc1H2SI" style="width: 100%;">
                        <p class="caption">모델 결과, 출처: Show, Attend and Tell 논문</p>
                    </div>

                    
                    <p>
                        <br><br><br>다음에는 Flickr8k 데이터를 바탕으로 Show, Attend and Tell 논문 구현을 해보도록 하겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#ImageCaptioning&emsp;#SoftAttention&emsp;#HardAttention
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('Image Captioning 첫 게시물 입니다.\n\nThis is the first post of Image Captioning.')" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('img2txt2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>ResNet, LSTM을 이용한 Flickr8k 이미지 캡션 생성</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>