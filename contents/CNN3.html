<!DOCTYPE html>
<html>
    <head>
        <title>Residual Network (ResNet)</title>
        <meta name="description" content="잔차(residual)를 학습하는 CNN 기반 모델 ResNet을 소개합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/CNN3.html" />
        <meta property="og:title" content="Residual Network (ResNet)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="잔차(residual)를 학습하는 CNN 기반 모델 ResNet을 소개합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/ALs6j_GjKRmTK2giQFP2j_a8vXLWNJyC1gggeqRwP-ilxwAT-CyC6Xav5m2opwowM9iC4eTWpXgQf_a6MBArSCuoSXeT3_PQn_9afZZurSWM_f0GUKLwBiJ7qCwdNeQAgoEEHqCuKzhZNP-tjn46htnIFDDnPGaSWoHMeFVVw-Pj6YLqFnD3QUPWdWl8CltaFLzqGemh1Uy4UxqyaCZK559zgah1WcMz942ULLrK323sHG5xXq7YPoUgmjlsxJ9lKBVkU_yxtUJfh-qlzt-KnNamMIOhSjzF29qCW94W1KB8sLlYwHpD8-e4GcImXBuTbaql3X-vvNohy4ONjcqgpq701fUhVVGpltjWlz0ZBDO06jEFnAX-6yB2fmbidyxjnDu9ANYwJgubAAgNGPztiDZ3mRnr8QA4Ty00DJLoZdefzIoYHQrkdd2dh704dkLkynNqRrrtytMu2ozDMOoZsznlMSw_b4ft_QWTbs0x13mQIb5pcGm7jBj5ovNnZ_TgKsYWc07sr-9NWosMpV5Pckt2Oz1AIMYoy1UxMuDuCpRwhymdwwWpSjjnYzgxk3zc6jx3Xgx5WeKg0Jd1tWaigvUTtmhvBYxdCfeiLr29pWnyiPqHCP1MHralg_xL1p4Bd_Ukd8y24Baf3A9EJdNPy9r9DmmTNUjswIodQHw0L0XgYpYmQBBkf7LZ6D87G4zmc261_d3hvp3yUxGzsZstPH8yufIlRmadXoILe54NO2i1nqesf93myiY8OWdl5ToppQ3gyWTBjXsbYfKqO9eY6jca3ohV_-2kqnOQNOuQ6EmHTPu4LEuUEtCrAw7NMiSVmdh5GEGNp2tiAF9GSqyrHUWLP2kP_m7cD1L4je6_xNUH8UDGFK0Fan35wUZySADojYUQNpf2XqIOR8PjdFKWUMiZnlGpAmyobZU-dhmsSwuUOkCY9LuINYdGMWqNHRb9S_4BSg6jFa5URPIZOrfyH5kwyBSTCQN90kVjJXnkpfLYDQt-hmPpasdabVkvWXTtWHr94CISvEeDcOha8QsVeSt_2CclFMzNxyWxPu1s3gxEUsQorzylvPMeBMqfR4K6n3Ua4BYJnBUMVmo_6-zrjh_cKSzSuVa_iqhHooO0Z9lXsdDQd_OGVgnAQ926WFYGhlYP_WC-XF5cN_0Qs5iI7cbOBa1L9GWEAewb5lPZXaxMEW9rLb-J-p008tu6DAaZxHKMrOzuBhq_BpASpvjoZqTicHsXgIIDcextbEUCbNzwq_DEhxazK6j-G_afQ5l2kV83YvqxMPHZV45WqvRBm-ovvnMQr2hWWaGMeY4MklN7yHGUyvtz7IBlgjci-HMDz3RgOhyMvB4k-Z3wdBYMoMOdj6F5I6FKoTTNTL-WKbQ-oY70SuWEkeOl7I1dp_F5ZarwwP5Vbp6-2Q2xZ-Gwkrpk7eSRdL3IYRu35xBuaNKqfEDLBXxZFYc-kHhwnzrlK09xkIyJlZP3Jj2rX6Ykc56fKDRlDR5_iRJaw3XJwHbUgO7_fKrRO64zdCsGWAMQVPk9zGXn0qYaOxgx" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Convolutional Neural Network (CNN) &amp; Residual Network (ResNet) / 3. Residual Network (ResNet)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/ALs6j_GjKRmTK2giQFP2j_a8vXLWNJyC1gggeqRwP-ilxwAT-CyC6Xav5m2opwowM9iC4eTWpXgQf_a6MBArSCuoSXeT3_PQn_9afZZurSWM_f0GUKLwBiJ7qCwdNeQAgoEEHqCuKzhZNP-tjn46htnIFDDnPGaSWoHMeFVVw-Pj6YLqFnD3QUPWdWl8CltaFLzqGemh1Uy4UxqyaCZK559zgah1WcMz942ULLrK323sHG5xXq7YPoUgmjlsxJ9lKBVkU_yxtUJfh-qlzt-KnNamMIOhSjzF29qCW94W1KB8sLlYwHpD8-e4GcImXBuTbaql3X-vvNohy4ONjcqgpq701fUhVVGpltjWlz0ZBDO06jEFnAX-6yB2fmbidyxjnDu9ANYwJgubAAgNGPztiDZ3mRnr8QA4Ty00DJLoZdefzIoYHQrkdd2dh704dkLkynNqRrrtytMu2ozDMOoZsznlMSw_b4ft_QWTbs0x13mQIb5pcGm7jBj5ovNnZ_TgKsYWc07sr-9NWosMpV5Pckt2Oz1AIMYoy1UxMuDuCpRwhymdwwWpSjjnYzgxk3zc6jx3Xgx5WeKg0Jd1tWaigvUTtmhvBYxdCfeiLr29pWnyiPqHCP1MHralg_xL1p4Bd_Ukd8y24Baf3A9EJdNPy9r9DmmTNUjswIodQHw0L0XgYpYmQBBkf7LZ6D87G4zmc261_d3hvp3yUxGzsZstPH8yufIlRmadXoILe54NO2i1nqesf93myiY8OWdl5ToppQ3gyWTBjXsbYfKqO9eY6jca3ohV_-2kqnOQNOuQ6EmHTPu4LEuUEtCrAw7NMiSVmdh5GEGNp2tiAF9GSqyrHUWLP2kP_m7cD1L4je6_xNUH8UDGFK0Fan35wUZySADojYUQNpf2XqIOR8PjdFKWUMiZnlGpAmyobZU-dhmsSwuUOkCY9LuINYdGMWqNHRb9S_4BSg6jFa5URPIZOrfyH5kwyBSTCQN90kVjJXnkpfLYDQt-hmPpasdabVkvWXTtWHr94CISvEeDcOha8QsVeSt_2CclFMzNxyWxPu1s3gxEUsQorzylvPMeBMqfR4K6n3Ua4BYJnBUMVmo_6-zrjh_cKSzSuVa_iqhHooO0Z9lXsdDQd_OGVgnAQ926WFYGhlYP_WC-XF5cN_0Qs5iI7cbOBa1L9GWEAewb5lPZXaxMEW9rLb-J-p008tu6DAaZxHKMrOzuBhq_BpASpvjoZqTicHsXgIIDcextbEUCbNzwq_DEhxazK6j-G_afQ5l2kV83YvqxMPHZV45WqvRBm-ovvnMQr2hWWaGMeY4MklN7yHGUyvtz7IBlgjci-HMDz3RgOhyMvB4k-Z3wdBYMoMOdj6F5I6FKoTTNTL-WKbQ-oY70SuWEkeOl7I1dp_F5ZarwwP5Vbp6-2Q2xZ-Gwkrpk7eSRdL3IYRu35xBuaNKqfEDLBXxZFYc-kHhwnzrlK09xkIyJlZP3Jj2rX6Ykc56fKDRlDR5_iRJaw3XJwHbUgO7_fKrRO64zdCsGWAMQVPk9zGXn0qYaOxgx);">
                    <div>
                        <span class="mainTitle">Residual Network (ResNet)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2022.07.28</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이전글에서는 convolutional neural network (CNN)에 대해 설명하였습니다.
                        이번글에서는 CNN 모델을 기반으로 하는 residual network (ResNet)에 대해 알아보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">ResNet은 당시 CNN 기반 학습에 있어서 큰 혁명을 가져온 모델입니다.
                        그 당시 최초로 사람보다 더 좋은 결과를 낳은 모델이기도 했습니다.</span>
                        <span class="highlight" style="color: rgb(0, 3, 206);">이 글에서는 ResNet의 원리, 목표 등 ResNet의 전반에 대해 살펴보겠습니다.</span>
                        그리고 CNN에 대한 설명은 <a onclick="pjaxPage('CNN1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.

                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>ResNet의 등장 배경</li>
                            <li>ResNet의 원리</li>
                            <li>ResNet의 구조</li>
                            <li>ResNet 결과</li>
                        </ol>
                    </p>



                    <h1 class="subHead">Residual Network (ResNet)</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>ResNet의 등장 배경</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        컴퓨터 비전과 이미지 처리에 있어서 convolutional neural network (CNN)는 필수적인 모델입니다.
                        꽤 예전에 transformer가 등장하고 나서 요즘 이미지도 transformer로 처리하는 vision transformer (ViT)가 많이 사용되긴 하지만 여전히 CNN의 입지는 강건합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그중 residual network (ResNet)는 한창 CNN으로 ImageNet 대회를 할 때 신선한 충격을 안겨준 그런 모델이었습니다.</span>
                        현재 글 작성 시점에서 ResNet 논문의 인용수는 무려 10만회가 넘어가며, 계속해서 증가하고 있는 추세입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 ResNet의 영향 때문에 transformer등 다양한 모델에서 residual connection을 기본으로 사용하고 있기도 합니다.</span>
                        
                        <br><br>먼저 ResNet의 등장배경부터 살펴보겠습니다.
                        당시 한창 ImageNet 대회를 통해 top-5 error가 어떻니부터 SOTA (State of the Art) 모델이 어쩌구할 때 등장한 모델이 바로 ResNet입니다.
                        ResNet 등장 이전에 다양한 ImageNet 모델이 있었습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그 대표적인 예가 VGGNet이 었습니다. 이때부터 이미지 분류를 위해 아주 깊은 모델이 만들어지고 있었죠.
                        그리고 대부분의 경향이 깊은 모델일수록 그 성능은 얕은 모델보다 뛰어났습니다. 어찌 생각해보면 깊은 모델일수록 feature representation을 잘 할 것이니 당연한 결과이기도 했습니다.
                        하지만 모델이 너무 깊어져서 10개 이상의 layer, 즉 수십개의 레이어를 가진 모델의 성능이 오히려 낮아지는 현상이 발생하였습니다. 이 현상이 바로 degradation 현상입니다.</span>
                        
                        <br><br>여러 연구자들은 위에 대한 문제의 원인을 처음에 아래와 같이 생각했습니다.
                        <ul>
                            <li><b>레이어가 너무 깊어져서 training set에 overfitting이 되어 성능이 안 좋아진 것일까?</b></li>
                        </ul>
                        <span class="highlight" style="color: rgb(0, 3, 206);">모델이 깊어져서 overfitting이 일어났다면, 깊은 모델의 training loss가 얕은 모델에서 더 낮아야할 것입니다.
                        하지만 아래 그림을 보면 알겠지만 test loss에서도 물론이고, training loss에서까지 얕은 모델보다 깊은 모델이 성능이 안 좋은 것을 볼 수 있습니다.</span>
                        <br><br>따라서 연구자들은 아래와같은 결론을 내립니다.
                        <ul>
                            <li><b>모델이 깊어질수록 성능이 안 좋아지는 이유가 overfitting 때문이 아니라, 모델 최적화(optimization)가 어려웠기 때문이다.</b></li>
                            <li><b>모델이 깊어질수록 gradient vanishing, gradient exploding 현상이 발생하기 때문이다.</b></li>
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FGTibz7FyEKhTY2Fo_OFla5TTbXwAQ7xzYfDKguFzdQIqRu7RkbNsEAyLcTPMTlP7SNRe6PNmQhMnYRitx9xb6YmhBTIFsPzAb7x0glJZsZTnFudg-ngvM9scmwuDmj-Qr_1J9ssFIBPMsJ2M_sb-a5GnCFJPMIrzzVwbzCo6J6S7-QLUadPNI6A094sM8qAx1xSA_isXH7LkaRWq2ObSfN5HGfUlZLhx_prhwclKpxnYzV3u7rNY2jk1N2PSzpbMH56NFKB7y0kOHiw4dp7H-TOLr81rjgKdOJkD18qk7cvavg_fJl5kkM-efqohfAOYUySHWd3N_SlXApI-o6vBHtUrfBLjxS8tMxgFgNCook3zXfIXYdnDn_ooB_m_odvcAxHj6cLLvsJ_2MPBUGDZ08yILD9SU1i5W5gazmpkpCs57zy1A8MwWTZAg9HW1fSggpXodr8g_0n-U6nEoGK6lImGVH8pH8VdlPI8429-adva_sXe4M9MXWEPRUegV0Z-jfJksb8HYY1LMjSWtwWlHsWWMyZB1s-WHjTQsJVvKKbF635408hCIlgmJgUvZt-JH6jDrBcKhwznOclSVt7Z1fATYGeMtGzeKXHB6Kdy-eGDBy4ovkD9ZDAkGKe2ELyKUnbNTKQNURtekHiwpOMzinMsQI2HKMbvGVmX12s0sfTaCf6FAn1XSXN_qfMOvhnhC2d231towmvtMBPzJ3kZI8Cy46mp3RiW71d_4XwV8YjVHJPe7nrQeosKDqdDqTER2WJUd2P2ge6m7dQrTSz8D52eHtpNHuhxw6I3osAu1wnZQbPdg-gdrbXxze4i03EZkOxgzWvIdP0OHyrhWHTSY1h4y8zAHFKPUxLMuc156HHbuBCU-Rg5FMWSB6T-4RgYjN64Or0M6JeBxnWbbgPDAHLKEbO-Xli51jHjVu_wgu9rdsZ-r2Ck0WU6RfuDeFgTAAkVsNY4957LBJnqRNMDgG9uU7BsbP9uWowdiiH0d8JXwf20JG97EuYTwX_AnYeFvFuQEuIMcUZz0Zan4giTyN6zOF5M75i9mgnPbBXOO1PR27lVZhvmODsBI8jvflgRgusvuFBE-7uOXM4OhZfpzQxVx57Cza3fxNSGA5vburGu_u0vqEY1VOeuQuO8kyglTM7iVDjwo3_gyiY7rr6YoI68KysYbYCBMy9jcpavdGcomLJNP3dMrlZpRIbfAoa4z4s7VAG4a_BMM0lXS4H5W65tV7ckPvJ0XrDU2r_ypebyA87vbkLsHam-4mUoTXEYv2zKBvJYbu1qj8wuHZ6vcPGEQxFZb8WXHL4mNJy4CdahkcrRMntQV8HhsnhhXGg4o1tRoDyu_s14J37ffUZxqd2MOcxI1PAS2V0GQV-BlszL14zJSb8TsXPk09YAh4m9qznBbIpVkojZBpkWtrwTQjanYWWmSfo6NtRlXa7sNLSh6r64_Ewm1o7FJ1blXn00m8zRyJR9zriH7mvf0Wnq-cF0j0D_cE-Pf_tNFx-bWTMOca9d6dzBcq2qY0Nkfz-qGGMeujje7" style="width: 100%;">
                        <p class="caption">깊은 모델과 얕은 모델의 성능, 출처: ResNet paper</p>
                    </div>
                    <p>
                        <br>그럼 위에서 언급한 두 가지의 <span class="highlight" style="color: rgb(0, 3, 206);">문제점을 해결</span>할 수 있는 방법은 무엇이 있을까요?
                        바로 아래와 같은 간단한 방법이 있을 수 있습니다.
                        <ul>
                            <li><b>성능이 가장 좋을 때 까지의 얕은 레이어는 이미지에서 특징을 추출하고, 그 이후에 더 깊은 레이어는 얕은 레이어에서 추출된 특징을 그대로 유지하면서 output을 내는 방법.</b></li>
                            <li><b>얕은 레벨에 있는 레이어는 이미지에서 좋은 특징 추출이 가능해야함.</b></li>
                        </ul>
                        <span class="highlight" style="color: rgb(0, 3, 206);">좀 더 간단하게 20개의 레이어의 성능이 가장 좋고 20개보다 깊은 모델의 성능이 안 좋아지기 시작한다고 가정을 해봅시다. 
                        그렇다면 20개 이상의 레이어를 가진 깊은 모델에서 상위 20개의 레이어는 특징 추출을, 그 이후의 레이어는 20개의 레이어에서 나온 특징을 그대로 유지하면서 output을 낸다면 성능이 좋아질 것이라 생각한 것입니다.</span>
                        사실 상위 20개의 레이어에서 이보다 더 좋을 수 없는 특징을 추출했다고 가정하고, 그 이후의 레이어서 성능을 더 끌어올리기 위해 20개의 레이어의 결과를 크게 바꾸지 않는 선에서 학습을 한다면 저자들은 무조건 얕은 모델보다 성능이 더 좋아야할 것이라고 생각했습니다.

                        <br><br>위의 해결 방법 아이디어에서부터 ResNet이 시작됩니다.
                    </p>
                                       



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>ResNet의 원리</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        ResNet의 등장 배경을 살펴보았으니, ResNet의 저자들이 어떻게 해결했는지 살펴보겠습니다.
                        ResNet의 원리는 정말 간단합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림을 봐서 알겠지만, 레이어를 거치기 전 \(x\)를 거친 후의 결과에 더해주는 것이 다입니다.</span>
                        그리고 이렇게 레이어를 거치기 전 \(x\)를 더해주는 과정을 shortcut 혹은 skip connection이라고 부릅니다. 
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_G-CdcOJqpBaW9PGOyarJZMCkrYBVVJ0c1o1FeWSznavHOR-RNPnu9vyRLS-_XGuQjfiDlQHIubw90G-RX2mhlyC6WVJ36OuyipKNAcbu3yua0YPw84QCU9iJTBY1NxL4m67iCET7_o4iUqik5n2K0VvkntR9QN7Bhp9-XPbN4ykmqgbzkXAUyEPFVrsWeBnX20_Wdn98HVZCf46RnoEwvi5PmUb9P9lXv--6wFgvpCIZFEpJ6tyX1JqXWLHVU0IdwYQh6qOI7tH68-nGELcZ7RdE5s_iWLDvSb38Znxi7bv8YS9qi3yxPe2vJF_XMA4qVEI4b0VWNMCtJGPBO-N4i9S8PaeDJDfMUNgO-ky9ZDd4VvPE5BRDUrmoBaaZ6KW4aaBgoP36Q49CVv41j_pSJlQy35lmzvYKYo5DobXDOG-A7P9pp7JlHH65JtqSihJxEOw9SjqKObgPeyAudzxSNt1wUZb4twTROyODJ_tSIJL7WMx-eSmiysfOPybg-3p_QlkRcVJhP31GGPPL3q2reuFq8QyB9IrnoZHHEa8P0teqfAkqVvKLRdxw3hemjwa6Jh1Hju5nQkQLGXM-zj6BOF_WpXE9AICcfSRLlYxfc1YIWmczxx77OzN-GYu0qkubvDkoazvOip9Fso0vS9Z-rqMSnALoWSsA8mN9_tiFXY6WM4qE8lWH-7wod4iDa-nRvGyRonNX62OQ9bmBCG2j5V30FY5CKV2TgERZWOa1Hjvs0Hg4XYTKIAPUdtsP6WcjSsfuxmZEAjRoaLwOj49N1rOvrD2gyo5jd_O-7DexRSCcH7ppsQbWALL9xTYY3U5ZmLYwPSdV21akhTwsz2QKa5XkKi9qawRe89PVDlQdfGDvx3sl3SIhNcJ05PRwTf8BaCuY979o99qOVNTID8LXchfjqfThGMqY5z9ZsgUqkFTfY1EqMQa93oLvV5TD3JG6-A057vLc4LmlJ8BR7JRK2MTxAFwqEFxQsEzLXv8Iif7M1gYy5Q2Zh4iR8-rnMkFiXqgpJLEGKeKxrwoScLIOHawxi-eSz8T0nK6DqO47e8IpEV8-3H1DvTEYcGoLHP4gbu5uNij9feryKUoCixF5zDGwsCdK9zCRrRhz4cEeP4rb2muehb9QFgXTgJFWSWYp53e_bI1hDRQJA68Iyupcl7EPlM3F5uZqq5bl4VajtzYBZdAo0WwIPzcbyKgjieqbpwl8J8RAOugvN85Q0aQMnDphlohs6URKsjcChpp1L29Wv6uBX49DNVrmAiXBEJ5JUnAYOBacc5vWGsOaKJhJw3MUT54PGIfN6QBxCo8kFixITgqi25sN3VoOp1rH1GTD8TShqE8K3KnwLB50M0LTUwdAmx51leX0D476Zaj64JXPgBjMotiCiyzBmuhaNMMvtwKWEqSVP2dxY7d3t5ycdhl9WhkC9CVimyrqiDbP80n-WlQNUXnF2UTSltiAdh8lFRj4_LKXvdh2TrAFqjAKtriz5FLh8OHvAX3Da3mq9fUNT7lgPTbptt0YIphLm6wko0_JntX4pB" style="width: 70%;">
                        <p class="caption">CNN과 ResNet</p>
                    </div>
                    <p>
                        <br>그럼 좀 더 수식적인 측면에서 접근 해보겠습니다.
                        레이어가 있는 모델을 \(f(\cdot)\)이라고 가정하고, 우리가 이미지를 통해 최종적으로 예측해야하는 label을 잘 나타낼 수 있는 mapping의 결과를 \(H(x)\)라고 가정해보겠습니다.
                    </p>
                    <div class="equation">
                        \[model\,=\,f(\cdot)\]
                        \[mapping\,=\,H(x)\]
                    </div>
                    <p>
                        <br>그렇다면 CNN과 ResNet을 나타내는 mapping의 결과는 아래와 같이 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[CNN:\,H(x)\,=\,f(x)\]
                        \[ResNet:\,H(x)\,=\,f(x)+x\]
                    </div>
                    <p>
                        <br>그리고 위의 ResNet 식을 다시 모델 \(f(\cdot)\)에 대해 다시 쓰면 아래와 같이 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[ResNet:\,f(x)\,=\,H(x)-x\]
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">위의 식에서 결국 모델 \(f(\cdot)\)이 의미하는 것은 \(H(x)-x\)인 잔차(Residual)를 의미합니다.
                        즉 모델은 잔차를 학습하는 것이며, 따라서 이름이 ResNet이라고 붙게 된 것입니다.</span>
                        <br><br>여기서 우리는 등장 배경에서 언급했던 기존 깊은 모델들이 가지는 문제점에 대한 해결 방법을 상기해야 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">분명 위에서 얕은 레이어 레벨에서 나온 특징을 깊은 레이어를 통과했을 때도 결과가 크게 변하지 않고 그대로 유지해야한다고 했습니다.
                        이것을 저자들은 identity mapping이라고 부릅니다.</span>
                        즉 identity mapping을 위해서 ResNet의 식은 아래와 같이 해석할 수 있습니다.
                    </p>
                    <div class="equation">
                        \[ResNet:\,H(x)\,=\,f(x)+x\,=x\]
                        \[\therefore f(x)=0\]
                    </div>
                    <p>
                        <br>여기서 아주 중요한 결론이 나옵니다.
                        결국 깊은 모델이 가지던 문제점을 해결하기 위해서는 identity mapping이 필요합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이를 위해서 ResNet \(f(x)\)는 0이 되도록 학습이 진행되고, \(H(x)=x\)가 되도록 학습이 되어야 합니다.</span>

                        그리고 이렇게 잔차를 학습함으로써 얻는 장점은 이렇습니다.
                        <ul>
                            <li>기존 모델들은 처음부터 \(y\)에 잘 mapping 가능한 \(H(x)\)를 다이렉트로 찾아야했지만, <span class="highlight" style="color: rgb(0, 3, 206);">ResNet 같은 경우 \(H(x)\)의 목표인 \(x\)가 더해주는 과정으로써 제공되어 모델이 참조가 가능하기 때문에 학습이 더 수월함. 왜냐하면 x를 기반으로 아주 적은 정보만 학습하면 되기 때문. 이를 pre-conditioning이라고 부름.</span></li>
                            <li>기존 모델에 비해 더해주는 과정만 추가되어 모델의 파라미터 수는 동일하며, 연산량도 증가되지 않음.</li>
                            <li>\(H(x)=f(x)+x\)이므로 이를 미분한 값인 \(H'(x)=f'(x)+1\)이 되며, <span class="highlight" style="color: rgb(0, 3, 206);">최소 gradient가 1이 보장되기 때문에 모델이 깊어졌을 때 문제점의 원이이 되었던 gradient vanishing 현상이 해소됨.</span></li>
                            <li>Gradient가 최소 1이 보장되므로 0으로 수렴할 일이 없어 항상 모든 정보가 통과하며, 지속적인 residual learning이 가능하여 깊게 레이어를 쌓을 수 있음.</li>
                        </ul>
                        <br>그리고 첫 번째 장점에 사족을 더 붙이자면, 실제로 \(H(x)\)의 최적 mapping이 identity mapping이 될 확률은 매우 낮습니다. 
                        하지만 실제 mapping이 identity mapping과 비슷하다면 pre-conditioning을 통해 \(x\)를 보여줌으로써 모델이 이를 통해 작은 변화를 학습하기가 더 쉽습니다.
                        이는 ResNet 등장 배경에서 설명했던 깊은 모델의 문제점 해결 방법 중 하나인, 얕은 레벨에서 나온 특징을 그대로 유지하면서 아주 조금의 변화만 캐치해야한다는 방법과 일맥상 통합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 identity mapping이 최적일 확률은 실제로 낮지만, identity mapping이라고 가정하고 ResNet을 학습한 결과를 보여줌으로써 identity mapping이 합리적이다는 것을 저자들은 증명합니다.</span>
                        이후에 나오겠지만 실제로 identity mapping이라고 가정한 ResNet의 성능이 기존 CNN보다 상당히 뛰어난 것을 확인할 수 있습니다.

                        <br><br><br><span style="font-size: 20px;"><b>ResNet과 다이렉트로 \(x\)가 나오도록 학습하는 방법의 차이는?</b></span>
                        <br>그럼 이쯤에서 의문이 하나 듭니다. 
                        "<span class="highlight" style="color: rgb(0, 3, 206);">어짜피 ResNet은 잔차를 학습함으로써 \(H(x)=x\)가 되도록 하는 것이 목표인데, 기존 모델을 통해 바로 \(x\)가 나오도록 학습하는 것과 다른점이 무엇일까?</span>"라는 의문이 듭니다.
                        실제로 \(H(x)=x\)가 되도록 학습하는 모델은 autoencoder라는 비슷한 모델이 있죠(Autoencoder에 대한 글은 <a onclick="pjaxPage('ManifoldLearning1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a> 참고).
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Gxxg5sllrLjFCCHHFJA4MhF_BG_dy7jsbGTF3GK1XVSgmIWN3X0RuMzmAvHUIqVz8LhjhKPnLEsm0N93USIpnFgRT-inyxWrS0CtWXETzSlTKiV9nBiykGxMZR03BXFjDT7hP2GZsTzFeaPslJjV-nVX3ITzDL3owDzBgBztXFq_br9_uINPD5ZAC1-OSnXthO7YZRtEe9NmjKZSItZWhXcfTmX13s5HzPq6PtzgopWh2mTYzCs0maGS3G-O6OdBu2TRZip3QFCxcfFd1SXO1zi9yJEEG_aJPL8ljnyo-jHwuha9gQuLXYxDhnSwGshWDeNON0D65le_wGthDLmrj0bdhNqtu90QTE_HzApjW2XuSmDUF6JeTjRIZg0sBOVM7Yb2lz4j4N2cHu_Oqacu5xTUOxdsWluQ1I1Z7cQ9hOk6E_AUK3Fe4Qo-YmRY5hEaTv7QscwTbHI9tunZu1pk9vtM30l5yexcBH64resoctJMHKOu8zEcy9DyzhJs1pThvzU7ddbd6JodWbg8rSeygHnXHOYVPeFnF6hh-6wpIyjIlUFCwR85el3mje-erRzbSvuRtA6HSnuPSK4-Nsad86kP2hxleYJ9hXYsNtPHyhn4WNq_ZPIo3Gcwop6mi0_kVrWH0Ci0Kqruv-1cxlZ0nNb9IgcnnOs4lO2LZLf5CEnm_eWDpc-WFnlRviXgLOuwUIh49pkFYGNWyc_C5sICqOUW5EaRlRO9PnUtCks-HdYkG2YHB6z-V9_pcRdtE5S8BWHPHUNgQVcg2mGlpLCYVds9xI4gvQRMa3POwkfDswN4qHxGcwpOEw9KZ4A9fRw_qCQiyiQiyHJhUynHH28MULh74Zhi2GUi0uA1_nxx8n3cmWPOzkltTP1NGLu_K7J-j3Eri9JbN9uNgQOh8txJ6Et8XpWS-xjbgFohYOQ8eCGZYKIy1UnuUaFMfRHpuHoOnRs2c9msDF-XC-KDo35Rtm1u_J8MhMOb1l1vIshA1sWz1dhQ94gVHfLeJWxeiTOGL_7Kv6q3FdXdil1tYC6bZgz9S-7wcIhFC7bn-QRndAKmloY1AbsKTr5HrwuNw5lk83OqjwrVpFLzjX51BdE2JWm4WcID1EGDtif4nH2qlYSoMG5NwLXJvrJZ05hBQ4lUX7S-Q2enCTj-4NJmVNjknAsQoeCuZBvbSvctI_WiOnkrX7WuPyGcgdQYL9KjaGB7iBmiEf-j0kDjHBQJn2sxeN1brl-HAcgMsbiRl80aNVIv3jqejgPaN8TtlSuiqgeeSJgCIWk_kKCgmcIzQy6xe-GzWwgBbj99IFgiP5-t71jRCDvnnL3YMHygjIYL1t-c_4QtPADObRHWRVllhmw8g4_eUx1CWOme8VOGS331WVWvmqmoH8Ip_T-0KcF0f3o6Y2Ki97DgBOfBC-7XV9lxPMWksEyD8tuxxKVPfh7zDmPXbluvE9xr7LyEbWONbaQk1teN0k3R_wbbqgujfcDK_8ChwQkjxFAaIxx69ERJf2MZpr7ImKYeRp_n-hSvzvfqL4ds3dWSuO" style="width: 70%;">
                        <p class="caption">바로 x를 학습하는 것과 ResNet의 차이</p>
                    </div>
                    <p>
                        <br>차이는 간단합니다. 
                        바로 \(x\)를 학습하는 모델은 말 그대로 \(x\)라는 똑같은 값이 나오도록 모델을 학습해야하는 것이고, ResNet은 \(x\)를 더해줌으로써 0이 되도록 학습하는 것에 차이가 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 ResNet은 \(x\)를 다이렉트로 학습하는 것이 아니라, x는 그대로 둔채 미세한 정보만 학습하면 되기 때문에 바로 \(x\)를 학습하는 모델보다 더 간단한 것이죠.</span>
                    </p>

                    

                    

                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>ResNet의 구조</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        위에서는 깊은 모델이 가지는 문제점과 원인, 이를 해결하기 위한 ResNet의 가설을 살펴보았습니다.
                        그리고 이러한 해결 방법을 구현하기 위한 ResNet의 원리 및 이러한 방법을 채택함으로써 얻게 되는 ResNet의 장점을 살펴보았습니다.
                        그럼 이제 ResNet의 좀 더 세세한 구조를 살펴보도록 하겠습니다.
                        
                        <br><br>아래 구조를 살펴보겠습니다.
                        왼쪽은 shortcut이 없는 일반적인 CNN의 구조이고, 오른쪽은 shortcut이 있는 ResNet의 구조입니다.
                        그리고 그림에는 각 레이어의 kernel 크기와 개수(out channel size)가 나타나있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">ResNet의 구조는 특정 규칙이 있는데 아래 그림과 함께 보면 이해가 잘 될 것입니다.</span>
                        그리고 구조를 이해하기 위해서는 CNN의 input, output 크기의 계산 방법과 CNN 연산에 대한 이해가 필요하므로 잘 모르시는 분들은 <a onclick="pjaxPage('CNN1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.
                        <ul>
                            <li>기본적으로 kernel: 3*3, stride: 1, padding: 1을 사용하여 <span class="highlight" style="color: rgb(0, 3, 206);">input과 ouptut의 크기 변화가 없음.</span></li>
                            <li><span class="highlight" style="color: rgb(0, 3, 206);">레이어에 '/2'라고 적혀있는 부분은 down-sampling이 일어난 것이며</span>, 이는 kernel: 3*3, stride: 2, padding: 1를 사용하여 <span class="highlight" style="color: rgb(0, 3, 206);">output 크기를 input에 비해 절반으로 줄임.</span></li>
                            <li>위에서 down-sampling을 할 때 stride가 2이므로 1인 레이어에 비해 time complexity가 절반이 되므로, 이를 같게 맞춰주기 위해서 kernel 개수를 2배로 늘려 ouptut channel이 2배가 되게 함.</li>
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_GNwqGfJVXOUWqzq9Hm61NoJKJH2LJt22TnpbqW5fyI8gwNKt45XpJgWKAuekJQ-JprUgiS3jFzxK9wOEYeuVl2kWw_msY-FLkkgxIFk6c4Eea2IsNuxGOT4a0NByXMgjaj2BUXp6xPFKgXGSxcIvSZMj0ntKvijoebse3mDJYRtnYukPwpRzb_Dq_DUw12vT7HxeSDk9D5cvtXZ4excAEj-_mheqZZCQ7uKn01QtNcKsL-HjtEzRXCvozmk5e9mWpAVkDzyIZ4-hXZ39kQYOSKnnH222_zX1-QsJW8L0pzGFuPQFW0pmnwIO87YOxl2BibtJQl5X3gshBN0YKFv_gpUTPqAJaAvEyj4xb-QiP8cflE2mmDREuZM_0fmb2MSYMYuHVC3Ru_3ZgLHV1fAHuJS9J6tyt10_zm3wfkHZL5F0cPkbbcp65NZQo3Yov3epsNbkDBwi7qj3UDHkzNSr_2XXULXwozVEHDf0XOjvBlDyNxbOuzq6TXwTbdqt3kgHKPfgjSAk0iEBr5HLR3HxLd-3ihAOBcXCQm_Ihgrq_WeoyYdqBqu5eNjAxVJxmt2oEW4EcS4d2abpn0YoZTDzFJPr3roidjpp4P7VRVTmFfTio9YLsfY4hoxYy7VqduJrvCpdF7zIUNTQOwpgVAHA-9lvfmV1N2FrrVKo_lFGnRl0kNaOMjNcez8b-3UekVbtu605ZFih5nU0vlSN_KBjq7i_i8a3cemDd_7OJw58PI126Dy-aXQ7h3ohupKOCW4AZ2H6N5bVDnLNWlTrpveVwH_FFj9rY9LwMyUQUeid5KgfpOxrx6b2WXFRkfIa-K5cN_nCVXKzQErcrQRfWnK0Q93gWsimC9amDYRarDoq04yQqpOzYiv3Lcm_Hd68ktWGLyLDb-rmd7x9ljGq_kNp6w1NtAu2Z4QnojZ4Z7ZhdsXtEdUApHQVI_l18u34mLUGbnAHu53cX21VJYtNatNlK0mdk3U3W79uTmImsubZNmRaPtB5CYmDBwAfcz-HOg2ckKrpzx1LQa5EBPDPI5_3Sv2CLKLhBg67d1-vbGgogxk7LMkdzqIYwY9GvY0y2FE7hdRaVTFOoABtrl2mdnYHg6Pb_Pjqh0epxg-JFDMfN1_cY4-flCDcePU3s8m7rbkuUaZ_3Lia1ilmzoHUJDocjVM2nYoubf4rt_LI_QGtYdD1ZCIqgB7DHLqbg_GQRYsDOrJ70MYrWbMv7YIicqo4nDeY7jUsGJlSgBvUTZ6-nd_Pq96b3fmDiadoHKtMF76UEC1PZ7KIh7bK_aJISXY4CSTOsIh9WLGUHvkDzPpaplGCOqr5V6nrpWUyI_FpYz-9vJZIoCKlRiMMzCdGj8u2w4csPEhyF-ip42CAPucTKABTkbjXGWUKfoxZYu3V0tOSjJ3iesClNcP2YR5b8puUzgS228BeTXUfzuM2z0bAPN8P-IF-He26Jgtnkh4W9UTVRgGlDH3LuaBSrHibcMJCdsgW_DhjjioTw8Ponxj0OKg8sSfVjQHFuNnn-lAnNTNyDJE_3HuC_X" style="width: 40%;">
                        <p class="caption">CNN과 ResNet 구조, 출처: ResNet paper</p>
                    </div>
                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>차원과 크기가 다른 데이터는 서로 어떻게 더하나?</b></span>
                        <br>그럼 여기서 하나 의문이 듭니다.
                        아래 그림에서 보라색 레이어의 실선으로 더해주는 부분은 레이어를 거치기 전 더해주는 값과 레이어를 거친 후 더해주는 크기와 차원(channel)이 서로 같습니다. 
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 실선은 두 값의 크기와 차원이 같은 identity shortcut입니다.</span>
                        <br><br>하지만 점선을 보면 보라색에서 나온 결과를 초록색에서 나온 결과와 더하는데 둘 사이의 크기와 차원이 서로 다릅니다.
                        예를 들어 보라색의 레이어를 거쳐서 나온 데이터의 크기를 (batch * 64 * 28 * 28)이라고 하면, 초록색을 거쳐서 나온 데이터의 크기는 (batch * 128 * 14 * 14)가 됩니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 위에서 ResNet 구조를 설명할 때 두 번째에서 언급했던 down-sampling이 일어난 것이지요.</span>
                        ResNet의 전체적인 구조를 보면 점선으로 그려진 shortcut이 많은데 이 부분이 모두 down-sampling 때문에 야기된 두 데이터간의 크기가 다른 부분입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이러한 문제를 해결하기 위해 저자들은 세 가지 방법을 소개합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_E28qUnnC_JGdZmRtSmz1u5NaPkSJECSTUURrpBLnr8avybxlEfqqdHj8SlH6D5hdDQq7gqbQ_25N3iIFYPIuZRX47TavfCVtIFzLdHQ7lEDEjq72rCe563fLAgZKWWCgBBUowBogRjvvJhZNy0KeyXcZvWWjXH2kBdrMA3RN0KBEoc9WdCc4C_JMinIsE3ap9xrtsClZHbMohL61qhw2hLiZz1Rg6cE3dC3RQK7qFeudStz2Z2JI4hX1b05scjxxdXauUhOD4sQC2upi7s7ghrpY6fPlyl5TxF93uDpHlCpwbx2FtBoyZLtakoROIo4nb1rNfijP3EVKSplQOxv4Cjnn813bVUgIuazy0zHeSrbVAXO9x2pfql7A39JphYw1S1yZDvCVTS-LcXnGKaVmu9ds-aOmVh9D5rGUtot8DVGc_V1RYFf4jtAV_3YtE9AqH9pCpQhXx9nRrzZ2v0w3x_rzVAbnp14Wmnw_Ng7DiVXJjIP9t7M6GjzMJHKQFCfbUMKhYmhNkCw-6WY5Sg_BATNJwgOSqpL6lQi2tcdjDXyyFltlVj7NwI2fXP_OwBDXveNx0_6vbeJ_s40Tf9oZbpdcl-ieGIFExUSTchpAl1Ilopi8Za4nkTUnFjzDyaZtWYefWcbMrwQVOyLfsnaFSR6YmskotofmICLSnod4ZKYKIBdo3eqS4nbHV3k4Y-s01XFcHsx2LsaZkJ7fzYvM91McfCQSbPUM6-R1rKpJyfQ5MrTtiet4TCdL5o9JAZrr9B2DQ0qadZusAH77F8lSpkGfLFv3YFS6cU6P_G5RTsFgHxWdWtaWNpmb2urZJgtsTISW6HxjRe415sQiAUpeSLj10uBHHhDYy-zUZ9jyS7-3fwFpznrdu-9cs-zs8VwmHAGvbiKSi4EOchuie82UT8tuZ7qFCx20VCTuHbCz7tuW7PGX4SyL_qYDRiHchYFtLzQ1uvRJW7t_UZahlfhFFeQfmg88MKTLrMbAcB2L5zV47pCWjLE7QUYJoyiW2LM_53WM1qMug-BMQLK6mpu_w_f0I51BMizgvigl0tSv3VHkgQvRnkiXc4QxgrlahvLmJIFxd1iAcag47QWh8jvhAPjHAG0SoSOQRXEp4rvFJ_wj6gLITayUwueUJIMC231tpJdXCFS7Q-8aCE-0frBBi-RlxMG-aFPunTGHA6yP6k_MWAvJrXLhzt51MTq83rqfUCdMqUX8l_cQzIc-yqiI-RA4OmI1lBLT9Kj3bDk7D4daqByrWZCCBVjj9BNStwGQQNysmfv1C1O0Tu_bCaxp8kLY8InnTLEeaiYcfwhT3B7gagrKSa9agPDyT_xOsVBmGvtUSFknM0MNyp_hFWzbTHGP6-p_-pLQP8YuWzS04pBsrNM3Z3q1ZpaYkjhq4fphMZTcUjlBKSc3b5f8fLU7S3TfgVh_h80BunGG0XNMIDLL2r2SdmskwozvbQAETXoz31Gll4djszxEwSDNUqT_KLhvhsmu5yLM6XUUqf8Tq8N4-87datUOurXisBvUAOxUVqoyMGSw" style="width: 50%;">
                        <p class="caption">크기가 서로 다른 데이터의 덧셈, 출처: ResNet paper</p>
                    </div>
                    <p>
                        <ol>
                            <li><b>Zero padding을 이용.</b></li>
                            위 방법은 2배로 늘어난 channel에 대해서는 zero padding을, 절반으로 줄어든 가로, 세로의 크기에 대해서는 stride 2의 pooling을 이용해서 크기를 맞춰준 후 더해주는 방법입니다.<br><br> 
                            <li><b>크기가 달라지는 점선 shortcut에만 1*1 convolutional layer 사용.</b></li>
                            크기가 같은 실선의 identity shortcut 같은 경우는 그냥 더하면 되고, 크기가 달라지는 점선 shortcut에 대해 stride가 2인 (1*1) convolutional layer를 거쳐 크기를 맞춰주는 방법입니다.<br><br>
                            <li><b>점선, 실선 상관 없이 모든 shortcut에 1*1 convolutional layer 사용.</b></li>
                            모든 shortcut에 대해 크기가 같은 실선 부분은 stride가 1, 크기가 달라지는 점선 부분은 stride 2인 (1*1) convolutional layer를 거쳐 크기를 맞춰주는 방법입니다.
                        </ol>
                        성능은 3 &gt; 2 &gt; 1 순서로 좋게 나왔지만 그 차이가 미비하다고 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 2, 3번 방법의 경우 convolution 연산이 추가되었기 때문에 1번에 비해 parameter 수가 커진다는 단점이 존재합니다.</span>
                        따라서 현재 ResNet이 구현된 PyTorch 등에서의 코드에서는 2번의 방법을 통해 shortcut을 계산합니다.

                        <br><br>지금까지 ResNet의 기본 구조와 shortcut을 계산하는 방법을 알아보았습니다.
                        이러한 방법을 기반으로 레이어를 쌓게 되며, 그 레이어의 깊이에 따라 여러 종류의 ResNet이 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HG4TyWWfKAhkb9jm45ZUwUMrluIrPk0H4rjwrhM-eBt5-D2SNnrTTXbE5rbN2GcaXDfAb9oMrr39CzBqxLU4b2z8GVdnQWKQqZIAVUQm53dM0AEQNnMvNhP0ksswNSKXSypKt7om_d7GlSk00s_qyc_2vdqe7WrTE4w_cCO-NVE5R2iRXP3X-mZMvW2wrerBAJ48PqK1D9rp6jrao1khHP88gQdbUCdtUiyaaDY6DbqTr3rfNmABv9rPU1My5TSwH6M1YjytbWHVdbif1ubeTHRRCH4-ExPeBFP-2xmpD6E7qf0Ij9QdMSe0okwa5m22Ut-m3z7hl0lQwAsOxzkAIQ9-hs7cxA4FbmMQyUeDRkE-NgSfVk29f1MZYo6Bgh689F7Otpnedux1IoS5s3a5Pir_HNX7loGzbSbmnU4kCeSgHW1r-XU_XB_WrIK9r7lF7YbvmkbBHEDX0S7G83K3sjI1DxCubU9QvuwaWZD4O6m3K8-MMEdRxnNvxJ0RiQ5KpGg9untOekUC4ggtBW2QMRdVcV1lq64yo3ipruB0i--ww5sthhw6db3ogmav0d7xCJWiG16P-ARkiuzrJsXIetNy5_rI3IOrhpKc7lGD4WY2Y3PfTmnDCn1W-hZoA5JUzlPY3d62A-Q4BZQSnwuU0LRIP9mXTXMlDVO-gWQlD0aT3J-HgsVMiNbn_WRT8MzfGk7RTaJyfJYm82EUALEsD5-ch7ZQ7RGU8QOfsyc3uy9bDp13G92MEIgoYtIkd2B5QBUxOy5exWA9SGGf7I4lM-l_vbBYxj_z_SNEdt58MdWPTYbNcF36oYqdOQeedglHut2cnplcE84Z-YZcw0QgQdQrnaqYtWNFxSJ4Lkuj19nF-dZCWCHmhskzIEO8S4oCp2v4kXQkAziOVaiZhvczhQz7VWVpi3MSW8nKlIzxM1ppH4RQDxqfT0Q9WEHCBRAZn5ghz_hntuR-M835tkokVnfRh4R0bRuv5NabUt0Yjqgx5aTd-Q4xsUiwpAbEfTdSHShU339h9QkQbhYethqAilVRdZSu1e_J3XTSIP13zp8Y6BPwWTWFY5bqSPgiDncYjyEb-S7Ip96WK7FJ3EbDpT2UfIWMVmP_M9ZEv_HGDRZHvLRu6cijenhIvBPzFGJF5EqFnJA3XWdT7pLO0tZBh8DVskyICZBhHs614mnzYP8Sof-cIbzFfdgAVaOTzxyrnHqL3Q4zaKk4nJTBxfEBEkY-eQMKoKxr_TOgwvZZA6Or71YHKpyN3i2g38zWiDobYl0gIq1knSY6ugenDaVpI21Ir25NZ7nQUq4OxbJRF1YGixNzybUqsdSzzbhQTT2--6UOIz7mpKRODcGn6B36X72L6HSER2B9PQW1FU5mmTPtAlPBAidZ0QaDpV509ugaPfPkqiA1QtE98DQ7Fn191Vv9jMq0RWXQui7kGT36mxDoA7vep8_q0XCx2l8hFgfvQYBU4cfLT54_lnRcXVab3F8-Hunoh43f0dRJPVkwYwZqc0kX-Db74tfj0zuhML2eoocnRm8Xxn" style="width: 100%;">
                        <p class="caption">ResNet의 종류</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>Bottleneck 구조</b></span>
                        <br>여기서 주목해야하는 점이 하나 더 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">위 표에서 18, 34 레이어 깊이의 모델까지는 하나의 residual block에 2개의 레이어가 존재합니다.
                        하지만 50 레이어 깊이의 모델부터는 하나의 residual block에 3개의 레이어가 존재하는 것을 볼 수 있습니다.</span>
                        깊은 모델에 대해서 residual block에 3개의 레이어를 쌓은 이유는 학습할 때 시간과 자원을 줄이기 위함입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">깊은 모델일수록 당연히 학습 시간이 오래걸리기 망정인데, ImageNet 학습 시간을 단축 시키기 위해서 깊은 모델에 대해 이러한 해결 방법을 둔 것입니다.</span>

                        <br><br>아래 그림은 bottleneck 구조와 아닌 구조의 비교 그림입니다.
                        Bottleneck 구조는 residual block을 이루는 모델의 형태가 bottleneck처럼 보여 이름을 이렇게 지은 것입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그런데 레이어가 3개가 되었는데 왜 더 빨라지는 것일까요?
                        그 이유는 레이어가 3개더라도 파라미터 수가 현저히 적기 때문입니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EzuR7X-4k8dIkQH1nY8e_uXShA3DDix07RX9RzuQpObPbxEJvnaKi_ILMsiCLCq6aVFfcOgVtmXH863z52nQ8T_RDaBOkSsNCANPzszCaSRfd7Z1J-PcOeDE8R-4e4j0Ko-Qi9AgeEjUJchOWyaO9ZJCa0u2DELn9NGnT3fLjQICcU3zm3D0DOILO-RuR_H9nsKC1OBaOcpMq8VSZ5vaB0zbJQuBaqc0f3VLl0i4liSNbc7jmlQGX-jSzNTMqMn9VOvnA3Iq0Bh4WquQgTxBeM-QoA_AhAJDct2x7V8Z8CO0QoNno3tkHlSLS2tyU4svlfHEHkSoEsK4zYNam1RSf0mUiqt2WYf1SRvYApYT_mDcSeIq38mucCIf6xhKaOxQ2W7YRnI2jtmfsZhMlLyljXIg1Bm8RQLAtL-FzbjM-eBrVot9uqMIfFE32SG2LAqIzWXZAVe9lyVj9wkeJ4l3JB8JzHz63_bl1WLA6X8hV93zf1H4v3-dgIOT2-PqGr0u2TWkWtj8Zd2j0mdP3TrZrzJecz0CddUIde8J59xhpNGLd5Ddru-hu60fZy9DVM1qmXpGsUeiI1B5ZaG-pvJ-V7F6h1Awf8dWkcjnuds-JufkGC_m4OoDadb-o-sWlAcpc4fLo0xcZWZ2ZyEVfFooxerSd9Ibm5Z6PNGqXAH1ZY0RIPUCmnKkCF4XiCTwbauViNBQjD0tMydZH2ge9HNpCLJ0PmWqK2vhdO7u8MM07Rd50GeiRG50Xj2AdyCDIfJSBI0zcbZn2t3tPaBVAQcLqwarFnP5KPNkO78MJGScfXcdPAaxxTfdC4hV_BElvCBXIc-xSZW0bqYXNh_VMgqe04_gpYpGMJq7v5jGMc3nMhnPxo2go1087oXXFSk6LpjUpjwh7PEACN6DkYsboMOwDQPu6J-_42tFqn5kejypaodQnwUHtSAzRS3NUJzW-LlQ_KHCF0IlRQT53Lv4F5_GRuKqnzOM4g-D5CXq7QwCRgEu_9eppyi1gtnT68Zm_s-7XvtGfG5MV3mHTAgnBvOvsNXaKcNQuHrVfgoKMm0ypC9gBbwBjkR_Rmzc_od_gm0c-5hT_HNMTDTzwI30dOl4j1K5sOE9jrO2IvGJ3PQZ5wC-hA_akAaRrxdo9_1sbSr-FA4W7js4FVTxDm0Uynp_ofsJvHJlY7kIOHVxHLfQJyGeo-_wL0nah-v1Iaou-yo2ZK_a3VhHObSx-Q-XebVNcf-LZrcJ5-8mSK0uZlOFl4m8Gi5jbRLWIvNWfCa0-o0nq7nMEf4E68nl9EXr54c_hl2chLXaxlfwlZAjMSb_TLyLB2rCPWT0XAAoxTJ5Wkp2H2MPtnaJmFA2JyekLxfYqZN8ilZfAw_JVsulR5D8LbBNmSyomPmJi3uDQOc40HXTIf_1N7l6buzFldv1CtE881e3I820cf6TR_LlW-m-cwoPaewQjbovA-vAo0Y9tOaE2TioTk8CQ-dC6e4saD9f_lKpcy1V8WezcbnFKjtS2BT44sGPYQsxZ6VxdO4DuU5qZHOcDnsQ" style="width: 70%;">
                        <p class="caption">좌: 일반적인 구조의 ResNet, 우: Bottleneck 구조의 ResNet, 출처: ResNet paper</p>
                    </div>
                    <p>
                        <br>아래는 각각의 경우에 대해 파라미터 수를 계산한 결과입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">Bottleneck 구조의 파라미터 수가 레이어는 더 많아도 약 22 % 정도 감소된 것을 확인할 수 있습니다.</span>
                    </p>
                    <div class="equation">
                        \[기존\,ResNet: (3*3*64)*2=1152\]
                        \[Bottleneck\,ResNet: (3*3*64) + (1*1*64) + (1*1*256)=896\]
                    </div>
                    <p>
                        <br>추가로 위의 bottleneck 구조의 그림에 input이 256-d로 적혀있습니다.
                        이 부분은 원래 ResNet 종류 표대로라면 64-d가 맞지만 실제로는 shortcut을 더해줄 때 64-d의 데이터를 (1*1) convolutional layer를 거치는 과정에서 kernel 수를 늘려 차원을 늘린 후 더해주기 때문에 저렇게 적혀있는 것입니다.
                        많은 리뷰 글에서 이 부분이 zero-padding을 한 후 더해주었다고 하는데 그 말은 틀린 말이며, convolutional layer를 거치면서 변한 부분입니다.
                    </p>
                    




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>ResNet 결과</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        그럼 실제로 ResNet의 성능이 어떤지 확인해봐야겠죠.
                        아래 결과는 일반 CNN과 ResNet의 ImageNet 분류 결과입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">일반 CNN 구조는 그 깊이가 더 깊어질수록 성능이 저하되는 degradation이 발생하였지만, ResNet같은 경우는 degradation이 발생하지 않고 깊은 모델의 성능이 더 좋은 것을 확인할 수 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_GjKRmTK2giQFP2j_a8vXLWNJyC1gggeqRwP-ilxwAT-CyC6Xav5m2opwowM9iC4eTWpXgQf_a6MBArSCuoSXeT3_PQn_9afZZurSWM_f0GUKLwBiJ7qCwdNeQAgoEEHqCuKzhZNP-tjn46htnIFDDnPGaSWoHMeFVVw-Pj6YLqFnD3QUPWdWl8CltaFLzqGemh1Uy4UxqyaCZK559zgah1WcMz942ULLrK323sHG5xXq7YPoUgmjlsxJ9lKBVkU_yxtUJfh-qlzt-KnNamMIOhSjzF29qCW94W1KB8sLlYwHpD8-e4GcImXBuTbaql3X-vvNohy4ONjcqgpq701fUhVVGpltjWlz0ZBDO06jEFnAX-6yB2fmbidyxjnDu9ANYwJgubAAgNGPztiDZ3mRnr8QA4Ty00DJLoZdefzIoYHQrkdd2dh704dkLkynNqRrrtytMu2ozDMOoZsznlMSw_b4ft_QWTbs0x13mQIb5pcGm7jBj5ovNnZ_TgKsYWc07sr-9NWosMpV5Pckt2Oz1AIMYoy1UxMuDuCpRwhymdwwWpSjjnYzgxk3zc6jx3Xgx5WeKg0Jd1tWaigvUTtmhvBYxdCfeiLr29pWnyiPqHCP1MHralg_xL1p4Bd_Ukd8y24Baf3A9EJdNPy9r9DmmTNUjswIodQHw0L0XgYpYmQBBkf7LZ6D87G4zmc261_d3hvp3yUxGzsZstPH8yufIlRmadXoILe54NO2i1nqesf93myiY8OWdl5ToppQ3gyWTBjXsbYfKqO9eY6jca3ohV_-2kqnOQNOuQ6EmHTPu4LEuUEtCrAw7NMiSVmdh5GEGNp2tiAF9GSqyrHUWLP2kP_m7cD1L4je6_xNUH8UDGFK0Fan35wUZySADojYUQNpf2XqIOR8PjdFKWUMiZnlGpAmyobZU-dhmsSwuUOkCY9LuINYdGMWqNHRb9S_4BSg6jFa5URPIZOrfyH5kwyBSTCQN90kVjJXnkpfLYDQt-hmPpasdabVkvWXTtWHr94CISvEeDcOha8QsVeSt_2CclFMzNxyWxPu1s3gxEUsQorzylvPMeBMqfR4K6n3Ua4BYJnBUMVmo_6-zrjh_cKSzSuVa_iqhHooO0Z9lXsdDQd_OGVgnAQ926WFYGhlYP_WC-XF5cN_0Qs5iI7cbOBa1L9GWEAewb5lPZXaxMEW9rLb-J-p008tu6DAaZxHKMrOzuBhq_BpASpvjoZqTicHsXgIIDcextbEUCbNzwq_DEhxazK6j-G_afQ5l2kV83YvqxMPHZV45WqvRBm-ovvnMQr2hWWaGMeY4MklN7yHGUyvtz7IBlgjci-HMDz3RgOhyMvB4k-Z3wdBYMoMOdj6F5I6FKoTTNTL-WKbQ-oY70SuWEkeOl7I1dp_F5ZarwwP5Vbp6-2Q2xZ-Gwkrpk7eSRdL3IYRu35xBuaNKqfEDLBXxZFYc-kHhwnzrlK09xkIyJlZP3Jj2rX6Ykc56fKDRlDR5_iRJaw3XJwHbUgO7_fKrRO64zdCsGWAMQVPk9zGXn0qYaOxgx" style="width: 100%;">
                        <p class="caption">일반 CNN과 ResNet의 ImageNet 분류 결과, 출처: ResNet paper</p>
                    </div>
                    <p>
                        <br>ResNet은 당시 최초로 사람보다 좋은 성능을 낸 모델이었으며, 지금에 들어서 shortcut 기반으로 모델을 구성하는 것이 기본이 된 만큼 AI 필드에 있어서 많은 영향을 끼친 연구입니다.
                        아래는 ResNet의 논문 링크입니다.
                    </p>
                    <div class="link">
                        <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">ResNet 논문</a>
                    </div>
                    <p>
                        <br><br><br>다음에는 원래의 ResNet이 너무 깊기 때문에 직접 custom ResNet을 구현해보고 CIFAR-10 데이터를 분류해보도록 하겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#CNN&emsp;#ResNet
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="pjaxPage('CNN2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br>CNN을 이용한 MNIST 분류</span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('CNN4.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>ResNet 구현 및 CIFAR-10 분류</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>