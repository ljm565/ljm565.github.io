<!DOCTYPE html>
<html>
    <head>
        <title>Residual Network (ResNet)</title>
        <meta name="description" content="잔차(residual)를 학습하는 CNN 기반 모델 ResNet을 소개합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/CNN3.html" />
        <meta property="og:title" content="Residual Network (ResNet)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="잔차(residual)를 학습하는 CNN 기반 모델 ResNet을 소개합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/AMPSemcIXqyZf5IX4R1Hwo5ySzppiflY4UE-g4apnxmbKRFnUmhH43iHwMnei4GbVPporSPf0v0ShhqGMcXYFrRSu8h1Lhn7YFmWkE40sjqKij0XrQqy39qWIVaojJb6w-5raFCRAtfU3NyMjRDvaRGmSgOhvlMVzp4heqYRsOCcDT5I2365gIISCFthg-SH2Xwwzk-3d32D24MqV1PlBJvamzQC06ybjtXceq2Hu6VWIlO01dY9PO13J3SBr7k-6f4pyo83NTj8fwddLXIofmHudkRQgP_7SdUdqJOto7yJGSD7b5HyBU0XYNPZb2ZV3xB4tnl7x5P16h-y9ySPYeWEqa64fknoZ-DYJ_2HmW7MbVomQ_Z4O9T2Xq1z_laK0q0qEOYoepTDJP5_iJpRrDH_iqTodz3qaCoiUSAEue03eNiiVvzdUIWKbvtQxoy7NOXzvjq32023SogCJM7QPlzXEHdtqaG6aIF3pXYg_ug1b6lYYYUiJsiLkTalPARKpZ7yIvA6TUZ1xp32pRVIfQOj3oh8qokC91E56g-LA-PjbJMB61HsN-qsZnPKSsLia2KOFagvSQVDg9a0NjAQ5PY1P3pw5hhyEbbOE9u91xJRnBPW3TaAjXt2H4psXJLWZL9R-Wci6p5vPd6ObEbhjD7JYkwRtTJc1hJ4RsM5Rg5l1SuDacYnQ0cn6Psxp0PecnWSetdA47akBAaUz2AEjAGg6hR7TmlpyKQdvyGQ9uFotl1krSvt7dyat6cxTELrF-wpHfofsSVxXxesi7w2MV5rRaC9BCnqalyHmy9Wv35pY-ESijmNkzlucrov3T5f10MYLmFbuQ1vsT5GsvKYbuWoXkiZ6n9r_LMg9xGaJ61OF2bBfuAzMJpLNtRQW_lztE6-O4liO7PP-C4nUYTlEgpVdZH46K7UWKjRpbnqb90AD8wOtq94YtA5aWnAqoYqVDR1Ha_X24-QB9jMT4p0ZdiZMZeDfXUrXC1e811YwZa-V0C8c2yv3TQODmDPuS_fXG0UNuy2GtndOfmfPjT2bdrZUJSdyS-DzYo6oDII-4Wz2ahsetyB_9lu5JBobi6zsyrDtHGne3GBceQ4RuGsCWThf7vsdpMpmHxXVHcJ1LyGnBAHy5yVzgdkhmoqEsNxLRwoOPle9dMo0JFge-KNgeEGWFaLYFaDnrx_Og9lsCY3fSp8r7ksxRfNvJoPUgkr0K1_z9PlG7KuNZS-W9TPlQ2tD4OnM95G3jn4qaaeLn8hiWuI3Iif4egJBDahvP1X7080b8juqap3YvgDL8my7mYUsBzn2XyS4DN0mJfBWzGlydDCf7s1tjPxoWBBIIgjzs7wQ1lmoMfSSZ3rmVZafXeFwH8VmMycCoQL5JV0gPfNL_lDq567C2HLxkn57hzekTZ7ZZPvNdch7SWrpXZQJLncZMYrjVtifq7_l6rkN-6AoQ5e1KaMu3gsRTTfTyDW162mhCuBljC5hTXS71UA6jVIne0" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Convolutional Neural Network (CNN) &amp; Residual Network (ResNet) / 3. Residual Network (ResNet)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/AMPSemcIXqyZf5IX4R1Hwo5ySzppiflY4UE-g4apnxmbKRFnUmhH43iHwMnei4GbVPporSPf0v0ShhqGMcXYFrRSu8h1Lhn7YFmWkE40sjqKij0XrQqy39qWIVaojJb6w-5raFCRAtfU3NyMjRDvaRGmSgOhvlMVzp4heqYRsOCcDT5I2365gIISCFthg-SH2Xwwzk-3d32D24MqV1PlBJvamzQC06ybjtXceq2Hu6VWIlO01dY9PO13J3SBr7k-6f4pyo83NTj8fwddLXIofmHudkRQgP_7SdUdqJOto7yJGSD7b5HyBU0XYNPZb2ZV3xB4tnl7x5P16h-y9ySPYeWEqa64fknoZ-DYJ_2HmW7MbVomQ_Z4O9T2Xq1z_laK0q0qEOYoepTDJP5_iJpRrDH_iqTodz3qaCoiUSAEue03eNiiVvzdUIWKbvtQxoy7NOXzvjq32023SogCJM7QPlzXEHdtqaG6aIF3pXYg_ug1b6lYYYUiJsiLkTalPARKpZ7yIvA6TUZ1xp32pRVIfQOj3oh8qokC91E56g-LA-PjbJMB61HsN-qsZnPKSsLia2KOFagvSQVDg9a0NjAQ5PY1P3pw5hhyEbbOE9u91xJRnBPW3TaAjXt2H4psXJLWZL9R-Wci6p5vPd6ObEbhjD7JYkwRtTJc1hJ4RsM5Rg5l1SuDacYnQ0cn6Psxp0PecnWSetdA47akBAaUz2AEjAGg6hR7TmlpyKQdvyGQ9uFotl1krSvt7dyat6cxTELrF-wpHfofsSVxXxesi7w2MV5rRaC9BCnqalyHmy9Wv35pY-ESijmNkzlucrov3T5f10MYLmFbuQ1vsT5GsvKYbuWoXkiZ6n9r_LMg9xGaJ61OF2bBfuAzMJpLNtRQW_lztE6-O4liO7PP-C4nUYTlEgpVdZH46K7UWKjRpbnqb90AD8wOtq94YtA5aWnAqoYqVDR1Ha_X24-QB9jMT4p0ZdiZMZeDfXUrXC1e811YwZa-V0C8c2yv3TQODmDPuS_fXG0UNuy2GtndOfmfPjT2bdrZUJSdyS-DzYo6oDII-4Wz2ahsetyB_9lu5JBobi6zsyrDtHGne3GBceQ4RuGsCWThf7vsdpMpmHxXVHcJ1LyGnBAHy5yVzgdkhmoqEsNxLRwoOPle9dMo0JFge-KNgeEGWFaLYFaDnrx_Og9lsCY3fSp8r7ksxRfNvJoPUgkr0K1_z9PlG7KuNZS-W9TPlQ2tD4OnM95G3jn4qaaeLn8hiWuI3Iif4egJBDahvP1X7080b8juqap3YvgDL8my7mYUsBzn2XyS4DN0mJfBWzGlydDCf7s1tjPxoWBBIIgjzs7wQ1lmoMfSSZ3rmVZafXeFwH8VmMycCoQL5JV0gPfNL_lDq567C2HLxkn57hzekTZ7ZZPvNdch7SWrpXZQJLncZMYrjVtifq7_l6rkN-6AoQ5e1KaMu3gsRTTfTyDW162mhCuBljC5hTXS71UA6jVIne0);">
                    <div>
                        <span class="mainTitle">Residual Network (ResNet)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2022.07.28</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이전글에서는 convolutional neural network (CNN)에 대해 설명하였습니다.
                        이번글에서는 CNN 모델을 기반으로 하는 residual network (ResNet)에 대해 알아보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">ResNet은 당시 CNN 기반 학습에 있어서 큰 혁명을 가져온 모델입니다.
                        그 당시 최초로 사람보다 더 좋은 결과를 낳은 모델이기도 했습니다.</span>
                        <span class="highlight" style="color: rgb(0, 3, 206);">이 글에서는 ResNet의 원리, 목표 등 ResNet의 전반에 대해 살펴보겠습니다.</span>
                        그리고 CNN에 대한 설명은 <a onclick="pjaxPage('CNN1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.

                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>ResNet의 등장 배경</li>
                            <li>ResNet의 원리</li>
                            <li>ResNet의 구조</li>
                            <li>ResNet 결과</li>
                        </ol>
                    </p>



                    <h1 class="subHead">Residual Network (ResNet)</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>ResNet의 등장 배경</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        컴퓨터 비전과 이미지 처리에 있어서 convolutional neural network (CNN)는 필수적인 모델입니다.
                        꽤 예전에 transformer가 등장하고 나서 요즘 이미지도 transformer로 처리하는 vision transformer (ViT)가 많이 사용되긴 하지만 여전히 CNN의 입지는 강건합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그중 residual network (ResNet)는 한창 CNN으로 ImageNet 대회를 할 때 신선한 충격을 안겨준 그런 모델이었습니다.</span>
                        현재 글 작성 시점에서 ResNet 논문의 인용수는 무려 10만회가 넘어가며, 계속해서 증가하고 있는 추세입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 ResNet의 영향 때문에 transformer등 다양한 모델에서 residual connection을 기본으로 사용하고 있기도 합니다.</span>
                        
                        <br><br>먼저 ResNet의 등장배경부터 살펴보겠습니다.
                        당시 한창 ImageNet 대회를 통해 top-5 error가 어떻니부터 SOTA (State of the Art) 모델이 어쩌구할 때 등장한 모델이 바로 ResNet입니다.
                        ResNet 등장 이전에 다양한 ImageNet 모델이 있었습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그 대표적인 예가 VGGNet이 었습니다. 이때부터 이미지 분류를 위해 아주 깊은 모델이 만들어지고 있었죠.
                        그리고 대부분의 경향이 깊은 모델일수록 그 성능은 얕은 모델보다 뛰어났습니다. 어찌 생각해보면 깊은 모델일수록 feature representation을 잘 할 것이니 당연한 결과이기도 했습니다.
                        하지만 모델이 너무 깊어져서 10개 이상의 layer, 즉 수십개의 레이어를 가진 모델의 성능이 오히려 낮아지는 현상이 발생하였습니다. 이 현상이 바로 degradation 현상입니다.</span>
                        
                        <br><br>여러 연구자들은 위에 대한 문제의 원인을 처음에 아래와 같이 생각했습니다.
                        <ul>
                            <li><b>레이어가 너무 깊어져서 training set에 overfitting이 되어 성능이 안 좋아진 것일까?</b></li>
                        </ul>
                        <span class="highlight" style="color: rgb(0, 3, 206);">모델이 깊어져서 overfitting이 일어났다면, 깊은 모델의 training loss가 얕은 모델에서 더 낮아야할 것입니다.
                        하지만 아래 그림을 보면 알겠지만 test loss에서도 물론이고, training loss에서까지 얕은 모델보다 깊은 모델이 성능이 안 좋은 것을 볼 수 있습니다.</span>
                        <br><br>따라서 연구자들은 아래와같은 결론을 내립니다.
                        <ul>
                            <li><b>모델이 깊어질수록 성능이 안 좋아지는 이유가 overfitting 때문이 아니라, 모델 최적화(optimization)가 어려웠기 때문이다.</b></li>
                            <li><b>모델이 깊어질수록 gradient vanishing, gradient exploding 현상이 발생하기 때문이다.</b></li>
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemfSYuLWter94XAqNS6IcRMVJAarh6kAVvx9L_0_hYHlr-9K56vgVpjzkOZjhMjZIV067w-C9xvGLcgY4DSI-_1RVhZ6BxP0cIgszjqhBlD2ixzmH_kk4wE_CA0fXi4KHlI6pNwSf-KG1gMCoQT0JZlRHewOu19n7L_aaSfWBoAIfcuPFZV6H36UbOhsHBc8OtfOgzoqzctro4a1GPGnwrz_N5AtSnPVNKQT-eTVqq0qMkm8K8g6fyeIBV49YVXl7LvuCgF5nLGu_568VzdFzNLiZYitPXnsoZf3FGFSA-1t6yevQEkFlc3FtNSNX0pkB4jgu4XeCytipKJwfBlj31d65baLMx4q2v_r0aCvTktrXLrUFI0oDFP-jckBn0U5UrO2Pkr5MJYIF4Dy3A0CTQNBAi4iQMAOqbvmOD2YJPe6GcQ3DfSQkoVMpzS9VTU3wp0b89Ch_2fvmfQ3P-2bTLdyqQgwaPhHoo2qXWqHFzqiqtP9y7dHmpAzc8VSZo7st2eD6crkyiPbHjDc_Yne6ys83i0gU1PeQidb5ztChH3KtdVyoXbD0wluCIWy6WMK6l_H-ux0GoB8EiRv7Z-faOLkzPautnT6mdVxxAmALlV3TfcRfwM7QTD0o3W_93osbWCftiRR_8hhQTGN186_V36MPRW661Vlg8n0pK3yR6WOylrp0s5vyYjyEVy2LBpq4fkFTW6V_PzH57Y0PPLdfNdmBpfsl7CiFq-_y8zdp1TlSqJ56TasaPJyz20Oviheoi-2gE_3IaleaObDErDANNqh8dQv1GAC6DMuSdQ1XTZAdw2u39-v60QDe37FzGhITSaNgIehP_3uWjLnBoAY1kQ1VuEcQ-RhliP2ANom8_QRMyGJvD5lKn33u_ABVS5ytpWc6kkZTzcLU770fnS2juUpe2wI7LHL_Hs2f_zytmBRJ3S3s25afm04uyD5PitzSAq2mqov1uffB_tF8rQm1lXTHq_1lMqkpiKdyR6Jk1YDUTF5-_8lGMxo5gfti8FWVkz3gbECjR7zFlQ75QHYm3ch8pj1VHk5720YWJxVVDG0ouY03zopWksvLPv9l2SBJxO4qeOtPhOsP2XgAofFrODkoiCOP7ji2DXB-7Iz38vO1td45f7j3Q9FMB948z6b9-rb1eY4H1IJ0Mm39Oy053Q1gMWwez-GRR1wbrdwx70ZlV3aDOeZNynbAf6CN4TWgVJ1OZiArJuoQc0XBqLdpO5nr_QAlSfS67kwStHrwFV3OMyCWUVZtsVKSsbBxrzdjv00h2lEMNj9ZDvivrjCf7YJKwOvpF9KNZ5GoIndf52PL7ku9pbO-CgOCDaWi12JG8ws-PQtb674qQqoxZOI1aF2cGtRBSbKmIdkRNRK2Wpu9NO6PJhnAokQqfYQu-RWFzeprcQWShV9Snvi2slWsgH5mATksK8jc2gwS5xSz2_TPNk_PvxVmjL1McPHaNLU2rec0dxil6uMlT_tjAJFL2I" style="width: 100%;">
                        <p class="caption">깊은 모델과 얕은 모델의 성능, 출처: ResNet paper</p>
                    </div>
                    <p>
                        <br>그럼 위에서 언급한 두 가지의 <span class="highlight" style="color: rgb(0, 3, 206);">문제점을 해결</span>할 수 있는 방법은 무엇이 있을까요?
                        바로 아래와 같은 간단한 방법이 있을 수 있습니다.
                        <ul>
                            <li><b>성능이 가장 좋을 때 까지의 얕은 레이어는 이미지에서 특징을 추출하고, 그 이후에 더 깊은 레이어는 얕은 레이어에서 추출된 특징을 그대로 유지하면서 output을 내는 방법.</b></li>
                            <li><b>얕은 레벨에 있는 레이어는 이미지에서 좋은 특징 추출이 가능해야함.</b></li>
                        </ul>
                        <span class="highlight" style="color: rgb(0, 3, 206);">좀 더 간단하게 20개의 레이어의 성능이 가장 좋고 20개보다 깊은 모델의 성능이 안 좋아지기 시작한다고 가정을 해봅시다. 
                        그렇다면 20개 이상의 레이어를 가진 깊은 모델에서 상위 20개의 레이어는 특징 추출을, 그 이후의 레이어는 20개의 레이어에서 나온 특징을 그대로 유지하면서 output을 낸다면 성능이 좋아질 것이라 생각한 것입니다.</span>
                        사실 상위 20개의 레이어에서 이보다 더 좋을 수 없는 특징을 추출했다고 가정하고, 그 이후의 레이어서 성능을 더 끌어올리기 위해 20개의 레이어의 결과를 크게 바꾸지 않는 선에서 학습을 한다면 저자들은 무조건 얕은 모델보다 성능이 더 좋아야할 것이라고 생각했습니다.

                        <br><br>위의 해결 방법 아이디어에서부터 ResNet이 시작됩니다.
                    </p>
                                       



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>ResNet의 원리</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        ResNet의 등장 배경을 살펴보았으니, ResNet의 저자들이 어떻게 해결했는지 살펴보겠습니다.
                        ResNet의 원리는 정말 간단합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림을 봐서 알겠지만, 레이어를 거치기 전 \(x\)를 거친 후의 결과에 더해주는 것이 다입니다.</span>
                        그리고 이렇게 레이어를 거치기 전 \(x\)를 더해주는 과정을 shortcut 혹은 skip connection이라고 부릅니다. 
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemct0Qs24hWjP7GIEKVfUKSMU9SKwRpUtgBDYG-tMzwWPogoCNqVXhxEdMZwnIR_M_ejqCddLMHmQyoGrLsGX7-jUfx0ckHe-nCOUqDZSvT2ynK06tQ65PsMLHLAQt5sXc1KRex6ApZfJcCxWmH6_ccgii6JWZXs_ba2l10gR3ZGECg9DSYY373i5LQVkCiJMh5zHalhnmW-dU4Q9UH3waCu6lOO55sGCXkgIsLgu2M3hJeasxIv6W0b9gmwpDZa_dNtIr4gEW45FDwyXoarOr-Yx9feRwiloejhXymMQS3JePaODYbQsZhs5fta0wErbVJJsrLisQovzIbbt8hd_lgXE90PdHCedcT5ZfcEzZXHDJ8cWxfujcrJpNSPYnvxGOFFmDCERpCbqGp5rO9T6_kYt9a7nH_bPAIdBW0IOPQ-PnBDrfV8KLiBeh_ZiWsw-eSCEkdl8QjxkhKWbLBIPb8oEjTlUWFCcwtNb30QG-jq9NZVfnOwmHpCjXcL87U0y5NE94-C2xeTi-5MRgPbMRWAYN35pNKhhwLiCMsyay1mnrcDiN1YUihZbtKSwGpDWMosp7aRNdGptMjrr9idevKkm1eYPUyJGsnXh7chbHxslr5aw0n1DqNhN56l2B37bjDzrZEDsF2lR0zMWhOZAGx-GXzUj9apvCA_Soj5Fs-836XNsgR34rXCpvifiT1iPSbB4LCSk5JlFZL-FeAYE8GJ0YLEG-ehwQ2TgRnWVfJrvxkg6CNmgruE1UVHxYVvaVumEPllJwlH5sPVeSlaDnCkFpS0euXIs19U8NEEjwRJi23Wm0HRWOolzJITgJT9Ros77uuyHEPXeLShpWNRKTDQpaRsxGTltws0Mx1DOFuu_sSY6Ytu7kN0deRq3J5UEv1NaYuaX99RqdVgd4mdVLb0vh_U39NQp_zW6dkWt8yiGGdsuWSttc46D8p_g4NIfCdI2J68-6gSzMdde3xRWbsYz-D5EWmhseR75ZPN5OyzSmicmyOG4gUK2YjYKR8J33U2GqIWzgonxMneZHwsEFZzgckx55-pdZ4GdUGbsWXXlC0bnQUPXQvgJHc7_jVbogXrdRN9rEK9gdfCm8_HdSbmogWyAJ-wfwmIwEfOZBwRaYTfeBXtdeKNwb_VcHy4Oylnqk12wbR2qyfBOWQKh24cCrtUxJd4ZwFzCnb01LRwMFEOB6ATDSbWrQfOViSz1fRLyTZ1JQCDvXR1y5LOBGKpPTgIPrIdmblnZzRZD76ESLLp1dN8oO_MJzfYTsHYnH-vCDVnkUR9CxnHWTMf8i9ekakAuRI77l3TWB882pagveW10W5JYLAvYQzf9stH2QkuR5p3QWA3priuznvPk1FJeybbFgeadbf_Pp8RLF3puBepkBy0WZabZBWuD-5tcpHr6iOYI3qpXZsxtIheSauo_2wSWIIvNU33Mh49rrT6NhcE1BiwE44P9EMdyaoyRQpG2iUfcypl33dpOt4Iu3I" style="width: 70%;">
                        <p class="caption">CNN과 ResNet</p>
                    </div>
                    <p>
                        <br>그럼 좀 더 수식적인 측면에서 접근 해보겠습니다.
                        레이어가 있는 모델을 \(f(\cdot)\)이라고 가정하고, 우리가 이미지를 통해 최종적으로 예측해야하는 label을 잘 나타낼 수 있는 mapping의 결과를 \(H(x)\)라고 가정해보겠습니다.
                    </p>
                    <div class="equation">
                        \[model\,=\,f(\cdot)\]
                        \[mapping\,=\,H(x)\]
                    </div>
                    <p>
                        <br>그렇다면 CNN과 ResNet을 나타내는 mapping의 결과는 아래와 같이 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[CNN:\,H(x)\,=\,f(x)\]
                        \[ResNet:\,H(x)\,=\,f(x)+x\]
                    </div>
                    <p>
                        <br>그리고 위의 ResNet 식을 다시 모델 \(f(\cdot)\)에 대해 다시 쓰면 아래와 같이 쓸 수 있습니다.
                    </p>
                    <div class="equation">
                        \[ResNet:\,f(x)\,=\,H(x)-x\]
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">위의 식에서 결국 모델 \(f(\cdot)\)이 의미하는 것은 \(H(x)-x\)인 잔차(Residual)를 의미합니다.
                        즉 모델은 잔차를 학습하는 것이며, 따라서 이름이 ResNet이라고 붙게 된 것입니다.</span>
                        <br><br>여기서 우리는 등장 배경에서 언급했던 기존 깊은 모델들이 가지는 문제점에 대한 해결 방법을 상기해야합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">분명 위에서 얕은 레이어 레벨에서 나온 특징을 깊은 레이어를 통과했을 때도 결과가 크게 변하지 않고 그대로 유지해야한다고 했습니다.
                        이것을 저자들은 identity mapping이라고 부릅니다.</span>
                        즉 identity mapping을 위해서 ResNet의 식은 아래와 같이 해석할 수 있습니다.
                    </p>
                    <div class="equation">
                        \[ResNet:\,H(x)\,=\,f(x)+x\,=x\]
                        \[\therefore f(x)=0\]
                    </div>
                    <p>
                        <br>여기서 아주 중요한 결론이 나옵니다.
                        결국 깊은 모델이 가지던 문제점을 해결하기 위해서는 identity mapping이 필요합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이를 위해서 ResNet \(f(x)\)는 0이 되도록 학습이 진행되고, \(H(x)=x\)가 되도록 학습이 되어야합니다.</span>

                        그리고 이렇게 잔차를 학습함으로써 얻는 장점은 이렇습니다.
                        <ul>
                            <li>기존 모델들은 처음부터 \(y\)에 잘 mapping 가능한 \(H(x)\)를 다이렉트로 찾아야했지만, <span class="highlight" style="color: rgb(0, 3, 206);">ResNet 같은 경우 \(H(x)\)의 목표인 \(x\)가 더해주는 과정으로써 제공되어 모델이 참조가 가능하기 때문에 학습이 더 수월함. 왜냐하면 x를 기반으로 아주 적은 정보만 학습하면 되기 때문. 이를 pre-conditioning이라고 부름.</span></li>
                            <li>기존 모델에 비해 더해주는 과정만 추가되어 모델의 파라미터 수는 동일하며, 연산량도 증가되지 않음.</li>
                            <li>\(H(x)=f(x)+x\)이므로 이를 미분한 값인 \(H'(x)=f'(x)+1\)이 되며, <span class="highlight" style="color: rgb(0, 3, 206);">최소 gradient가 1이 보장되기 때문에 모델이 깊어졌을 때 문제점의 원이이 되었던 gradient vanishing 현상이 해소됨.</span></li>
                            <li>Gradient가 최소 1이 보장되므로 0으로 수렴할 일이 없어 항상 모든 정보가 통과하며, 지속적인 residual learning이 가능하여 깊게 레이어를 쌓을 수 있음.</li>
                        </ul>
                        <br>그리고 첫 번째 장점에 사족을 더 붙이자면, 실제로 \(H(x)\)의 최적 mapping이 identity mapping이 될 확률은 매우 낮습니다. 
                        하지만 실제 mapping이 identity mapping과 비슷하다면 pre-conditioning을 통해 \(x\)를 보여줌으로써 모델이 이를 통해 작은 변화를 학습하기가 더 쉽습니다.
                        이는 ResNet 등장 배경에서 설명했던 깊은 모델의 문제점 해결 방법 중 하나인, 얕은 레벨에서 나온 특징을 그대로 유지하면서 아주 조금의 변화만 캐치해야한다는 방법과 일맥상 통합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 identity mapping이 최적일 확률은 실제로 낮지만, identity mapping이라고 가정하고 ResNet을 학습한 결과를 보여줌으로써 identity mapping이 합리적이다는 것을 저자들은 증명합니다.</span>
                        이후에 나오겠지만 실제로 identity mapping이라고 가정한 ResNet의 성능이 기존 CNN보다 상당히 뛰어난 것을 확인할 수 있습니다.

                        <br><br><br><span style="font-size: 20px;"><b>ResNet과 다이렉트로 \(x\)가 나오도록 학습하는 방법의 차이는?</b></span>
                        <br>그럼 이쯤에서 의문이 하나 듭니다. 
                        "<span class="highlight" style="color: rgb(0, 3, 206);">어짜피 ResNet은 잔차를 학습함으로써 \(H(x)=x\)가 되도록 하는 것이 목표인데, 기존 모델을 통해 바로 \(x\)가 나오도록 학습하는 것과 다른점이 무엇일까?</span>"라는 의문이 듭니다.
                        실제로 \(H(x)=x\)가 되도록 학습하는 모델은 autoencoder라는 비슷한 모델이 있죠(Autoencoder에 대한 글은 <a onclick="pjaxPage('ManifoldLearning1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a> 참고).
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemc8IXqG6P5NHthSvX8wijwVSIm-j_NGlZpgSVKnogORlr-xw1ycTxIq9tbEMkqCpAicWtz9N8phmsbXsh5fUTN_c0cY6AkbUulqA8GDhLoumLZcl1wYOivslymcXDnTqFZKdj6KnyGwrjDSR6lLNMFETqFkm0cicwv-qmS8mkms7Cml1tAFFUPF2ayjZ-2GkENCGm5gjqGdw8hjWp7Z3xAdCCWLRq4qqs70rn04JZva_Cip4SvwJTl24NA-l2RoWl_IaDcK2nRZo13qpqQdB2MWecrnTiDLa7mrQiJ-Ad9SfWK2TitRFIQVeKlUunJuABY8-bxW_vsSkQooHvbIaWFqqCbq2IAOFHQv-WaqgZXSyBj4kF34S6xNhR8TAZH52z3wAvrs1fFQHVBQEorIp84IUT7W2Bd5RZNfyGdYOzpoQHQ0QGISPAXPdBuGgGUl7y48wmxzS4YdvSGJhQwewb9aYS-NC8w-bOIrYmBbgs0pJrpogL36eaHxi4EsuaoG8Y3VNqIPLjwMc16iA0Aupu1kPHSyJftqXD5kGyNI623y8Sfn3gw8avggv2Pcf-OvPhOnj3L4r470iJnT-kj6tdyxRUjAA7cJIHyoMRuHfhoyekYfIbeHC0LVEoJ5XxzGiXkjJWUlYTIlx1Kbio07Jwu2LI3RnrXOXZFyc8eGYkdY9TR9yj0xUuks_vyQVv3UnW7UkPP5nGbPzgFIOdhcxbINX9qBwtJxvLcPsZybjEQyZ5pqZMiorlito9R5G3s9un4EuaO-Ysmzujn3czsiPdeqtWYtFYFnUwzDKdQlMvd4MhVP4zM1aLT5yo-C06-nV85h8cg0yrhhSgLoyyPS5OHLK1MExR7UjZPCpBwmL2SehEtwr-CBkFABU466Ixfkxv1kIoM3eY-rw4pjcrD1GCf7DsUwcZL_0j1AhOzhZNDvN8zuUgWlbM6AGn-4uJmc1ubmUEtIKU5Tsoo7uzEpHconpZtgG7I76_IHwiFTIpddnx6O2FfznuSeKRHJ8KLyglaykcbX5mX1grDjPH48IL8N4fDQyLBr5JbjeAtmgM3zZEMwYVUkUFt7M-P_n5kY8b6Bvv4jqmBxAf_sv3LBRYCIG314EVQfFuUjrZ1f2ubPPDIVRX0Sfv38I9tajwX1b0FdSNWAw0QCqpsYn_-aeFjxgUTN7Oiu1dt76HU8f-bPjhEcwYR6l0ffQVaabM3YuAVucfwSNkeBtcJu2bG98MKW04eb7iI2CfHWKBs1W9WttlcFgnne27KBm9JYalSXEkEOoFMdx_YnIWwzAfDMOXJ8J67o0TA9SN654H5IzUUxq964zo0x08VFDHR4qhu_B-UnkT-wwqqbMWKEObyGjnj0YZnV6zAog3rDUZk_LmqvdAZRTjMxHk8ceCXdUbM5PtVowd9DTF7mhpBliterHUEdSlqt7K6TJDfmrFrQn2osrc09MOEs6CeRkivp-NUq2Hw2Jh1fOGBvhEJgceKpaRo" style="width: 70%;">
                        <p class="caption">바로 x를 학습하는 것과 ResNet의 차이</p>
                    </div>
                    <p>
                        <br>차이는 간단합니다. 
                        바로 \(x\)를 학습하는 모델은 말 그대로 \(x\)라는 똑같은 값이 나오도록 모델을 학습해야하는 것이고, ResNet은 \(x\)를 더해줌으로써 0이 되도록 학습하는 것에 차이가 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 ResNet은 \(x\)를 다이렉트로 학습하는 것이 아니라, x는 그대로 둔채 미세한 정보만 학습하면 되기 때문에 바로 \(x\)를 학습하는 모델보다 더 간단한 것이죠.</span>
                    </p>

                    

                    

                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>ResNet의 구조</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        위에서는 깊은 모델이 가지는 문제점과 원인, 이를 해결하기 위한 ResNet의 가설을 살펴보았습니다.
                        그리고 이러한 해결 방법을 구현하기 위한 ResNet의 원리 및 이러한 방법을 채택함으로써 얻게 되는 ResNet의 장점을 살펴보았습니다.
                        그럼 이제 ResNet의 좀 더 세세한 구조를 살펴보도록 하겠습니다.
                        
                        <br><br>아래 구조를 살펴보겠습니다.
                        왼쪽은 shortcut이 없는 일반적인 CNN의 구조이고, 오른쪽은 shortcut이 있는 ResNet의 구조입니다.
                        그리고 그림에는 각 레이어의 kernel 크기와 개수(out channel size)가 나타나있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">ResNet의 구조는 특정 규칙이 있는데 아래 그림과 함께 보면 이해가 잘 될 것입니다.</span>
                        그리고 구조를 이해하기 위해서는 CNN의 input, output 크기의 계산 방법과 CNN 연산에 대한 이해가 필요하므로 잘 모르시는 분들은 <a onclick="pjaxPage('CNN1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.
                        <ul>
                            <li>기본적으로 kernel: 3*3, stride: 1, padding: 1을 사용하여 <span class="highlight" style="color: rgb(0, 3, 206);">input과 ouptut의 크기 변화가 없음.</span></li>
                            <li><span class="highlight" style="color: rgb(0, 3, 206);">레이어에 '/2'라고 적혀있는 부분은 down-sampling이 일어난 것이며</span>, 이는 kernel: 3*3, stride: 2, padding: 1를 사용하여 <span class="highlight" style="color: rgb(0, 3, 206);">output 크기를 input에 비해 절반으로 줄임.</span></li>
                            <li>위에서 down-sampling을 할 때 stride가 2이므로 1인 레이어에 비해 time complexity가 절반이 되므로, 이를 같게 맞춰주기 위해서 kernel 개수를 2배로 늘려 ouptut channel이 2배가 되게 함.</li>
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemfhz7qP-f740StuVy3I4mIUFk38D02ROnA4zYjUFzTFFUt0XDF1qWbRUqJNe6Ofh91_BG9XsRQRA8HoxTDdiiW8JmIzl5pystZBFzceGC_3GlCC6hwU-rNgkIcRyIN9aKA4J7S5PIuDHdmo9pseMXVkoJc6jSkQUe6xct_K94T7SOmObg6xnU3g44kH1LLqkX2jIfeGU9FT85dC5RV4KUhO8foELORb20RhcTmPxMpopsJQ1g_S_Cp5s1UP6URvQRMW5e9l0rb2LwG917zxnwRelplYA7k2kNCStBRuN6xNIIhNLChw7RQraXGnN-dbHCQqZD3xONOexU9MFpa75gy-NzArqk8iu98kplcGvFEFp9aAkk266jwlo_-Ym01b5eYj4dSqnHHPiYmQoEsjvNY7riWfA90W86P6EulczZPunEtabfqQFPN9b61sVlEDM0UAlYNdGTw1HcHyePw3-ndJcAdDsSTInRoMRMHm8knujajjYdPQNiiNE5ZoZBlRs_pp3Dz5aJML_89UGcmkqAwWpl-mtIytQs3eqbRSGuigEhab-52BIJ0GQdyS0kUApD5d8-N2b5n-tHhKLDXHVjX8S153sYjudlTDlgNe6PfN85iSGWhfbgv24KfOLy82CIIJ4oitGY3SUb0U3pnBxKarQlBKgyCTLkvVAReAphUeqbzCTP2hLyo47eNF-eqQh2p6hNixf5F4uzMnk4PaXAIPD8af7nlJ3MBLsoNSKFzbYSr9I4tEw2wskcmDlp39ugLGVCNsNX1WKxDF72wx-W2OSjAndSyWbRB2-Hn5i9TjNnK-cm9-FGm6Pr81AWqeLOo9YzrGkFvx_uLab1QEwr1iZKA6y-0lLTScIKZa7_7obgj4LJWUaTiPnRQicZzhpDQFzB_elCsSmtgtZrWkAK7YyC5si9rKvaILOKYBH9NjlWpVJWql7UlQDN8D9CmtccJLKtEhXzzAdIsexzAf_AyEeKWicHT-thot4IQfgNKX9HVEKBzNBFZJizYi7KFxFDn7wBAZWodB9jDsMNbc2SwRgWo2wkn1l57AFRUtZJzCaLJKoYJxVK2SMFJalWkHLLJKYi_YjEbiZMT1XEaPdHjcJXJK7pyMkD93VO1mx5Cq7pnS3-kf62pgNRnIFTlBpROe6kPO3UU9rwmBbslNAc8l_w81RvBtzN7o-sloQL4tkC8oVYhUWzKPADstG3SsGMuNx0si30EXtpmu2PP-2fLpRh2ow83VfSeCPlV3JAcRr_EhNQRsVmHpn2C5o3Xwh5cuHnTKI74EfQ3L6lMgE0Vyk86abcj6DbeYbzusyNEFNqB_YC-mqnYipR-3f19TbwDS0utFRU_1RG_1Olwp0XrU2E1-kW0BeQUm_3fXtB2g2eMRpALs5T5YRwOvCPWcXvkHZU2S00nIDjSJZubCVQReeMCyjYDRQBLn7-Lxm92p0aB-R_KPfZ_Oyk6zzFN1SiYfBTCtW6ifaYbNPCb6TGc" style="width: 40%;">
                        <p class="caption">CNN과 ResNet 구조, 출처: ResNet paper</p>
                    </div>
                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>차원과 크기가 다른 데이터는 서로 어떻게 더하나?</b></span>
                        <br>그럼 여기서 하나 의문이 듭니다.
                        아래 그림에서 보라색 레이어의 실선으로 더해주는 부분은 레이어를 거치기 전 더해주는 값과 레이어를 거친 후 더해주는 크기와 차원(channel)이 서로 같습니다. 
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 실선은 두 값의 크기와 차원이 같은 identity shortcut입니다.</span>
                        <br><br>하지만 점선을 보면 보라색에서 나온 결과를 초록색에서 나온 결과와 더하는데 둘 사이의 크기와 차원이 서로 다릅니다.
                        예를 들어 보라색의 레이어를 거쳐서 나온 데이터의 크기를 (batch * 64 * 28 * 28)이라고 하면, 초록색을 거쳐서 나온 데이터의 크기는 (batch * 128 * 14 * 14)가 됩니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 위에서 ResNet 구조를 설명할 때 두 번째에서 언급했던 down-sampling이 일어난 것이지요.</span>
                        ResNet의 전체적인 구조를 보면 점선으로 그려진 shortcut이 많은데 이 부분이 모두 down-sampling 때문에 야기된 두 데이터간의 크기가 다른 부분입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이러한 문제를 해결하기 위해 저자들은 세 가지 방법을 소개합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemdEvjvPO6IzSuFyi4B_n7r7dsybvqw06nOqsRkH_3XxEcxyyKZwvIPIUlGC_OWvmsIzVrlXtAItDCt34QSaoJivdsIIvF1_ve1vqgnLxKTik9MpzUeLNt0onnQLL6sSX1Q4t1MP83o8mUmX-T9Qm5VrcBvJj29diOV3G9UrcFf3VgTUROQ5_T6z-_heEJkKq-aALH31bDgvUY6sDBjtP9R8MmRpoGpF9X83HF-utm8Nx8lMmOy3dLznMymuf7xv9UR-jVXtOeha3RYTELyYplDF_L3XiQlogzDhuYmnaljMRqAkLWHCzJqGvkDWJNOVADcjw-44Ymu1KfL24qaqKYPzy0dgAAgWT7PIuHTCZWUoIRB_fyKlnsfXR66Z55vzU1vC-cZ6gHWNmeC3oghhYMaZReR43NGgd6pQveX2dc7HDbcVfC1Ej8CK43I902UwI8xP_gRBJLxfv2R89JdCTwVFCwAn-zUDJgvw4N6I0lojLkrXVZMOaFwx7kVPZ8zuTvQ3fiRlEgXcWDZqou_aNNK9cbvf_MGYtQqLTnhzuOJ-X7t5s-HvLgLvyi5cRe_zPKaz8OyriAqR1DyyGeeNHdJUKmP9Spl3Ko_3MLK_KySND58YiocUupFPDcPriGvJ9FF17uhM5NNSFChRj9Ybsj-yezaiZgnvpUU8vGcoYpTq41HL6NopZaB2XJm7DtuJDvscWdxoUF_kCgiPepcLkmx41LQv0Nq2R-1GqyPJzyhKwAfxepc47eB9hYZHOieva42JbWZ282bGCK2cC07-HL94BLdtbCrw68fYDpV1Ur8XjOzwxl2pZ2wf72UJYFMrmIOFlSYX6iI09V-2Z0coro_F00viYE0XD3DiZjJ9X3amMK7inN2NEQwnZ9CJru0TY8B_ea_xknfB4ieKyMOkVNdw0q8qFvL-nzKLhPtyxSIF75V09l1H2QX8bFwCP3BXIRCmO2bexBjuNo0WSEni_db_MiptOdx6s66riglmureEgu6LZHLFMcCW-EflwBi4i2WmoGa3KAcy9Mt0PTif_kPxIYZageGKCDZ1oo-zl_MMG_gDRiaaAiXKyx6XA4fvu-twXKvGD5Gk7w62XLBv2wcRdo7rbQ3L7fNgyk7CZqgj9cY5HQ0C9TFB14n5voq13B1_XsHXtOs4qHbqct-jeBh9hCFAapr56iyPDcDjWGU0cFRBqsYBu_MyL6pGP6Vy_uPnmYzYBboUMSNPQcw5eeA9lCrFoskLsHg-oht69AMxhG7zi33HXiowBSs4npo_6ldK8msJorya7cHFIjhhDlgx6KrUMs46cOhDO9nYYskELfSG4LVHtYuVqGvMUiU8_tS1SVTWHBqzTwckmPGV9K89MW_bYPP-Cxeg0v8mLDEdFX_evHWzpNyhVwefzyNvHNrRVAECtfwzsquo9gRTwr2H3mgdaaL8rZuGzWo0n2_fhwBKs7SKldfhQfDctbTcGobJxnTIQvP5-6bhKqTJ8JE" style="width: 50%;">
                        <p class="caption">크기가 서로 다른 데이터의 덧셈, 출처: ResNet paper</p>
                    </div>
                    <p>
                        <ol>
                            <li><b>Zero padding을 이용.</b></li>
                            위 방법은 2배로 늘어난 channel에 대해서는 zero padding을, 절반으로 줄어든 가로, 세로의 크기에 대해서는 stride 2의 pooling을 이용해서 크기를 맞춰준 후 더해주는 방법입니다.<br><br> 
                            <li><b>크기가 달라지는 점선 shortcut에만 1*1 convolutional layer 사용.</b></li>
                            크기가 같은 실선의 identity shortcut 같은 경우는 그냥 더하면 되고, 크기가 달라지는 점선 shortcut에 대해 stride가 2인 (1*1) convolutional layer를 거쳐 크기를 맞춰주는 방법입니다.<br><br>
                            <li><b>점선, 실선 상관 없이 모든 shortcut에 1*1 convolutional layer 사용.</b></li>
                            모든 shortcut에 대해 크기가 같은 실선 부분은 stride가 1, 크기가 달라지는 점선 부분은 stride 2인 (1*1) convolutional layer를 거쳐 크기를 맞춰주는 방법입니다.
                        </ol>
                        성능은 3 &gt; 2 &gt; 1 순서로 좋게 나왔지만 그 차이가 미비하다고 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 2, 3번 방법의 경우 convolution 연산이 추가되었기 때문에 1번에 비해 parameter 수가 커진다는 단점이 존재합니다.</span>
                        따라서 현재 ResNet이 구현된 PyTorch 등에서의 코드에서는 2번의 방법을 통해 shortcut을 계산합니다.

                        <br><br>지금까지 ResNet의 기본 구조와 shortcut을 계산하는 방법을 알아보았습니다.
                        이러한 방법을 기반으로 레이어를 쌓게 되며, 그 레이어의 깊이에 따라 여러 종류의 ResNet이 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemdPnUaXtCwyEb9akBJ2XMTGuNB72dM4DULoFGclOJKqGWJgnZ8jI_raE8Akm0scZ68k42n1hplyJ2sT811pIEX2R2O3U6e8Cj1xFsGlleH2C-K-XcCd1PPUnlpE83OWWteIhOF15eBPbi98W9BRkiRIaxOnkZxqpHVtVPQrljOcsYWpR9raPJ66RzBd8L7i8mze9tS-9DLxItmjvZqW1wQj1ONucymKLy-SwnXvWwPkobLNDRVocCoAjFnJ1TFmIfI8K_DHP9XPX2WuM1dcz8t4DAj_C_BSrfBoHs9Ur7rbl9jewASd6SHKiWeo0ZW0qZgt5ROq-_m8OG4mfgmi9zJuxib6KRUtItzOfXbiwpeaNaUaCP92AD-lOZsgKhz7AS9fazKztk-dcbxfMmW5kXyFXeAkxfsI7UD2l8Y7UFV_e3adK4zncClCjfuOCrnq5Xo4tzKXKm-GCa8wiJ2G_yOKwUfR47tDjQpmkZAOWdIKloe_xQfiQjv71Z92sTL8Mydv0ov4xHBlldgYYUmOmNdKc6x2DdCRg2fwlTAfTVqr9hRPogiTNz07nuQCxYPbYfcCPkIA8K6SfkK7z0IUpo3mWgkfeYHiW7dGqGL54qe48E_qDTeeJk8rxXVTwAVM_OikW6rYbG1w7lMDIJHIDZgN4Do5GbUVLbaHEMQ9boyMbg-ZLnS7CtrCYD-Opo1XN0KNydbghyXbRpO-VHwUUmu423gEWcqyFIpV2U9YKwO6JUNvqgpHovXLNSxUNw5npdOL-wdXuP4-SXJx4OrW7NVz6ffraOkPhGljbL_bmHCFdwLNEqjUpUn4OVRzq_5Eb-Bbn-37HegUgDRepNbwFJ0Uf2tGfuLAthWxKlb14k9xgKzpp9Y-U66sBB8hy7_QPIXpaNbICYSQzs3Q9Xo4JR-3ynxn5du3UR9ePtsSmAlPBb2t1mRYPq4jbuEgHsmRF-rfKo9_RhPRiRlTLZ9K_53zr7FWC-GHk6DXcC7p6Is0Hn_fPrDKT72ZwG-6jIPWHSnKt1vKvkQXUTpzTMvnLVap7B70i_I-tdKVAVw_4hd1spYGirp0fZA2LxnRQbI-kkSNvxo_RWdG42-RKE-jLsv329uI5kS3zLO6J0T37XXUTr0HQTraAYechXnEcKnKfDCj4Bd4u82lI5kbYmoC1DBGEgJ7vX3MoCqj4EdTyHC2dShLbSsG0Nq1y3PBy0rCpPdMFTrfejj7UlOlmsK8w5fVZduKUmoorIf-A9fFB8Q4XVlFKWg0ksrRWZYfPk7KWBQCQ9NZs1c3lhV9JBdc-roIpvpYv4jEm4AIsXkZ2ujKVT8qC2GG-ASEOnf1KEkphmlVzeipWfrQ-49ojXTmVyivOrMFd5k_LGCyMIGhojhn8cz7G5ZhTTevDsXRWl861eYJzRFydzLCHoK26ux9h7-RWUkGO6WuHNYNeLBLXH6bFXDOB6wEPxwvZaJIIzB4n0U36evVXP0MILKPj9aoVlI" style="width: 100%;">
                        <p class="caption">ResNet의 종류</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>Bottleneck 구조</b></span>
                        <br>여기서 주목해야하는 점이 하나 더 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">위 표에서 18, 34 레이어 깊이의 모델까지는 하나의 residual block에 2개의 레이어가 존재합니다.
                        하지만 50 레이어 깊이의 모델부터는 하나의 residual block에 3개의 레이어가 존재하는 것을 볼 수 있습니다.</span>
                        깊은 모델에 대해서 residual block에 3개의 레이어를 쌓은 이유는 학습할 때 시간과 자원을 줄이기 위함입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">깊은 모델일수록 당연히 학습 시간이 오래걸리기 망정인데, ImageNet 학습 시간을 단축 시키기 위해서 깊은 모델에 대해 이러한 해결 방법을 둔 것입니다.</span>

                        <br><br>아래 그림은 bottleneck 구조와 아닌 구조의 비교 그림입니다.
                        Bottleneck 구조는 residual block을 이루는 모델의 형태가 bottleneck처럼 보여 이름을 이렇게 지은 것입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그런데 레이어가 3개가 되었는데 왜 더 빨라지는 것일까요?
                        그 이유는 레이어가 3개더라도 파라미터 수가 현저히 적기 때문입니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSememNJ42EXUYjpeD6z7h8gpvWnJyaac6TqHZNcX6mL_ZM_2JhnA3WHwUMVm4BYSAAAQu-0klWB0STs6hgiCpqdwpaU961qc3I7mVzoHoeTJ_Ys5oedOD2x7fu7jrS2HR5dfWEM9zyaabWK-6CfAwjGXjcOgA88lfxlRxzPffN1aouOyWzq1qYCvmewtkVVpFTr-YDOYDyoF2HpFewmXKh44nHwKk_6IhFbrlTrA1gDgDCWB8aQNIQNmzJF5vTOBO8BkHDlHmUFivhTFTtSwCOkZV-9TEDL5jWgJSUwBhXyLhE7FAB40cpumJU6D-Q7cpLtOYO_bECEAiaSWaOdpZk-LSqvGFhvti0NgANtgyYAmNHtVQS1NFJ2vLNUrxAEW5lDLnp1AoNFWhKri8XuXpvKXAzRLFf8xozIK0knE0Bb_9aAtBaDYo-fqlCehGswY07xZPCjmNlK8EfghBD3cC9QqKYYtyQx-dfpxlA0Q6HH8QUmjIIRr35varkXsC67H7IRfRwh_csuBx1yIGEJYSPY51-pe09EHSDyquNUXAHGTEp2tA4iZelwyfnSrG64agnKOf_TdbUH_lIaQmBvUTocOc0qSgFF0-BK4z09iZ97OwCVuHpeoH8XmnAG4CeNC9cJtD__O2j0uS8rN6aih4r4sFYzi59BJjYhgy7iHDB-NfaaahMiwQerqhsN3FgNsrmXLJbRomORYx3OUr65pMf8rTjZ7Nc-IgyTesD-FnSHRISLMimzqXH6gEV35Z-25oNeyQMeVl7mGo8PrALyKK17s8sihxuwymWO5R0HGmbH_-lBLNZF3eARQcgLx5TcjeYvlyScbr6rCkwZf4YEhLYdwm_-heYBLMReepRchZcaLRkf4ZfQtkRf12SP_NpobQp3_0l45mfkja7kWQ67ARquXg1GNII28aB_fF1QJ39hP2dTug_uJ1wzUlt44YqLOvbSIH4ZqvYMwemQDeJ3TICA_ji3GQO8_8ZeJy3QZufbmNdgOyZTF4oH2ESnbyrr5TbFpQsjG1d1Q7LHWwAm3Fu9BV1Hawr8bUIn0JPqebj3OHYT14QvXGcN9bqXK6xotEQF7lXSCB0qcdFaRs6lHhLzV2H9jw_6g7UvhV0DBKiz3voFLyPUMcAceBncNqaeXhBqXvIQP5xBWXB7GCm0gY5dw6GEN1hIbOAHJ_H_TDNY-ftjbylsBailjwMe4qEb1mh9GzuknzkIubczgnlbeNxS8Cu9BREx6dto7ZfcvUu9MpyLSIdysaCRFFG2Cw6wiKz_JuZribOSKzONUJCypimdbKrFbQ_IPV6KPa8FBUwW6eedzaLsnePdM8I0wVtzAER84xDGyu0_lvdWNuidYTOB6kj0dOfT1nxD7QYrfcCL-O72_MlddKetLup0rPptpVzMnXRN_lnodPhL6AQnVyP2BPyNd9iuKHs_M52kT0DGP_LzsduEd2nuF0yW0r1lKJV3RDzZuuA-7pPp1P70WhGd8" style="width: 70%;">
                        <p class="caption">좌: 일반적인 구조의 ResNet, 우: Bottleneck 구조의 ResNet, 출처: ResNet paper</p>
                    </div>
                    <p>
                        <br>아래는 각각의 경우에 대해 파라미터 수를 계산한 결과입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">Bottleneck 구조의 파라미터 수가 레이어는 더 많아도 약 22 % 정도 감소된 것을 확인할 수 있습니다.</span>
                    </p>
                    <div class="equation">
                        \[기존\,ResNet: (3*3*64)*2=1152\]
                        \[Bottleneck\,ResNet: (3*3*64) + (1*1*64) + (1*1*256)=896\]
                    </div>
                    <p>
                        <br>추가로 위의 bottleneck 구조의 그림에 input이 256-d로 적혀있습니다.
                        이 부분은 원래 ResNet 종류 표대로라면 64-d가 맞지만 실제로는 shortcut을 더해줄 때 64-d의 데이터를 (1*1) convolutional layer를 거치는 과정에서 kernel 수를 늘려 차원을 늘린 후 더해주기 때문에 저렇게 적혀있는 것입니다.
                        많은 리뷰 글에서 이 부분이 zero-padding을 한 후 더해주었다고 하는데 그 말은 틀린 말이며, convolutional layer를 거치면서 변한 부분입니다.
                    </p>
                    




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>ResNet 결과</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        그럼 실제로 ResNet의 성능이 어떤지 확인해봐야겠죠.
                        아래 결과는 일반 CNN과 ResNet의 ImageNet 분류 결과입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">일반 CNN 구조는 그 깊이가 더 깊어질수록 성능이 저하되는 degradation이 발생하였지만, ResNet같은 경우는 degradation이 발생하지 않고 깊은 모델의 성능이 더 좋은 것을 확인할 수 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemcIXqyZf5IX4R1Hwo5ySzppiflY4UE-g4apnxmbKRFnUmhH43iHwMnei4GbVPporSPf0v0ShhqGMcXYFrRSu8h1Lhn7YFmWkE40sjqKij0XrQqy39qWIVaojJb6w-5raFCRAtfU3NyMjRDvaRGmSgOhvlMVzp4heqYRsOCcDT5I2365gIISCFthg-SH2Xwwzk-3d32D24MqV1PlBJvamzQC06ybjtXceq2Hu6VWIlO01dY9PO13J3SBr7k-6f4pyo83NTj8fwddLXIofmHudkRQgP_7SdUdqJOto7yJGSD7b5HyBU0XYNPZb2ZV3xB4tnl7x5P16h-y9ySPYeWEqa64fknoZ-DYJ_2HmW7MbVomQ_Z4O9T2Xq1z_laK0q0qEOYoepTDJP5_iJpRrDH_iqTodz3qaCoiUSAEue03eNiiVvzdUIWKbvtQxoy7NOXzvjq32023SogCJM7QPlzXEHdtqaG6aIF3pXYg_ug1b6lYYYUiJsiLkTalPARKpZ7yIvA6TUZ1xp32pRVIfQOj3oh8qokC91E56g-LA-PjbJMB61HsN-qsZnPKSsLia2KOFagvSQVDg9a0NjAQ5PY1P3pw5hhyEbbOE9u91xJRnBPW3TaAjXt2H4psXJLWZL9R-Wci6p5vPd6ObEbhjD7JYkwRtTJc1hJ4RsM5Rg5l1SuDacYnQ0cn6Psxp0PecnWSetdA47akBAaUz2AEjAGg6hR7TmlpyKQdvyGQ9uFotl1krSvt7dyat6cxTELrF-wpHfofsSVxXxesi7w2MV5rRaC9BCnqalyHmy9Wv35pY-ESijmNkzlucrov3T5f10MYLmFbuQ1vsT5GsvKYbuWoXkiZ6n9r_LMg9xGaJ61OF2bBfuAzMJpLNtRQW_lztE6-O4liO7PP-C4nUYTlEgpVdZH46K7UWKjRpbnqb90AD8wOtq94YtA5aWnAqoYqVDR1Ha_X24-QB9jMT4p0ZdiZMZeDfXUrXC1e811YwZa-V0C8c2yv3TQODmDPuS_fXG0UNuy2GtndOfmfPjT2bdrZUJSdyS-DzYo6oDII-4Wz2ahsetyB_9lu5JBobi6zsyrDtHGne3GBceQ4RuGsCWThf7vsdpMpmHxXVHcJ1LyGnBAHy5yVzgdkhmoqEsNxLRwoOPle9dMo0JFge-KNgeEGWFaLYFaDnrx_Og9lsCY3fSp8r7ksxRfNvJoPUgkr0K1_z9PlG7KuNZS-W9TPlQ2tD4OnM95G3jn4qaaeLn8hiWuI3Iif4egJBDahvP1X7080b8juqap3YvgDL8my7mYUsBzn2XyS4DN0mJfBWzGlydDCf7s1tjPxoWBBIIgjzs7wQ1lmoMfSSZ3rmVZafXeFwH8VmMycCoQL5JV0gPfNL_lDq567C2HLxkn57hzekTZ7ZZPvNdch7SWrpXZQJLncZMYrjVtifq7_l6rkN-6AoQ5e1KaMu3gsRTTfTyDW162mhCuBljC5hTXS71UA6jVIne0" style="width: 100%;">
                        <p class="caption">일반 CNN과 ResNet의 ImageNet 분류 결과, 출처: ResNet paper</p>
                    </div>
                    <p>
                        <br>ResNet은 당시 최초로 사람보다 좋은 성능을 낸 모델이었으며, 지금에 들어서 shortcut 기반으로 모델을 구성하는 것이 기본이 된 만큼 AI 필드에 있어서 많은 영향을 끼친 연구입니다.
                        아래는 ResNet의 논문 링크입니다.
                    </p>
                    <div class="link">
                        <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">ResNet 논문</a>
                    </div>
                    <p>
                        <br><br><br>다음에는 원래의 ResNet이 너무 깊기 때문에 직접 custom ResNet을 구현해보고 CIFAR-10 데이터를 분류해보도록 하겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#CNN&emsp;#ResNet
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="pjaxPage('CNN2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br>CNN을 이용한 MNIST 분류</span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('CNN4.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>ResNet 구현 및 CIFAR-10 분류</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>