<!DOCTYPE html>
<html>
    <head>
        <title>Contrastive Language-Image Pre-training (CLIP)</title>
        <meta name="description" content="CLIP에 대해 설명합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/clip1.html" />
        <meta property="og:title" content="Contrastive Language-Image Pre-training (CLIP)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="CLIP에 대해 설명합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/APg5EOYag2YLKwg4Hkl3OPI4MS58D5L4JcI9sguJ0vR2v335Fzn_tCz5-sBu0RxMDTCbAPTQeKLTGL1IhqTpgbhL9Yg0vUQ-DPotQb9fF6YhCrgjG4gFMZWJqx_CrCIkF_OgdbQIuLOPNybLkHwhIgZia82nsYb4MmSzeIrwQ4J9Lf1B9aUfGitEw10CvEEjjMshPpNOU_0Im_wFcYMcivRUQib_Y5RkXe4CFhCGoPl3JB0wn1No1nv1GX_u2j9A8oCp-GWShaMW97LHl1jEVRL0hvbRn3YrgkzT0dibsG3lB_PAOX_v5AKixkmZ-D9c7bSXF5YdbCfzVcbgXAAZ3VNkfamGUe6zkfflOGbzn_jDtxhaOh7Iw_fF0tSfIE7fmoS3dqbmed34cVwvpyNNvaWhlosI4wC3gaOTlQhGNRf_2E0FM2gnsNaQuHv3TPIhU9EM-tkfV3ecdB5mc5wXURD6Ify9NPOHmjPRW4mJPL_v_5MoLPanUz4T9MZPO9yTF4oDHZ9AZKhXRggeACjyktVtahRgIY6Sqvq-FxGe5X-_lRaiW5VUnYU9VK7OMteloDYOWvDS1Z3PDdV0KiP-zxd5H2sG688Sxy1U_2-DVexznnyRzBlgVootfbhP3hHpXWh6Ud_rnpTIywe-ZDMN-x717YhKiruIP3yvws65mrPRSXY_rrpKIjXjjWblbaaFjalaJmKAYX6JhZp4tQj42Bhac1mckaYw_2wqPOpncAhADxI98r-KMkN0ZJzfNuKGT3bkeaeejdugrwWJDrLTUvqQwF5uFeSCccHbFzNXCzvZwYymZvpK5zNhofldhzcH-VA6twcjWp49SVHzvwwPEDjgu0F0MfGfZnS_FGgS_s2GfRQCYS5JNXA-b3qc0b1lr68pzcxPgJqzgCSHJ_h6ac2U_us1kDwKOfs3cXnvzvMSzIi8Lkex0w2DjodKNUV8HCauYYhOns2LxbnLQuS8ThlbYm9WR9BhVpqvv8FfzdakZJQZnm5X8r7yDM7lYYVw1Elc3zmpjOX67-DmAO5JjNZ5_Dhn1vLWlG82mV8RCdyXZtZrAkbNIlSRQU5h4WBfzC546zYzKpo34z9EvlHyAcUvs6ukAui82sqt1h16JBp_LLIZrm4tq2FAYtvqec98K5q0KXsSgpbZYws2QTNBJurhQvTtUlAIcJ_o08gVS-6QZF7Q1ZRJLYfPqnMSJK8mx91v77Gb_oVLEuZc5FPaTu4ZoIM5T0YvO7E3JQ--5CSrst4xDwijNDrHVJIfO4fF29Ugm30s7_aAfKGs2eZUpPkMAKNFJ7glrRv9MYk0r7qnK5aOLRUCQdygkZ7nOv8VG5qFHfwM-C8pyvwyHC4bD-JjDgxXyP50iYUMXgKtzbZn9Jz3dzKaF_VY0AxURXyVHDmCYTrYIpQ1bz3yFf8hhXpQYm_DXBKBIBIDPRgyQPV9RFWk2XXYq7kzd6fiCoqYO916c98gHrvHDusLzX4r1aWDWsA" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Contrastive Language-Image Pre-training (CLIP) / 1. Contrastive Language-Image Pre-training (CLIP)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/APg5EOYag2YLKwg4Hkl3OPI4MS58D5L4JcI9sguJ0vR2v335Fzn_tCz5-sBu0RxMDTCbAPTQeKLTGL1IhqTpgbhL9Yg0vUQ-DPotQb9fF6YhCrgjG4gFMZWJqx_CrCIkF_OgdbQIuLOPNybLkHwhIgZia82nsYb4MmSzeIrwQ4J9Lf1B9aUfGitEw10CvEEjjMshPpNOU_0Im_wFcYMcivRUQib_Y5RkXe4CFhCGoPl3JB0wn1No1nv1GX_u2j9A8oCp-GWShaMW97LHl1jEVRL0hvbRn3YrgkzT0dibsG3lB_PAOX_v5AKixkmZ-D9c7bSXF5YdbCfzVcbgXAAZ3VNkfamGUe6zkfflOGbzn_jDtxhaOh7Iw_fF0tSfIE7fmoS3dqbmed34cVwvpyNNvaWhlosI4wC3gaOTlQhGNRf_2E0FM2gnsNaQuHv3TPIhU9EM-tkfV3ecdB5mc5wXURD6Ify9NPOHmjPRW4mJPL_v_5MoLPanUz4T9MZPO9yTF4oDHZ9AZKhXRggeACjyktVtahRgIY6Sqvq-FxGe5X-_lRaiW5VUnYU9VK7OMteloDYOWvDS1Z3PDdV0KiP-zxd5H2sG688Sxy1U_2-DVexznnyRzBlgVootfbhP3hHpXWh6Ud_rnpTIywe-ZDMN-x717YhKiruIP3yvws65mrPRSXY_rrpKIjXjjWblbaaFjalaJmKAYX6JhZp4tQj42Bhac1mckaYw_2wqPOpncAhADxI98r-KMkN0ZJzfNuKGT3bkeaeejdugrwWJDrLTUvqQwF5uFeSCccHbFzNXCzvZwYymZvpK5zNhofldhzcH-VA6twcjWp49SVHzvwwPEDjgu0F0MfGfZnS_FGgS_s2GfRQCYS5JNXA-b3qc0b1lr68pzcxPgJqzgCSHJ_h6ac2U_us1kDwKOfs3cXnvzvMSzIi8Lkex0w2DjodKNUV8HCauYYhOns2LxbnLQuS8ThlbYm9WR9BhVpqvv8FfzdakZJQZnm5X8r7yDM7lYYVw1Elc3zmpjOX67-DmAO5JjNZ5_Dhn1vLWlG82mV8RCdyXZtZrAkbNIlSRQU5h4WBfzC546zYzKpo34z9EvlHyAcUvs6ukAui82sqt1h16JBp_LLIZrm4tq2FAYtvqec98K5q0KXsSgpbZYws2QTNBJurhQvTtUlAIcJ_o08gVS-6QZF7Q1ZRJLYfPqnMSJK8mx91v77Gb_oVLEuZc5FPaTu4ZoIM5T0YvO7E3JQ--5CSrst4xDwijNDrHVJIfO4fF29Ugm30s7_aAfKGs2eZUpPkMAKNFJ7glrRv9MYk0r7qnK5aOLRUCQdygkZ7nOv8VG5qFHfwM-C8pyvwyHC4bD-JjDgxXyP50iYUMXgKtzbZn9Jz3dzKaF_VY0AxURXyVHDmCYTrYIpQ1bz3yFf8hhXpQYm_DXBKBIBIDPRgyQPV9RFWk2XXYq7kzd6fiCoqYO916c98gHrvHDusLzX4r1aWDWsA);">
                    <div>
                        <span class="mainTitle">Contrastive Language-Image Pre-training (CLIP)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2023.04.21</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번에 소개할 논문은 바로 OpenAI에서 소개한 multi-modal 모델인 Contrastive Language-Image Pre-training (CLIP)입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">CLIP은 텍스트와 이미지를 바탕으로 multi-modal representation learning 입니다.
                        기존에 이미지만 가지고 훈련하던 supervised learning의 한계를 극복하고 대용량의 데이터를 사용하여 놀라운 성능도 보여줍니다.</span>
                        실제로 CLIP 모델은 이미지 생성 모델인 DALL-E 2에서도 사용되기도 하였습니다.

                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/2103.00020.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">CLIP 논문</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>CLIP 모델의 계기</li>
                            <li>CLIP 구조</li>
                            <li>CLIP 훈련</li>
                            <li>CLIP 결과</li>
                        </ol>
                    </p>



                    <h1 class="subHead">CLIP</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>CLIP 모델의 계기</span><br>
                        <span>Motivation of the CLIP</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        CLIP의 계기는 간단합니다.
                        BERT, GPT 모델들은 자연어를 기반으로 한 모델입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이 모델들은 어떤 특정 label이 필요없이 단순히 raw text만 가지고 학습이 가능합니다.
                        그럼에도 불구하고 이 모델들의 성능은 뛰어났습니다.
                        즉 다른 말로 supervised learning을 사용하지 않고 senmi-supervised learning을 사용하여 학습했음에도 불구하고 fine-tuning을 통해 여러 task에 훌륭한 성능을 내어주었던 것입니다.</span>
                        이는 NLP 분야에서 task-agnostic한 모델 연구로 이어지고, ChatGPT 등은 이러한 task-agnostic 모델의 대표적인 예시라고 할 수 있죠.

                        <br><br>ChatGPT가 나오기 전이지만 저자들은 이렇게 supervised learning을 벗어나서 학습할 수 있는 방법을 computer vision (CV) 분야에서 적용할 수 없을까 고민하게 됩니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">왜냐하면 기존의 CV 모델들은 labeling이 되어있어 classification task로 국한되는 것이 대부분이었기 때문입니다.
                        그리고 이미지에 대해서 CNN은 아주 강력한 성능을 보여주긴 하지만 이 방법을 가지고 학습을 했을 때 zero-shot learning에서 매우 낮은 성능을 보였기 때문입니다.
                        뿐만 아니라 이러한 기존 classification task는 아래 후술하는 데이터의 문제점도 가지고 있습니다.</span>
                        
                        <ol>
                            <li><b>Labeling의 수고로움</b></li>
                                먼저 labeling 자체는 어려운 task가 아닙니다. 다만 수백만개의 데이터를 사람이 하나씩 labeling을 하는 것은 엄청나게 큰 수고로움입니다.
                            <li><b>정보의 빈약성</b></li>
                                데이터의 label은 말 그대로 이 이미지가 어떠한 카테고리로 분류되는지만 나타냅니다. 이는 이미지 전체를 나타내기에는 그 정보량이 빈약하다는 문제점이 있습니다.
                        </ol>

                        <br>저자들은 이러한 여러 문제점을 아래와 같이 해결합니다.
                        <br><br><span style="font-size: 20px;"><b>1. 대용량 데이터 문제</b></span>
                        <br>먼저 CV 모델을 학습하기 위해 이미지는 필수로 필요합니다. 다만 단순히 labeling 데이터를 통해 학습하는 것이 아니었습니다.
                        따라서 이러한 task를 수행하기 위해 여러 종류의 데이터가 고려될 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">따라서 이미지에 캡션이 있는 MS-COCO나 지식 그래프, 객체 정보 등 이미지에 대해 다양한 설명을 하고 있는 Visual Genome 데이터를 고려할 수 있지만 이 데이터들은 데이터의 크기가 작았습니다.
                        보통 많은 데이터로 큰 모델로 학습 시켰을 때 좋은 성향을 보였기에 이렇게 작은 데이터셋은 부적합했던 것이죠. 그리고 1억개의 데이터가 있는 YFCC100M은 물론 데이터 자체는 컸지만 퀄리티가 좋지 않았습니다.</span>

                        <br><br>따라서 저자는 직접 데이터를 만들기로 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">웹상에 있는 대부분의 이미지는 설명글 혹은 캡션이 함께 존재합니다.
                        즉 이미지 데이터를 labeling하지 않고 이미지와 텍스트를 쌍으로 가져올 수 있는 것이죠. 따라서 저자들은 웹 크롤링을 통해 약 4억개의 image-text 쌍 데이터를 제작하였으며, 이 데이터를 WebImageText (WIT) 데이터라고 부릅니다.</span>
                        
                        <br><br><br><span style="font-size: 20px;"><b>2. Label의 정보의 빈약성 문제</b></span>
                        <br>위에서 한 번 언급했듯이 이미지의 label은 그리 많은 정보를 함축하지 못합니다. 그리고 labeling을 하는 것은 시간과 자원의 소모가 큰 일입니다.
                        이러한 문제점을 해결하기 위해 자연어를 이용하여 학습을 진행합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">자연어는 별도의 labeling 과정이 필요 없으며, 이미지와 동시에 자연어의 representation도 학습할 수 있다는 장점이 있기 때문입니다.
                        그리고 위 1번에서 언급했듯이 이미지와 자연어의 쌍을 수집하기에는 비교적 쉽기도 하기 때문입니다.</span>
                        그리고 이렇게 이미지와 자연어의 multi-modal representation learning을 수행하면 다양한 task에도 쉽게 적용할 수도 있습니다.
                    </p>



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>CLIP 구조</span><br>
                        <span>CLIP Architecture</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제 4억개의 image-text 쌍 데이터를 수집했으니 pre-training 방법을 정해야합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">원래 저자들은 VirTex 모델처럼 CNN 기반 image encoder와 transformer 기반 decoder를 이용하여 image captioning 방법으로 학습을 수행하고자 했습니다.
                        다만 복잡한 transformer 언어모델은 아래 그림처럼 같은 accuracy를 확보하는 데 다른 모델에 비해 그 효율성이 엄청 낮은 것을 볼 수 있습니다.</span>
                        실제로 최종 선택한 CLIP 모델보다는 4배나 비효율적인 것을 볼 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이렇게 비효율적으로 나타나는 이유는 image captioning 방식으로 모델을 학습할 때, decoder는 정확한 단어를 예측하려고 하기 때문이고 이는 실제로 데이터에는 비슷한 이미지에 다양한 텍스트가 올 수 있다는 점을 미루어보아 느릴 수밖에 없습니다.
                        또한 하나의 이미지에 여러 텍스트가 올 수 있다는 점을 생각해보면, text decoder를 통해 이렇게 정확한 단어를 예측하고자 하는 것은 'representation하는 데 이렇게까지 필요할까?'라는 의문이 듭니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EObOxEaWmxg9oEqzKRzg9ontmP9rYdmnABX9EsyVECPLH2smwfwkTypJPw7w6K5O1w6mp4Y3aSAVQR_Tq_PEDYdSW82DJhCSUb0ocIG4yceTJYlUQCVwwkIIdP24gcL0iM8yNawD1gznTqn7N0fgqvtBgZp-Eb-gOCqRjs8QmAAE0ufACppVTFqIkxmAkBO3MBrMvlJfm4W08jVjXg_i4qu-6965NIRR9j-XS9kqEMj_B0c6uGSRkSE7tZYbrdkH7j54lFKsfnaM7Txu1YT8DLdMF9lc1m9jvCMSFt7r_Kb82vZor0MeKqIbhGdYaCywyqCyORDzXUP3Q-RGN0ZpQ3UH6kFCx00ZmhsJgecl6SXzVS6-cRhtP9stxw6O_DmlSYNWulRbsNJGuuaoLilA4-_BdB7mfZJz3lomV3GxVb8ZptcHeA610ghaHfQeEaQwi9_54N5Yle9L5UjqVhrPgv_MvvPCKOprJ1p6SJwAuc8BB0n9cVZx84w8qwAcliTicynTrsfhXje2Eh5nj_2yhGVO8Th9EHY8c95trMtVwHhSyi3AfFJMnesW3JpY8bFhqDdAWcWtS7cgkvpgXvh0AYoEKxobdOSOtYQFuqcdx_pvxpPjEnJ-b1jgnyIL_zSbh2xzua4kf3B9_jfzW5iTRVRLzPNcyxVr9Y9Ii9Dt_lTNORNVurTD5sMAyrSXyHsIPkH4DIoX7o_T9rJ-XwZ--xiNLmw9xmguq_5OqVq3cRnFvh9Sc7iaL5-jtpO4kMqc5cp-HUIdBeXzGsb793MFlcOAHpi4svrlJx6BT-tBaUMXchAi3XGxGJSRPtyilBNPt2v1S46Tkmb0jva6uo_dQ0PuEN0slaXA1a9ACIeZSHDL4bI11D_g2-vEc9Xo7KJfF_0HIQsHgWd12hFii2tFK3zCWzl1zAWTQEySNCUtgdck7lb8lodGVJaUlCPysOfcOiOM60jlVpzG-BvyKVshiGO08bB2ijmiYh0V-Q-jANspVT6HrpuxxBnLyX_CnDlUgDuP5vG5qa35REQSe-XtgEvFYq7OLHTiYNMMoh76wTFzpmq1PWXtOqZngBekOf06_wpald1F2cuqqtH6ISTtbCihD3NDhgTuA0bc88Wfpe7NjPiEW93XqqcQEsDbXfZ5F8Uy1HpIC3E-cWu0mpfGP19ETxEG49iA2TPmp-3MiTYFmAvvfias8Klvc8x-S-7o_fhVdha_FyiAsuFppZsC2QYP9qVZJzsSQv2kmbAd2Z6RwYPtV5K6qftLQzA3Nf4I9Ilft6XcU_5kOeFr2_0k-oP2HExtJH4hU63oWbtwtQwpilCXNgbHnnDhRGSsW1WRoPiMG1hjcPIJe5yuR3Hw6dwdTpdhC7Z9ETwj0sjaeI-JGCiCgaSoZBjQaKJ9k6O4WlZEx_rHjwHALeJjnDs_5QvO9BRuKgaIQ3dVkKADiKcwEqAAO3BoeW_j0W6pkA4s_SUnxJYC9h2gMrh0IJjgs50" style="width: 80%;">
                        <p class="caption">언어 모델의 효율성, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>이렇게 너무나도 비효율적인 언어 모델 때문에 저자들은 contrastive learning을 사용하기로 합니다(contrastive learning은 실제로 현재까지도 많이 사용되는 기법입니다).
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 contrastive learning을 사용하면 아래 그림처럼 하나의 batch에 \(n\)개의 image-text 쌍이 있다고 가정하면 총 \(n\)개의 positive pair, \(n^2-n\)개의 negative pair를 얻을 수 있습니다.</span>
                        즉 하나의 batch로 두 종류의 데이터를 얻을 수 있고, 총 \(n^2\)개의 데이터 정보를 얻을 수 있다는 점에서 데이터 효율성이 엄청난 것이죠.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOYag2YLKwg4Hkl3OPI4MS58D5L4JcI9sguJ0vR2v335Fzn_tCz5-sBu0RxMDTCbAPTQeKLTGL1IhqTpgbhL9Yg0vUQ-DPotQb9fF6YhCrgjG4gFMZWJqx_CrCIkF_OgdbQIuLOPNybLkHwhIgZia82nsYb4MmSzeIrwQ4J9Lf1B9aUfGitEw10CvEEjjMshPpNOU_0Im_wFcYMcivRUQib_Y5RkXe4CFhCGoPl3JB0wn1No1nv1GX_u2j9A8oCp-GWShaMW97LHl1jEVRL0hvbRn3YrgkzT0dibsG3lB_PAOX_v5AKixkmZ-D9c7bSXF5YdbCfzVcbgXAAZ3VNkfamGUe6zkfflOGbzn_jDtxhaOh7Iw_fF0tSfIE7fmoS3dqbmed34cVwvpyNNvaWhlosI4wC3gaOTlQhGNRf_2E0FM2gnsNaQuHv3TPIhU9EM-tkfV3ecdB5mc5wXURD6Ify9NPOHmjPRW4mJPL_v_5MoLPanUz4T9MZPO9yTF4oDHZ9AZKhXRggeACjyktVtahRgIY6Sqvq-FxGe5X-_lRaiW5VUnYU9VK7OMteloDYOWvDS1Z3PDdV0KiP-zxd5H2sG688Sxy1U_2-DVexznnyRzBlgVootfbhP3hHpXWh6Ud_rnpTIywe-ZDMN-x717YhKiruIP3yvws65mrPRSXY_rrpKIjXjjWblbaaFjalaJmKAYX6JhZp4tQj42Bhac1mckaYw_2wqPOpncAhADxI98r-KMkN0ZJzfNuKGT3bkeaeejdugrwWJDrLTUvqQwF5uFeSCccHbFzNXCzvZwYymZvpK5zNhofldhzcH-VA6twcjWp49SVHzvwwPEDjgu0F0MfGfZnS_FGgS_s2GfRQCYS5JNXA-b3qc0b1lr68pzcxPgJqzgCSHJ_h6ac2U_us1kDwKOfs3cXnvzvMSzIi8Lkex0w2DjodKNUV8HCauYYhOns2LxbnLQuS8ThlbYm9WR9BhVpqvv8FfzdakZJQZnm5X8r7yDM7lYYVw1Elc3zmpjOX67-DmAO5JjNZ5_Dhn1vLWlG82mV8RCdyXZtZrAkbNIlSRQU5h4WBfzC546zYzKpo34z9EvlHyAcUvs6ukAui82sqt1h16JBp_LLIZrm4tq2FAYtvqec98K5q0KXsSgpbZYws2QTNBJurhQvTtUlAIcJ_o08gVS-6QZF7Q1ZRJLYfPqnMSJK8mx91v77Gb_oVLEuZc5FPaTu4ZoIM5T0YvO7E3JQ--5CSrst4xDwijNDrHVJIfO4fF29Ugm30s7_aAfKGs2eZUpPkMAKNFJ7glrRv9MYk0r7qnK5aOLRUCQdygkZ7nOv8VG5qFHfwM-C8pyvwyHC4bD-JjDgxXyP50iYUMXgKtzbZn9Jz3dzKaF_VY0AxURXyVHDmCYTrYIpQ1bz3yFf8hhXpQYm_DXBKBIBIDPRgyQPV9RFWk2XXYq7kzd6fiCoqYO916c98gHrvHDusLzX4r1aWDWsA" style="width: 100%;">
                        <p class="caption">CLIP, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 아래는 좀 더 자세한 학습 코드입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOaI3-qPlIo9X78EpCM6MO6uN01HVmrAOTTWroWgKBXXXAzcczl4wj2B6-jlvNwvWIQpxRuQoHEisGZUUDA02b7uarTCEgV00hcstcBFqRRw1yWHrSTRloISpTewaj1v5rAHWjJU-eHdLs5OcyLYhh2YBGWMwPjK15ODaQSlaGJfGhAiIwnQG0_9QR8MY8-vAjvaDyKKmXwcYhRhpLkEcbUulhkb_gmx7Y3wQSjhJmiLWPxjwTpUkDviUtEBoXmMps5cweshd9GdoRQJ0CRef9KNKeI9xvGWK2F_udZcVAnXfUn_zSaEvKC53cVEw-RlDzI_HCOmpoOWkNRtdWd4CQ0rU6jE_MSrsXJPbdh7NKzBXE5O8WFTB_SXkc2u9XPPPHLkeri417n1oBczQfitHz2UYhzOYHI26x1MRMhBhxYKeTXKQ6DyBjceryvs1Rsn33z0sFv2K1qp2L-oldPHE6YM6_uFAToYj-1e-O2qobe1Yw6v_tFz82r1IaUQTh_gjgCkcsrhrIHJLyLlyFzy6ffVZ_XS68pdAUI1g4wY8ba_fEo7B_e7FrebuPEHGkGqosjE0idvJai_VuiXi_sKMWekQtlMmcVnEe-fYiNyJXlAel5pIZK0OhbRqd2K0pf4P6S1itatFuj7KOyRyv0HrMJ-6P01UsuUOjnOFVIeKvTTigQ2JxP6lvSbNrNvWsFGiTL2zRs6-vZHZ_fburCUMrFkn8NcFfJOgES9AP_8zmB9GLrbqEnR640AUQKjgHmml7VntUsAHwBU3_3XGvIkzQtxeSJNgFfuVB_itPtRDNg2vuyXPPtBEic8lVXQRTBI4b2mGWoV9-sVriCuYj_lcx9GzOfVLrHa7BbLOKDCILgVwTqSbbSA0-n28dQ4hBCqtekWGAFjQCxy4HjbRGo7ChgPOD7-FSUd4bnxYkq55cN5LgsKtXaDlSBgPgwY7txvFELnNGQrh6uZanpiy3TdtFAbEtbpugI0zBFkEaPRTk2Zks8XVYS3SaJAkR7PxuUPwf_dYYjp1u-Yp9uFznOmelSiV8tJOx2o15YDDZPN4rggI4ccye4dKxssS4yfBB1MYAj80K_wvalAaSyVU5JIMenh6tqNk04amhLZdlT78P0GUWsUIFNdC9eyw8MNrDlAREIRusdicKeoRNJex3KggplPXIMSq_ggWoOsNAllBiZylwo_Ch0FFJuPUWUu50ABYlLQ3RMuOmsonWiUaoZgF_pprb9UOgaxzI9hZd8RKm9Cc67dArOgfM6VuI5QnlkFmIr9Zb0C08Ur4Sv1ER5CSDYO_AlleOZWPMFqbSeSqHCODBH15H-R34oTA5VcIN2sXBfLOZXwr6_tYVRZTOw5Kpx2gFqyT1xh0UCBY6yxnyVcAdRPdhWHQ1VvhLdox2cFAsYxKkLBYe5_GAJZyfNJAUtGQarNKEj6-6vy0WdCNe0kwqNZCvufM36AOzRutqRWmfk6SsBYFWZenbaRdA1qtxY" style="width: 80%;">
                        <p class="caption">CLIP 학습 코드, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>먼저 이미지와 텍스트의 인코더로 어떤 모델을 사용했는지는 좀 더 아래에서 살펴보고 어떻게 학습 되는지 살펴보겠습니다.
                        <ol>
                            <li>이미지와 텍스트에서 인코딩된 feature들을 추출.</li>
                            <li>각각의 feature에 가중치를 행렬곱 해준 후, l2 normalization을 수행.</li>
                            <li>이미지, 텍스트 feature를 dot product를 수행하고 학습 파라미터 \(e^{\tau}\)를 곱해줌.</li>
                            <li>그리고 모델 그림에서 보았던 곱한 \(n*n\)의 dot product 수행 결과의 diagonal 부분만 1에 가깝게, 나머지는 0에 가깝게 학습.</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">이러한 방법은 같은 쌍의 데이터의 dot product score(코사인 유사도)는 높게 나머지는 낮게 학습하려는 과정인 것입니다(이러한 과정에서 이미지의 label은 필요 없음을 알 수 있습니다).</span>

                        <br><br>저자들은 이미지와 텍스트 인코더로 각각 아래의 모델을 선택합니다.
                        <ul>
                            <li>Image Encoder</li>
                            <ol>
                                <li><b>ResNet-D</b>: 기존 ResNet 모델의 global average pooling을 attention pooling(QKV 연산을 진행)으로 바꾼 모델.</li>
                                <li><b>ViT</b>: 기존 ViT에 layer norm 추가한 모델.</li>
                            </ol>
                            <li>Text Encoder: <b>Transformer(L: 76 ([EOS] 포함 77))</b></li>
                        </ul>
                    </p>


                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>CLIP 훈련</span><br>
                        <span>CLIP Training</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <br>실험에 사용한 모델은 아래와 같습니다.
                        <ul>
                            <li><b>Image Encoder</b>: ResNet50, ResNet101, ResNet50의 4, 16, 64배 연산량에 해당하는 EfficientNet-style 모델(총 5가지 모델) + ViT-B/32, ViT-B/16, ViT-L/14(총 3가지 모델)</li>
                            <li><b>Text Encoder</b>: Transformer</li>
                        </ul>
                        그리고 아래 결과에서 후술하는 CLIP 모델은 모두 가장 좋은 성능을 냈던 가장 큰 모델
                    </p>




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>CLIP 결과</span><br>
                        <span>CLIP Results</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>1. Zero-shot Transfer</b></span>
                        <br>저자들은 모델이 representation을 잘 학습했는지 확인하기 위해서 zero-shot task를 수행합니다.
                        Zero-shot task 수행은 아래와 같이 분류 task를 수행하는 과정입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOY2nkxVgK0-OoLyzwf8ctbYHStQ93be3P6jYM_kU9lKqf0sdPbRFBuMczvxyGt_xd3p42zSFVG6kZam_66yXNvay4HrxPyOQ4tqOEs6-sq5_6amR-ZmPBgyg25VgkrH-fZfOXiHUYBCJkBXFJ9bF-axIBAQl_tWpB7K_NLnR6Nyh6fyOY5cb5t7g3M0TXmEm7_kZJIqpMqEuuW1p5xU5OMZfXQ5FyLgz2TiK5pKoWIOGdVJTM9Xh7AcyrO279ypygcNq7MGG87uAfLNCUxUG4bNPFGqS4qP3HKoFtE6SSx0DX6lcGVXztVxE-8thmR5Un7RSob2XlaH0yU2W5gZcuTF-lCOHfUZHcmEhLV5Wa6hJszU2K_UFPTSuEo76F56aP64xrGw1laUJ8BLIZm2RA5pUhkjdlY5tBBuSLEjTuFicQKOV0SfkDALZR2Vn0rAi5bt6UWmD2ROogQtOtdHfp2vId5pGeKdsQX5IXXBG5JU54tkwH1I1bXXAOR5cEbQdTxGAXuPiOYDE0p4mfTg7MWH_U3C6EjgNqfAooIVK6gCUVbC3Z9rtKFjVm3MDBnclHMqsgpbwKkLYKt039qdFhIh172nqdYbyAU9XH2V7pNP9C7-bYAiBxShjcLvM9oBv0nDjOjATxt1ETbNryyWMG2wEapb24c9W1Qzjh_BCwJRP2lrieOCyGz3Mv0RtZbluglDR-KpK5KzsRkPGCfTEjNvnoMGFn6XnTVj3ADcUiVJAogy8M32datUmigSPkRsZWJfuQZLOGU6-6zcOAWLRWX5qtyVTAvzJT3PLkmLai-h_MAgXcDto-d49626hNZiM61NIGvY0ksowmOeLPesipr9YfFGXDPg8icqT3i3l5EvhlJBT7vJfOLp6Cmg6kC5jxIcDHk7mqVw17OzsidE9zbOiLJdvSZ-8546IZL9gHXqMG33CJwGSdVne623eXEeBZsjHCpbe5DVO2BMUWGS7MDw5zyHrSB2kmCAGurt4cvgHggochZx1Ez_tMuDDbE7xfTZSVr-tydlgsHi-vo-ClO3wKpMHmdkYC4VacVE_Fh-eg2B4oVijz5SeiDjv2oXxOCGvBG0_pw82ck2JXfCrvqAaXLqhKIIAYm5A0zwir_ccaTg6xHu2cWu21ODZLVak0S0ZY0fpKKMwcMAd5-14gkMJqZzkOU4HPzIzkC43ML06b6Bygg1-bhyIQnk9drnWoZ4sVcu7qtzj64h1kehbE3rjCxuGX9oCP-0PJsNpCkkJv-Lv0Wp-3pzINzb_4CaQZHYLap7G5JPCAFfubArEoc2fsHgl4O1ZHt8668BuDqPxh3lXxtobPzMrLVyjdx9vjkNvfAnWnfO1m4S1NLQNkiUuxloGIGTC6lEUsNSu6SRXIegd6HYahpxvKI__puh84MGc8YtkPBj8k7xKTAB1eNeafok0jorqITITB8lOxr9L_ZlLX-jzdpLZZ1MciFU1uPL03zLHQ6za0OsMWx0rRU" style="width: 100%;">
                        <p class="caption">CLIP zero-shot task, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">CLIP은 분류 모델이 아니며, zero-shot이기 때문에 한 번도 모델이 참조한적 없는 데이터를 이용해야합니다.
                        이 과정을 수행하기 위해 CLIP을 이미지와 더불어 class를 문장(프롬프트)으로 넣어주는 방법을 채택한 것이죠.</span>
                        그리고 이 task를 27개의 데이터셋에서 수행한 결과 baseline 모델인 ImageNet pre-trained ResNet-50의 linear probe(pre-trained ResNet-50 이미지를 분류하는 top layer만 학습시킨 모델) 결과에 비해 16개의 데이터셋에 대해 더 좋은 결과를 보였으며 STL10 데이터셋에 대해서는 SOTA를 달성하기도 했습니다.
                        하지만 일반적인 분류 이미지 데이터가 아닌(e.g. EuroSAT 등은 인공위성 이미지 분류 데이터) 데이터셋에 대해서는 baseline보다 성능이 떨어졌으며, 이는 어찌보면 당연한 결과일 것입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOZtKS1hM1yTq3dyqNAfb0A-nBX3nnBSYBoAzEmg_4CeWLM-zOXpEyS-cavauvjP3vtdqdEeqvFfutikFcr9G6bKAsC1aVHmBvhdyM1zXRzZbAYSr0NAlkKd9mkRKSh-VqwmzGZu-xiLk4MJ11S3y_OTh2qkd_ZvEbY_cD0BKZXH0sk59zxqo2JQR6OtUdGKVP5EMftrjJPocUmo0xn_PLCui8FH72YciiSC6ta7eZPHHNF4FKQz93aY0tLay02F-YjR1eeirUkS6KRvbUTCSCAolJsURH9iOqdCQUTRHYRCmqhYefaLV6nr7qwDmDlTMFmsj2zGhKfrNFPK9zn0PVS5Iae9Y_aElettYkNwJELr4WUUP540fpiyYHPDy35cWH2_z-2yUI1Nft6IJ-9i5d_-mJoYsomftseZb0m66eoFOszGZHyS4c98hzDsmhW658QGyosUBfJvyEwWjkXDZPrJxYXRnjLvOOChuAgdemyOoA5-iC86UOn2UfLhwvn9Wx7IMBsDbgq96eEUpdf_YvfL021sc5FS8v8QuGNaPK0gm-Ios1039MAhD0eaqUk5PjLFyJLC23V7O-gn3NyxQqhBHojmUjRF24Ufz-cKKQRggXLItEPKi_l5_z3flhbVBg-j7BkNubw2qiK5gTlj1VaU_QfF-IkQ7VLgXLMbQ9ZtC7OKKOLeS-TOx3IhWrJjNTbMawkpgHuEi3k1DqTqS28lh0veuVQC62CChmHDSeFhgmt-ZT0sNy8jFYzvoh6mJ3hOYT7d52MNe9b2FI7imNLsYHs_crEUkTTzY4Ua1bOAfrD3L18Hxg2gAxLalET_gY6mjqvXTQY2ytM0FHB7A4Ss0ZRKB7gHPicUa9cs8P1ilOxhS_IuhBx-qMqpYiizyyXU0w9XxBBHCrfSafJxfWizIde217Gmo5Z0fkJ4WVMjgiyjP5jygBRvMqDw0FTAZWv5OVmNsFmEjGdbBlWu9FvOpU4Hjqj4YTKCbAKZ_YynXGWK-MgnJvj8bbzsK2P_i-5vDWXyyud1Bp2UTce3CBeb8NZPlmzlB7tlrBjUe-YV3hQz4kUMJsp-B6kdpZLc3MDnXaEup77hZDCJf8dO3ITc5I11JfGmDBe0-rO-b1zdwc6CIYZGeYZUOznMNl_Ll1WzKJnb-NSX8JNKzhcMh6oNqzdKFEKb0_Tcq_aU8PcN20R1NVddxBqiy9HtrMJB2qSDWjbk_KnL8g_r3G5D77AkSNigqZWjILI2eDv-1jGDoIJtdcVyeyV7bKoEDPHxYS10peMGYpzOqjV0B1u7OwpK8y6TE3PYaFT5K_LScMn04HOofB5p74FS7RfNLKCI58rRxL4icz832OsiUalJ6bf0n-0_hSj4S-rzKBObIACF1NeGI84EvvxEdsE8JheiyjrNz7gkZ654gN2ol57N36O2Za0ItB683tkAmqCF8RaStgtUy15VqC7eAmFqvuzMkX0-cH54xHwU8-DE6hSc-Q0" style="width: 80%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 놀랍게도 CLIP은 다른 모델들의 few-shot 결과보다 좋았으며, CLILP의 zero-shot 평균 성능은 4-shot 일때이며, 이는 다른 baseline 모델의 16-shot의 결과와 비슷한 수준으로 아주 뛰어난 성능을 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOb18gwWECP0zgo-610nUQVmwt3iTCBwNRQ2xWp9KYpNi9BDCLyODv6pW5t84UCyhcnGJzJlqCeZIWXi-_jCom0yHJucNAsmMeRSLCLl7IBBXO5IbugXtqtpDCNRT6gUD62tEsQzCO4oEd7DU_TECnryZyE3ZIW07WIN6r-GvxcAASBN7PXfxCI-9mdbXx_SD1N0obHsOps6hanqd3aswcfQQUaNCtPnOtbZjqEo_70pY43WdkvA4ffnPjQnrpooiavPJzY9yIhcdAN2LjH3rH7BoU9B72YiVtqFnAwMG6ETARUFsDBdQDe6bRTHDkaMzNwHEiO3alyKcKDeGjLoh6er5AgXT9d8mj-9upz73PAUN0V9CB_O_542RJ_PGR8E9C5QtFlssY9tzG7Ll9fx2VDJyT7aFmRN_JTYY5CuZrTBWj3_X3ZM4_BPZmmIUIhjNCXbJvVuqoQPGCiN_nYJ2CXJJanqFdwTdAmkITYGTC4A1qRHgGK6T1cXPQWmY5hWd5etKCrbGC8GT_RRYcMJEzZyS9iCFRFS54-bSUqV7swWwaxNhsEQmC49VcwNodIxFwQFWy5xomDptk5XLluDpwU5IWOs-GOj7_0adfhkdUrG48aG6kdC4mPQg-K8n8OD3fZ4C_5AcDMeR1l51O4hvxWsxFA3QfaG5IQdcrrxBxidYOhbBB_4_SzywntXoo_AkxLZsCUg7ag6bSr2XUoc-4t4D7TTyT6bxvbzKzwiR-ytwklOCI64V0uoDVTueYBD3jTgJxVnZeKJdgwkDptTygKBoJxNTM-zZ6dxfRXgqvpgEpKkgasdy48BqjXdRJFPm7ZhOG2TECxzzIe8lOG8CsorIGuA0f3ejgjRoZCxWL5PX_wXRJoHAJ2kJHLHen_S1eRPP6oylYCvQxaMAMZouXt7MSiC4gVO-pd_VA1WtNRRSZeiVjahO_tl1kEktkTNyzgWBP6WbJG8AEPtSdipvmtrhtM1QCIYcZVRzoGc1ykPGgFrEVYJqsxtyGX5fJ43ZXj3RVGttdCnRGLuIztiLnaXaaj360dpGXP4UTZMZO5M52e6U8VDqi54rkjFJKW8owfcgfu484YaXJn6WTTCgwKGcZcy5w8kUh1zXx1KOOuiPM_6zitOhLllzZOitCKIaThqgHa0T3DSAvZliGAY4t8uhRm8g-k4CNLIS9Nhly5mYlMegpAoegcabZEdgHWN3_p2GqCXFZtc48xY4F8q_e47JTZXjef9zMMt_U3b6IEGUuYRYTS01Tg4g-PfTdFauGW3wb1aUB85ubhtVBEqPfsRt7aXqZP8z0DahQTVM6hJ9bNXH-t9MrJX95EB7CAfcX76MshTj46S3srl1a89D3P9ITw5b6UVEl5isVglHcc7eEypAE0sCjctF4NiODIw2RWCAfDuJhlk439BGYB3f77PXlix9Mi-YkSs_JTtMk0tyPfft9PiKfwig3BjYcgJm4ZtL5PbUmaTPGXnVpzU9fU" style="width: 80%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>또한 Visual N-Grams 모델보다도 zero-shot 분류 성능이 더 좋았지만, Visual N-Grams 연구가 나오기 전에는 transformer 모델도 없었으며 학습 데이터 양또한 CLIP이 많다는 부분 등을 언급하면서 이 모델과의 비교가 완전히 공평한 조건에서 이루어지지 않았다는 부분을 언급합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 요즘 ChatGPT에서 프롬프트를 이용하여 다양한 task를 수행할 수 있는 것처럼, 당시 논문에서도 일찌감치 프롬프트를 이용하여 zero-shot 성능을 더 끌어올릴 수 있고 다양하게 적용될 수 있다고 말합니다.</span>
                        아래 결과는 실제로 프롬프트만 바꿔줬을 때, 4배 더 많은 연산을 한 모델과 같은 성능을 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EObM_yYhhueUnUqtVDlvVvlQLp3DTVzytfTqC8a4aeblPfi3T0IEVdmz_mcs92_ryfFRHO0y1MjFeTvJDk5koHpCEBBR4PkF4cl_x7a07D71raYxJu5GOPB_DZVZ3k7bvdgAzVfyzVuDFo50EqymrSQSt7zVj8b1I3ehNqQ8iLV0oGXNzIiXUoKQUKeKKAb38p9z2OpqZjkThVpdOGwosOgDJCwXajk8N_hPjG9d1I4NMB8Lh3covoC-1VneRT05ZR8OJ-MSz8_WCHwxougl4eluvEIUNe5bxwb__95qoNNt-aOF1irgIjV4XUafW82ujiTVjn0rQVK2I8cMJ3hUwsrHUnh81-AVgfkrJTOX9zy30rIDwR2156X0iTUD4LP41M4lkmD4gUHUHG4wxWk57INPgezYZ7JZ6UmNvVXe7JTakVPAE46C76crLft3TLfXtXK2J2wx0GBSM_H2BLj52rZcQNd5jF7grRcBEwT0FPzfT7rBf-ftBtIueZvMxQJRyg3f6dDYTysLnTIE1JrnBcYMJauVzub_jTn80BSOPF-w_C3Tje9JsjtExYnZOhOwq00_OPrKUvuHbeE5DWoLArHFIp9Q5nxudgQfS4EFVRBq3Yp8O7PUwCbkT-g9gNavL3n5uURH6dsM-VqDItx7YiEU6EE2KBrjFgg8xAXWxzDHc2aLqjj9Iev7YmbqNI2FBqUpy4_FkQz5DeVCc3g7uwBPW-J_TvfVU1hJZ3-wIHPqD72-kXrt47K5iHUxHGO3vT5PjXd16Bi0Dv35LMil17yF0LKpu9Xx6isECmCz1NhSyNKAWZMsUpDBJbImYa5-P-tO3D9mkQo9Rzmv1MHWT_uCxFJZPsUECmU9XsxPOSSyYkQyOd1dtvRb2HK_sS-3W0vKdsarn4Afkh2xuCrebu_U1aI5Wm_kslY2xYJO_I5V4NcpRXQowbtX8rZNDQesDlA6hBSMLbgXB2PLVemHC_gF0olyvhcM_3Y7XRPRo7dFAytzjwOzxhN9KjrjAbCn-tZXU2sbxzfIH5gLInzY7zwqbe2r7zCK-a3YbOd8oMel8DgsCWi31r-0L7LhC39WYd9YCHVEsq2Xrx0toJPep0wjSQSbBy-1kMULAwjGU1FPbXLXH7ks_i3iJSsiQYhFo75GZC9jRMmmRtTWS44THbA7_wXGzdQpq6b5RBsXPMJTAT07hO48WquuEpO-JsHFWxp_7D0VAjnY4kv-JwBWYfxoJC4EQ0Cx2v-s-DvyaCyyu2fFfSe2rZr3HUsQUXBBoTdNlreu9_1Redl8n2iSRfqtZ4SjzW0wwYxGFtKuQIpoLNRyocPhDC9vnvgdA6jTV24rMoX-rg0OTgsID2AMsvA9IUqgSK__tYb6LhNJBu-znW7mqfjGFibBsYNfhOsqg6V-UOqmrf9nou9qrT8H6m5R1bXuj0OrvQ4O2F7oBvPo1zKQCFhIGrAmAwte_s15b1YmfHwyFAm2n1JNghD0aN0" style="width: 80%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>실제로 zero-shot의 결과를 보면 상당히 괜찮은 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EObOQkxvTj-KOfn1isKILUzOl_KM8oLJGOoMwgJTdbdebW-R2Cs2_mO3Io3Ieh1dgX34Ge10pCEqNp9e2dHWX7UZ0jMUNXcfDACs_yKb6ot9D3sNlWscF8dEJJjg5CsETiOerticPNVlC1JAT7_qVgwYAnyBxMu200ATJ7z-hOX52NoQG28Fiik-Es4JErmwwtCRlO1oMC3Bg3g5hPsmd3qNjC3a-8z_H_AQDGxp1bDUUpZqD__NCn0o_RXZfgh_xcZOuDrkkyaDlYru0ovg7hoSgOZkZ1TWIpodK0fXupgisNLtwpbokH8Aqequ72Wlj5-ddSz7sW88N0iTLoZdpJC_Wi5i4xPB3no2lpoabGXwUXp66qfvryT38BADpIgttZosskFoczzdZrzoO0AS8qz26JPiMMJfmEs_WjMjyEi1ONd22SEA9Kk0ndlfEC_xdHdKCxar5WnAWwTxQTeOZFx1GxwLKdMM5s6fdb1LIJE7MfQWSE9RCrspiWbVYY8K8_3XQ9HPSLJKoLNJi5ozFQB8iURNqtczPCGMC7ir2xjAAC00wVl0NTm5YIhCsX4DCatcV9oxJiURf_LzVjJpWuDoW9cTQhqKRVTvydF0MprCKYwO6zVElP2miUzF_aOavc2ZEqfXa4wCVC-hjO3znsK82iEQIBClH3u4E_tjyQuxSphf0QrvbEYSpdnvZoi42VQ-610rYLlh7SnOAptCRJulJ2pUNEx7h7PMS6E8yudwqljncXtHJWDNvJAzjH15fu8g1GO8nCr4GJlt-IlEQlWapt3m_f6vK1DnOQhQs4OuPnxnxE61aU0vwrMEh-JCLYvwBiBJolEB1FMSeMqpj5fuRTru4UgiVYUTyEwZ2olEDeOEgqGU0Yk7UqMv_JNT4hxWrRgFz7ASXuq5iwIPYSEJ78ATf1wP7uBh6c7b2b_HWBreoIPJEBbqljS54rPhL-Rove4dbyxHsoB85vsnNwPZCuNumq8LnRMUpJGlPoqOdu1lIGBfCgBTMyBemA61Q88ns49WSQKb0bUjF9NWT4PbN6V_x4nagJr5QpE8LiPycovtX3pyD0UkVpWuvg7_wKlodlavGa46fSG5NFEhnGzqQIOjyBCIRLts8zykHCti42G8PuHfnW379wECLlujWmVhEI6E-1wvWkYCGvYCmp3-u7L6BLQ-5aL1KlaWIAFkm8NQ-BeKsemOet7iN8pIihbhuWSG-C17juNQU7qtt5jotbXaSjOwdXSc2YpI20TnB0unZM75fJvmyfJAM6B-H4hGKDcD_fYEP9wpbKFcCxP_70os1JVXpyeDdmpmt9CDsmQ-V4aiSQK2MPPeccEruXsIzhUt0Bs5fPLk5yrL1-RhnzxF_1fkfxkg5S03b_IFYxNWrW9jrV6gVeZSbOKNBq6ocU6dM5TsAyxeMCQJubPd5iEUQZidJs1iZKuR2jqsxSSjrvPQs4oIhaWiY21gjU6LjJxylkG4HPME-pRct3Y" style="width: 100%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>2. Representation</b></span>
                        <br>CLIP은 궁극적으로 feature representation을 위한 모델이기 때문에 저자들은 feature representation 결과를 보여줍니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">먼저 CLIP을 포함하여 baseline 모델에서 나온 feature를 바탕으로 간단한 logistic regression 모델을 통해 이미지 class를 분류하는 방식으로 실험을 진행합니다.</span>
                        그 결과 ViT를 사용한 CLIP의 결과가 가장 좋게 나오는 것을 확인할 수 있으며 SOTA를 달성하였습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOaAujbcQQ-w6TOBbPV8-fivg7KD2qlN1SCsl42zdu5SqfFcBuK9pYrev7umHUUVbpPoufn5th7OB7CAhW7CTGD6oSvPJmizWimjeA0z3TwwoMFvSUB6XV6SVLdvoERRg8J-L8evc1zOjeaVFvQVLjkQUiy9t4tVI_WdzWvh4tWqjhvnF6J1_lBn33zHlF6wuateyIFE8KkI8XtqE1TkvWe3jresHRV-SbT18n4Ha1-beckOt9fhKAYzttr2lbGWwPALbx8ev8ngiZeyqCdPkw0abaj4LZh4iNbZ5r1rYDF1_wx6imlbvH8QKOg2voR0JvUlOs8IgHKJGtgtPfLQZ3HMiWAqxmF7AoimgztG5dqB_z8dZqpXfb0yyNIUsOkvhl-i1RaczBv9Din41B1eorNQT2QdhlgtHzsJYkWt2f_Cru-ECAIH3LCPX1FknFRmRt4a6wTqWHSNCBEtTjrcAzoglTRbpflkHAdFHSxrIk5THuWtcR0n8SISnUOQTp-WzUcuHp5l5sQd6FyKIe-BoC6pTnOsoIhVXgz7Z8D08TH2kmz_K7u82TwXYH3_4Ea7FBUpc3N8_8LEooKrtRBM_JNjHGQ4rGVpBTNTF7PdnezPGrdIIJgmzxsZBQd-F6i0gM2KVnWyoFjCDNF34gkOYlgGBMgGwgDEd8KyaT2b3y-GVNXa2dGun9C1e-tZG84iMKbPmCfNQL4hDpkzxBMEBX3DnrLsnhBo7di5h8DsiXCNLS-RF4o_oNpZrqcpVc-8ehxtOSe0Gbrl0TG6XoFkWnn2bHvzLgajISSvY7Meuz2BAUSMlQtVP_EycbuYG9nK_naCc1GARyItM4D3ehUNN_0bmYBFH8y4pkKef2Tom_IJow16Xh5q2Rq8-2iEMogRnplrUMB5PPt-Sj4VXAzMJyxWln9igEw7r1Oa3YzNNXfJmZtwikk7Kj7A1oodLmgCPNwVPZPg2y9QezR1cOUdEI8kAt5JJHQdtlFf6s3q8qgo-zRqfvEaL_lDZ02UN6QhmNwo6J-IsqFy5v6LjwfkR09CN0xU6kSwYF1fujO9N0hruMTW2JxvJBsI4Yzcw67EahTz0nUTYRr2EGJ1_bBvHhJVN-1G4GTaNCYxlvkQ_3Ebuc0I6uKxmSxtTG3C3xUthZWquGNdSa_EDLGM2_P6oSqLQ58B7jkAfDLz50n7USA9XP1qN2oic67dGLzy5uvccqYU6wkFwCLnzWBBNaY6js8MaZsnokfCoXHfnxa9_AQR-OEGF1A1kWSDvkEcugOxerJ1xZnHUBgg4xrKztX2xexeN0DtxCl9wAmi7SxUyy5elLZaX_K0FBE20CvKG8brSn6PunhZcNwU-8MpM8dwEVpPccvtHzbMARmW18w4GYo3zvRh51Nv1u-fzJ0OsMN-uQC1d474gOpEmXmpeIakGOe-rBh0U5krEciiOG9M48IWIWlOT50rZJduDXfLNWhHEtVsd-UohxyxB5ktJao82jo" style="width: 100%;">
                        <p class="caption">CLIP feature representation 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 CLIP의 feature를 바탕으로 27개의 데이터셋에 대해서 분류한 결과 EfficiNet의 결과보다 대부분의 데이터셋에서 더 좋은 성능을 보여준다는 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EObjOOY864EfB7dp4utCKiC69CtNggDhPP77LIxidGfDQfGR26TJYsjspkgg35mCTdo_USA3C68NfRgl27zfVGvLg4RDldW9nj4XjdaIjf6rzISviNbbEmMxVAhptWltN5eN1edOFQvlmHHQybR-JYBhuxk238O3R-aKRYKI9kO8dKy0IbW977BqG3hQWYWWSSiMWdLEcMDgdOAqSmYuBk5ZbtiG1b05oSFBzh2BJv26T474N0n7vWjflkyodmhGcVDOTCuUZlj5sJxayDpsHKqlRZo8ZldeTX4ZTIGKvzetTRUsHsyzLR9KB_DIc2gI-Sxr8tpEp2Xs4UH32J93tEjBdVc2WCl3UrmIsKE3CwnFYhKEbY-ULyJSECAhS-2pVYNotH4Zy3YD0j9umeahkUSe0GeEJWth_fvfbfzbg3CsmuviiQLxRe0wMMG9k6U_6hn3_Vwhg_2eOrFvWMw_RRFfI0HB0BGLS0fLbY_Stogu70MICA2QVyEEcSDGhkOglImA-5mn-qKqJJSlfsup3UMT96nFxHUXn0BLZoLoElb4IjMKbnbDvT-AoexbpegmjNX75ztIxutflcv1g7HtqyKc6KeiCs4e6WnLZRfVPmrsR2ngcSGA6vEsBnrDx902ydzh7xVlDliN45qfOLrI_itXw1ZehMsORojBSd-BdItxHpJP9jtcGcnGxgNY3D25lBkuM5N731KnAPVrgi-64RCXmYED7Xue82j4PG_VvuMCYCVDoaWJQw-bag58WOxU8y8Imai4IUI8xnHqLcp_vsiRmf1TaMDHbgs3CUB6YQYFuBlWA9KPKhmaf6VMH5jfcRrBRNu5w8d_7s-11mwaszAo6k1CfjRO-VDbNdgDvOyek0kF9vTvfgWox3y355ebAPRJkJWuk4e34P1kkUqxSYoYmNeASzFI0gKyU9wKDZ9YuPQ63y2INc6OMonccJgcEpk8ti8GZ8emOCxhQv_7wCssQMEhiNIcn0DTujABD9KjgTzDTHBjHCurL71RxiBg2K16Plvrth--BmfT_4DJmUQa35OFPFzWvSde7j6hGzHrqaT6HnUV_DIuAl8Wn2kOIDhPxHlK1xhk9NMU1iTbyizd-ucVwLnejjbIqvR-6byL3V8y0yjWesbwmx3Zao17r7gxDgTP8l7n5j7Hk5Y4YAHlzmEj85C0nLsrREjHvNRQXRMpb-3HUAT1ixtNWNF8umB0IPHI2QzQCQKeqg0HqBymRIuQfYlgQSF69M2ezpCwtNkREEt-cDTWjgABNP8cBB6FE3Wg0ZhgV6gIaBf7-5KPHlAvNOosru4Dbp4t8M9DTfy7f7LbKoNlDYDgQuL-AaksS_wbnd1opQY5vCuVXmUaxXmFiFzze-MYeysVeYWAQfviJJQ2hMRjsK8bekERX1pUxp-VLE1MAAfjBldklLa8PeHILU5oezlV87_2t4tmJZRByFRfuqnfXU1SG1hnejdPkXZFIL1VVM7AVjslebg" style="width: 80%;">
                        <p class="caption">CLIP feature representation 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>3. Robustness</b></span>
                        <br>CLIP은 다른 모델들에 비해 그 결과가 robust하다고 주장합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림의 y축의 결과는 위의 Representation을 설명하는 처음 그림의 결과와 같습니다.
                        다만 x축을 ImageNet의 분류 결과로 바꾸었습니다.
                        CLIP을 제외한 다른 baseline 모델들은 ImageNet에서 pre-trained 된 모델이라 다른 데이터셋으로 평가하였을 때 점선 아래, 즉 ImageNet 결과보다 낮은 결과를 보여줍니다.
                        반면에 CLIP은 데이터셋의 상관 없이 똑같이 높은 결과를 보여줍니다. 이는 CLIP이 robust하게 학습이 되었다는 것을 방증합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOZd5NrQyz3xLnarCnVT6W7_4ElO88rKET4-xoowwTxWXP8DVdC5wl2Ww8Yr3P8IaJteht6-vcGQsDa8cxhTpW5yk_boGIWVguiKcQoZdBXaxsCiAXihbyyCZJxz8duYinz-s5QARr2YiPPFKsvNECYLqa1_euvwaKS6fHXdTW8JIRM2FeK0DJYs71yTur74X68pJFmMV2BtPi67hp7wyGGWWZ0pQyiV2PH2907VaWGM06qXjgc2_bo8V4iYoEjw1NF8BHdd8Zt2MM_Vq2jBh8LhkAOa1PAdkNAeohQ6OlAYCAiIA_lIVHK951VLzYo9SMcAsi_COpPxZyjD_OmVwC7v2CDrAih_xELWeS_L0NmwlPQfbvQSAxqGZlfzzqdJ3cZ-g3BwlrjhZuIo8LHqbkvF4GVXCkuX8ZSsOIO7JIfFpH3WVavHe3a83qTKz6bCoL1Al_LswG6q5luLaMlA3VxkT-CUbUWcuKIfQv5EUZ-XDwvn4DkL6rd31HMrPhEd0JQdx4UKoiK_XTmzrIZrrQ8hXxk4PRQMH_oaHO_FKf_CUL8MkjGislpVlYqeG8AY0HNeZUk3-mQiPun5N70-KYrddmjcNiPuWWjLbadBFkYPh6CTowo0tShF6X9cUglvNiBjLOtpVHn8w-W5PRi18VNv0_h7aLkcqBPKL1Gemg60-MfCJgpqtLN-zTzEOBjaOcmPkKhUHBtFcafxm118lTrqpDZUeKEoulT8eZC9Dzot9Bk7AOESNLEs0uDUU1PKHKj4rX2wcCN6w4dZb8QlkBKlJlTQGb8csq6wT9aJtnDuF_9ZZ19gNy6-vcz-doLnLdU-z0bCuDj9Ts8_slUM4fJ5Xe5guzKuRYMoNzfmU8pQdARlr9xzXhlCKkXY_-f3K9PBgSoY7moVsJ8yyKeQL7-DbyZw641WhLpmYSbwlw-4W6cZCdzwoiEdA73K1_FwQz3dzn8N8RxM_HRHSta0o_2gEQy9aaSF2H19MQn_biu4hAc_xvnIlp3gWTj-esc3Mv2s0VzgPkPoF8e_fwQzFOOhAiSZ73Zixi6Vxe2HEoMu2cjN1rsR6YlbFaQm8ZKdpRjiTrt___ZmvWkTNszy84auS0lhQrYy4-9UTAzmCu--XQQn_w0zvfMEV6hsm76Q_9fRa_3dwvoGZoIeDql6dr43pD6HXse73T6pJNpTUrFJ9pD3r8Ak1dydgKn3IBwi19VC7KbXgae_WWl4HjA6CFOW2onfdaEcT3sY-KS9g9pOFb-1EZBxyemRGCqqgz9t6jJxDcA7EWazyk-j76x2YNQThU44vP7JKBnAf8fBjAvjow0-7X0m5h7u1BNFa7lUSplP2v6-a6aUYGONy5FQRrsIipNHUjRGogtHnd1LZQ-oiod9bLNJ9n3YuG6bX4CGU89Evh1hsQMvwPzje_eNL7fdHwNwKO1Vw1cZnClVdl_g6dW0_9NNMimfrYniUmMiL6NJ0y4VNoHwluWJsUt79X8" style="width: 100%;">
                        <p class="caption">CLIP robustness 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 아래의 오른쪽 결과에서 보면 CLIP 모델은 ImageNet으로 학습한 ResNet101보다 다양한 데이터셋에서 더 좋은 결과를 보여줍니다.
                        그리고 왼쪽 결과에서 보면 CLIP모델은 이상적인 robust model과의 gap이 다른 모델에 비해 차이가 적은 것을 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/APg5EOZF_VLTkUsZc4DSx5E85BOvTHbfzSMBkQIQKJONDuWgEXc09dfv5wOgIIAqmCZzOwX5urnVO19iyfhwik65TcMIHntxtwitqdCQkTVI5vI4QT9zTN7DeGcJXAMmokAfHrg366F8AfqIdFiw7EdpKqwnH5GMookHc8QfnH7E-KJBtr5_DBKdcxhoO-Ng-ZsNgA0joxRPIgpC2nH-apsiNpdEc1Ieoa8iQAzP12UOChuBkJdhP2ryWihmwnP2lGARQ3eIW8OLcfbR32EK7R4r3-5NtvWfrbgowgjQAHE5BRNZzlPaI40mhT92BETgE4z4k_dtWUlzjHiI9p7w6k_Y2sou-srZqCZPwP8IuqEN69ID2vnP0N6h8_F-ggCnxUTEVJVhhiC_Da1eZFiQbvfYo23N50T694jmZA281KsJccVH38iZwSUERUxqfHXA8LB5E0tLbl2eJXjSnKhkB_7IhlJJxaW6x1siCuhWW24N_6jH3eQGXUunSk0MEjKHFngfcOPLEa8h705ri384QCXh6zlTO4lCTMoDVDe88LqVMVphZqP8aCNl9mQk-pkiIjoi0tGD8aYtFaBRe_5GXzpOK_B6vx7D4iXspoVx6prd8tmO9lHa7hEbs4icHV4RwCUWyEeIvZVB2xhGkVEuKA3Mf2XZHGrK7Szm-gGMGNwuPPRlauggnQiTtPxNoxnXRJp993MBWdqh8tywmtzjEEwqX4dtH4_w5hul-sPfkw-X37sCQSPkNpLJvUOcHD8k1eXZ2Ppk-4YsAVqwPV6ty406bzhfYrYtv_Jp3ypfEOuZNdp_hleDrtP2L0xNSP7sJVuTDBrKn1v4w0ks_c-3AeluT1hSyqieaOf3igrAe7DoxH6mvlecT_os8GX3nz5MHA-wqTWFI-CLhDwf51gr5m3iiqzFqVCZaAZXzzAeVPtpYfKEVCU8x8YefLIM3RlkLkPk8ts3Ei2FvCVwBqCubYHu9sEXLqUoKPZtf1RcviRUrGy0YaUnRF6_uuOXmKt94hPexJTK31kHeKFQF4WJVP2P0pJjXRUZmOuDQbKi159ki0H8egnT-HsDsolseQdzl0xW4450KyVqEFPGn9f96UYzOZeaUAQGKcSwJHYqKdIkywVXVqq589C_vvR16Jq8SKc93BnPmYQy_D_2Tbui_uMtVrkEns0BheCpcy_NlPuT1BQ9SMSxJwYszqXBNlaUGClhDz24RDYI_49DfOleXbWp_owvBq7btJMJvcgqgNt0v7eNjJZ5OReQY3JMZj3-sGGGXrtGH8dg7KdRZnUEGULERdkMkmK1jRPzP3O87VyKnUNtQVN-XZBAHgilQ-noJSTVBvJE3UIayyKSNJeELTSirq1qExMl9mhTGyibrewGKrhJiS-NHgHKmaG_QsiqfyUjB3cHgYAeFSOaIBlBN3o7KqOg1YevcTso5zkC4iY6_eW4w_KBU_hZonhKYGImIHZGcZYs_Ztrn6n3keHc3cVFpO4" style="width: 100%;">
                        <p class="caption">CLIP robustness 결과, 출처: CLIP 논문</p>
                    </div>


                    
                    <p>
                        <br><br><br>CLIP은 OpenAI답게 4억개나 되는 image-text 쌍 데이터로 general representation을 학습한 모델입니다.
                        이 모델은 추후 다른 모델에서 많이 활용되었으며, 그 성능도 엄청났었습니다.
                        
                        <br><br>다음에는 image-text가 아닌 text-text 질의 응답 쌍 데이터를 통해 주어진 query에 대해 적절한 answer를 데이터에서 retrieve 해주는 text-clip을 제작한 프로젝트에 대해 이야기 해보겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#CLIP&emsp;#ContrastiveLearning
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('CLIP 첫 게시물 입니다.\n\nThis is the first post of CLIP.')" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('clip2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>Text-CLIP을 이용한 질의 응답 retrieval 모델 학습</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>