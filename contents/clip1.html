<!DOCTYPE html>
<html>
    <head>
        <title>Contrastive Language-Image Pre-training (CLIP)</title>
        <meta name="description" content="CLIP에 대해 설명합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/clip1.html" />
        <meta property="og:title" content="Contrastive Language-Image Pre-training (CLIP)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="CLIP에 대해 설명합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/ALs6j_G6IxPqVJ5d32QoIHuZ_miEZHENxlW7IJwof6SnoBlIaC8kyqZWuDuhhRyFX-J5hTCuJaFi7yptZ4vOhMwcfWNZ0gT4JC9ApTxZU0EdbtWpTZLYW87qEIbHQpad6Y4R4sJ7LnZEaK4o3oHSfqadXu0ULQjW7g7P4dRa5KlWemwdEbPnTOYO5HkEavblGSf5upaYT0Xyz-mRpDRvJKBiJ7-yPV8n4JgzB7jBDmcouas8vLo94UDSMrG-fnFexWLEXZkYUGO--2TUTOVhVni0WNeP-NY1hiYjL_gUXQ_2OUA_gnpo2ZESzES4q9c2oeLd9yG-UM0sneOJY2QPbmT8kd5FkFm0EGBlGmRkt6KZniH_tKyBAgYTN2jAHnll1bY3CrD_rm7Jlwe-W6SXuEP0TqQ_9dM_RUmlV8Vw1foFZIkH2sLQPn2wn0mCR_q6ng2U7cxg6rcj0s067-tTOyoMBvnvwikJcxLkGxJKB1DVXgwe76-G2tZMu-m99oKo6ls_Z_EVjw5BMbhKnTPh5saF-2I3pIuIK6YBN1u1Kp-W1f3sw9LwyG3JEl2Qlg3NqTBpOT6VVoJOQyl2LX4AKGx-0WCR80LZWu9B4lOhD44vqrYmEVjJI5Sbn8KT1BkPruhfBG6gQuMClMBJ0RyFY9kCJmXB3PyugbO7xrMslgLEJeLjoa86xnp5LE4k5BldsSDkyDf6v3rcbHQRQiHlFhjptZqu6uj3ppKqt3kRlTglUZ7R_7AMZkp_GCvQqaPGTH55OXSTMMpqg-SDDplDiL4WlWDMqbSHmYDlwGzdHBM4FoIlNYb-sJoAEwHg8Mc9I45jNrxraAEZn1FY6agPJ426fJIuaiusnJ2mV6dvCKpnwfKWRWRjhQ0oO-R3QZBwshkulZzUnTgz0Jf_i-uSR1mRBnhwKoRynA3HJkKUeXPt4unktMMZb1j3z7yTJUe9bwq6hVh-wqBRndLiI1pFSqNLUJ90rkyktqSQWocX4Y5FEOqHIp2aUYexl2nDMjfIfdyMbpD_1xjmtk-ZUM0QB_bT1MV507jS-RZdoQEHlTmH21zRNecxWPdxQOH1M4kkr9B729PWfu_ThNkUTxapnC5H5_gXi6Oo9D51pjL2ApbGXFNeVFHGrT-pYogD7deNnVVPY1st23KVjENCQiWn3_Tz2oGQDP11yhKvf-OVluccC9GwNRlGvV7idb4Tt-lHTEEktvnmF9pxcERae-6KMpHDVkarb5XJQbVWzrcSUZzKtUWteqqW0bqqo-g1K0lI1oUYE3-7MUG4LBMSPESJfagoO7vpRrRjM6ZegA3tAedTY5PmTTg_QQcMDhNE1IZxOfL_R7z7O7QwL6MuSuw5VoB-aUMjB5OgnjBZeNyR1sRdKg5RMdP-escSw4ZAmCxWQKym42bH1IxgXcHMzAdPoDlL_kEBONkEcv5ACJqzqA2wazPraAQhNOtAUZcUMR06MjpoXLI_ASo2szlP7LJ-gA0w884gNNROQrZ33uTXf6G0iNdUx5UQOIZOZMGpsMTnCdSPuCb40PApqg" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Contrastive Language-Image Pre-training (CLIP) / 1. Contrastive Language-Image Pre-training (CLIP)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/ALs6j_G6IxPqVJ5d32QoIHuZ_miEZHENxlW7IJwof6SnoBlIaC8kyqZWuDuhhRyFX-J5hTCuJaFi7yptZ4vOhMwcfWNZ0gT4JC9ApTxZU0EdbtWpTZLYW87qEIbHQpad6Y4R4sJ7LnZEaK4o3oHSfqadXu0ULQjW7g7P4dRa5KlWemwdEbPnTOYO5HkEavblGSf5upaYT0Xyz-mRpDRvJKBiJ7-yPV8n4JgzB7jBDmcouas8vLo94UDSMrG-fnFexWLEXZkYUGO--2TUTOVhVni0WNeP-NY1hiYjL_gUXQ_2OUA_gnpo2ZESzES4q9c2oeLd9yG-UM0sneOJY2QPbmT8kd5FkFm0EGBlGmRkt6KZniH_tKyBAgYTN2jAHnll1bY3CrD_rm7Jlwe-W6SXuEP0TqQ_9dM_RUmlV8Vw1foFZIkH2sLQPn2wn0mCR_q6ng2U7cxg6rcj0s067-tTOyoMBvnvwikJcxLkGxJKB1DVXgwe76-G2tZMu-m99oKo6ls_Z_EVjw5BMbhKnTPh5saF-2I3pIuIK6YBN1u1Kp-W1f3sw9LwyG3JEl2Qlg3NqTBpOT6VVoJOQyl2LX4AKGx-0WCR80LZWu9B4lOhD44vqrYmEVjJI5Sbn8KT1BkPruhfBG6gQuMClMBJ0RyFY9kCJmXB3PyugbO7xrMslgLEJeLjoa86xnp5LE4k5BldsSDkyDf6v3rcbHQRQiHlFhjptZqu6uj3ppKqt3kRlTglUZ7R_7AMZkp_GCvQqaPGTH55OXSTMMpqg-SDDplDiL4WlWDMqbSHmYDlwGzdHBM4FoIlNYb-sJoAEwHg8Mc9I45jNrxraAEZn1FY6agPJ426fJIuaiusnJ2mV6dvCKpnwfKWRWRjhQ0oO-R3QZBwshkulZzUnTgz0Jf_i-uSR1mRBnhwKoRynA3HJkKUeXPt4unktMMZb1j3z7yTJUe9bwq6hVh-wqBRndLiI1pFSqNLUJ90rkyktqSQWocX4Y5FEOqHIp2aUYexl2nDMjfIfdyMbpD_1xjmtk-ZUM0QB_bT1MV507jS-RZdoQEHlTmH21zRNecxWPdxQOH1M4kkr9B729PWfu_ThNkUTxapnC5H5_gXi6Oo9D51pjL2ApbGXFNeVFHGrT-pYogD7deNnVVPY1st23KVjENCQiWn3_Tz2oGQDP11yhKvf-OVluccC9GwNRlGvV7idb4Tt-lHTEEktvnmF9pxcERae-6KMpHDVkarb5XJQbVWzrcSUZzKtUWteqqW0bqqo-g1K0lI1oUYE3-7MUG4LBMSPESJfagoO7vpRrRjM6ZegA3tAedTY5PmTTg_QQcMDhNE1IZxOfL_R7z7O7QwL6MuSuw5VoB-aUMjB5OgnjBZeNyR1sRdKg5RMdP-escSw4ZAmCxWQKym42bH1IxgXcHMzAdPoDlL_kEBONkEcv5ACJqzqA2wazPraAQhNOtAUZcUMR06MjpoXLI_ASo2szlP7LJ-gA0w884gNNROQrZ33uTXf6G0iNdUx5UQOIZOZMGpsMTnCdSPuCb40PApqg);">
                    <div>
                        <span class="mainTitle">Contrastive Language-Image Pre-training (CLIP)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2023.04.21</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번에 소개할 논문은 바로 OpenAI에서 소개한 multi-modal 모델인 Contrastive Language-Image Pre-training (CLIP)입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">CLIP은 텍스트와 이미지를 바탕으로 multi-modal representation learning 입니다.
                        기존에 이미지만 가지고 훈련하던 supervised learning의 한계를 극복하고 대용량의 데이터를 사용하여 놀라운 성능도 보여줍니다.</span>
                        실제로 CLIP 모델은 이미지 생성 모델인 DALL-E 2에서도 사용되기도 하였습니다.

                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/2103.00020.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">CLIP 논문</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>CLIP 모델의 계기</li>
                            <li>CLIP 구조</li>
                            <li>CLIP 훈련</li>
                            <li>CLIP 결과</li>
                        </ol>
                    </p>



                    <h1 class="subHead">CLIP</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>CLIP 모델의 계기</span><br>
                        <span>Motivation of the CLIP</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        CLIP의 계기는 간단합니다.
                        BERT, GPT 모델들은 자연어를 기반으로 한 모델입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이 모델들은 어떤 특정 label이 필요없이 단순히 raw text만 가지고 학습이 가능합니다.
                        그럼에도 불구하고 이 모델들의 성능은 뛰어났습니다.
                        즉 다른 말로 supervised learning을 사용하지 않고 senmi-supervised learning을 사용하여 학습했음에도 불구하고 fine-tuning을 통해 여러 task에 훌륭한 성능을 내어주었던 것입니다.</span>
                        이는 NLP 분야에서 task-agnostic한 모델 연구로 이어지고, ChatGPT 등은 이러한 task-agnostic 모델의 대표적인 예시라고 할 수 있죠.

                        <br><br>ChatGPT가 나오기 전이지만 저자들은 이렇게 supervised learning을 벗어나서 학습할 수 있는 방법을 computer vision (CV) 분야에서 적용할 수 없을까 고민하게 됩니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">왜냐하면 기존의 CV 모델들은 labeling이 되어있어 classification task로 국한되는 것이 대부분이었기 때문입니다.
                        그리고 이미지에 대해서 CNN은 아주 강력한 성능을 보여주긴 하지만 이 방법을 가지고 학습을 했을 때 zero-shot learning에서 매우 낮은 성능을 보였기 때문입니다.
                        뿐만 아니라 이러한 기존 classification task는 아래 후술하는 데이터의 문제점도 가지고 있습니다.</span>
                        
                        <ol>
                            <li><b>Labeling의 수고로움</b></li>
                                먼저 labeling 자체는 어려운 task가 아닙니다. 다만 수백만개의 데이터를 사람이 하나씩 labeling을 하는 것은 엄청나게 큰 수고로움입니다.
                            <li><b>정보의 빈약성</b></li>
                                데이터의 label은 말 그대로 이 이미지가 어떠한 카테고리로 분류되는지만 나타냅니다. 이는 이미지 전체를 나타내기에는 그 정보량이 빈약하다는 문제점이 있습니다.
                        </ol>

                        <br>저자들은 이러한 여러 문제점을 아래와 같이 해결합니다.
                        <br><br><span style="font-size: 20px;"><b>1. 대용량 데이터 문제</b></span>
                        <br>먼저 CV 모델을 학습하기 위해 이미지는 필수로 필요합니다. 다만 단순히 labeling 데이터를 통해 학습하는 것이 아니었습니다.
                        따라서 이러한 task를 수행하기 위해 여러 종류의 데이터가 고려될 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">따라서 이미지에 캡션이 있는 MS-COCO나 지식 그래프, 객체 정보 등 이미지에 대해 다양한 설명을 하고 있는 Visual Genome 데이터를 고려할 수 있지만 이 데이터들은 데이터의 크기가 작았습니다.
                        보통 많은 데이터로 큰 모델로 학습 시켰을 때 좋은 성향을 보였기에 이렇게 작은 데이터셋은 부적합했던 것이죠. 그리고 1억개의 데이터가 있는 YFCC100M은 물론 데이터 자체는 컸지만 퀄리티가 좋지 않았습니다.</span>

                        <br><br>따라서 저자는 직접 데이터를 만들기로 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">웹상에 있는 대부분의 이미지는 설명글 혹은 캡션이 함께 존재합니다.
                        즉 이미지 데이터를 labeling하지 않고 이미지와 텍스트를 쌍으로 가져올 수 있는 것이죠. 따라서 저자들은 웹 크롤링을 통해 약 4억개의 image-text 쌍 데이터를 제작하였으며, 이 데이터를 WebImageText (WIT) 데이터라고 부릅니다.</span>
                        
                        <br><br><br><span style="font-size: 20px;"><b>2. Label의 정보의 빈약성 문제</b></span>
                        <br>위에서 한 번 언급했듯이 이미지의 label은 그리 많은 정보를 함축하지 못합니다. 그리고 labeling을 하는 것은 시간과 자원의 소모가 큰 일입니다.
                        이러한 문제점을 해결하기 위해 자연어를 이용하여 학습을 진행합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">자연어는 별도의 labeling 과정이 필요 없으며, 이미지와 동시에 자연어의 representation도 학습할 수 있다는 장점이 있기 때문입니다.
                        그리고 위 1번에서 언급했듯이 이미지와 자연어의 쌍을 수집하기에는 비교적 쉽기도 하기 때문입니다.</span>
                        그리고 이렇게 이미지와 자연어의 multi-modal representation learning을 수행하면 다양한 task에도 쉽게 적용할 수도 있습니다.
                    </p>



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>CLIP 구조</span><br>
                        <span>CLIP Architecture</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제 4억개의 image-text 쌍 데이터를 수집했으니 pre-training 방법을 정해야 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">원래 저자들은 VirTex 모델처럼 CNN 기반 image encoder와 transformer 기반 decoder를 이용하여 image captioning 방법으로 학습을 수행하고자 했습니다.
                        다만 복잡한 transformer 언어모델은 아래 그림처럼 같은 accuracy를 확보하는 데 다른 모델에 비해 그 효율성이 엄청 낮은 것을 볼 수 있습니다.</span>
                        실제로 최종 선택한 CLIP 모델보다는 4배나 비효율적인 것을 볼 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이렇게 비효율적으로 나타나는 이유는 image captioning 방식으로 모델을 학습할 때, decoder는 정확한 단어를 예측하려고 하기 때문이고 이는 실제로 데이터에는 비슷한 이미지에 다양한 텍스트가 올 수 있다는 점을 미루어보아 느릴 수밖에 없습니다.
                        또한 하나의 이미지에 여러 텍스트가 올 수 있다는 점을 생각해보면, text decoder를 통해 이렇게 정확한 단어를 예측하고자 하는 것은 'representation하는 데 이렇게까지 필요할까?'라는 의문이 듭니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_G3R6W18kSMegkKVHi42JtrbwBhk-gpRSRazmVQUtQ2HZScKiCXtdlx3M46pg2XDVYZzCtTCEiFHNZWf4ndmZu5OoNlCsazUO_lO0xnWz9wcbbSHwZZ5fiUw4s_-zOA7CCZE87Y_nvCkTHmDAc8eNV_V6r-xW-zk_ricepPM6ufWfz-Upz8vF8EzM2tWDh2VHW75NPCt28sFS0ECLQrPL8X6TqUEgr27bvdRYWGYfBd9eM3gQm-Sa9fgaHrxwgi6zyJ52JUKZ5rXsnhDySsx7WY4d292BCEKfmj-uL548oKDcfqyV2eGXogKw8HAQJs-RderTfI4zF636mZhibxHwtIMY_uQZ40Q2bSYjPuPje_rEiAAT0Qo2AzdPS0wFeTsGSHwNnwe6J3Vh-5rZjU9NAKO3a8HyRgFGIXv_H1EgeuvIm9VZOl0E4I52lKDEz8aFm-rKr3SspvhB3mHw6Snf0xJOUlNKHmSS0RUT4QgJBqiIsxpwpbwH79EiifSLLrRQLS_faTIVHo0pBwhxaBQM7aYmCk2FaULA6irvK0I_tuO2XZHtK9q3vH-Q8xsaUz_y2Pv4Q5mJuR7t-c3-pzGi_6gomr_ypAsjmHREjon-OcdqTB-1Z7lfL26hUJYD7PfdDBuYB_i2YlHp414iVWGmu1Z9U5tZXhSsD3Op3jPM4Xtj9x6cbFaicnsJcf0ZBXa-XdM4vXSZqmMGyLsbYF8HY0pOaTV3giKb63cJRMjecTGaNHjZv-WQXJ4JCBk0AwNBOTb3l-Ew0wR-e3scziqL5roPe-GMaChbSvgNUgfLTe6obj0yNWfd06cp5SE_6KrvaoPIv4xMLoAi-EZ-OxqzxzjP6IAF6Cl3H_NWYhPu8BUijZELKxyCstU0BgAfnh8YPhDNRd_ctblF-8AcVJu-5is-kZry8SEoG6sTtrAPEd9fu8bTjwQhYdFiGcVQqM2wNOlzmXvmRXhNQ6VGeLiNyBUnxGok2FdY0236x_-_fRvFSkq2Uan3TWOtiB_XiNWy94CAmU6Wobz69bMn4y02xl6NOfT7id_csOjuIP3Yfh_gZ-v_PRBwpcuUA_swr5w5WxIPbZ1Y2qFZWqQF8jO_wc5OkUJWJRPcfp--0blWYJ5zi-M4m_t-QJ-DuIToEN85EIgz1TaXGGanSKrlnMHygv4jcMEG_TzPbVua6-yCCRcQD5kVeb8dKaJYOLvX82QPR6G0gIrx1JOAg0Yu-THD-NTDM-1kJFW1hIYyv3rhkhg2YVLNHxF2PqC0fwR_MvuBjtWVDBnhjwR8xImrTIXkS9LAAMVStN-Dj6ZaorlCSnR0sKe5xU--sujkE8alDx8JJsrqPmT2zpYwRYdtPjq3PcXi3E85m147QZo6wIf3a_USJYUPrfREAGoO_gDwHovFVY2FPqhqkZvmpwwKNGTbBn2FiZqiSaHqriJmb4gv1bm3znt_c4wBFT7vELGWeWSZICCpFoLPsQx7P6rnHXkLBoDjmuWenciPO-zMYalJgdGBc1Ixm69v8FI_j_qxM2eZFgyzsAXnzY" style="width: 80%;">
                        <p class="caption">언어 모델의 효율성, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>이렇게 너무나도 비효율적인 언어 모델 때문에 저자들은 contrastive learning을 사용하기로 합니다(contrastive learning은 실제로 현재까지도 많이 사용되는 기법입니다).
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 contrastive learning을 사용하면 아래 그림처럼 하나의 batch에 \(n\)개의 image-text 쌍이 있다고 가정하면 총 \(n\)개의 positive pair, \(n^2-n\)개의 negative pair를 얻을 수 있습니다.</span>
                        즉 하나의 batch로 두 종류의 데이터를 얻을 수 있고, 총 \(n^2\)개의 데이터 정보를 얻을 수 있다는 점에서 데이터 효율성이 엄청난 것이죠.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_G6IxPqVJ5d32QoIHuZ_miEZHENxlW7IJwof6SnoBlIaC8kyqZWuDuhhRyFX-J5hTCuJaFi7yptZ4vOhMwcfWNZ0gT4JC9ApTxZU0EdbtWpTZLYW87qEIbHQpad6Y4R4sJ7LnZEaK4o3oHSfqadXu0ULQjW7g7P4dRa5KlWemwdEbPnTOYO5HkEavblGSf5upaYT0Xyz-mRpDRvJKBiJ7-yPV8n4JgzB7jBDmcouas8vLo94UDSMrG-fnFexWLEXZkYUGO--2TUTOVhVni0WNeP-NY1hiYjL_gUXQ_2OUA_gnpo2ZESzES4q9c2oeLd9yG-UM0sneOJY2QPbmT8kd5FkFm0EGBlGmRkt6KZniH_tKyBAgYTN2jAHnll1bY3CrD_rm7Jlwe-W6SXuEP0TqQ_9dM_RUmlV8Vw1foFZIkH2sLQPn2wn0mCR_q6ng2U7cxg6rcj0s067-tTOyoMBvnvwikJcxLkGxJKB1DVXgwe76-G2tZMu-m99oKo6ls_Z_EVjw5BMbhKnTPh5saF-2I3pIuIK6YBN1u1Kp-W1f3sw9LwyG3JEl2Qlg3NqTBpOT6VVoJOQyl2LX4AKGx-0WCR80LZWu9B4lOhD44vqrYmEVjJI5Sbn8KT1BkPruhfBG6gQuMClMBJ0RyFY9kCJmXB3PyugbO7xrMslgLEJeLjoa86xnp5LE4k5BldsSDkyDf6v3rcbHQRQiHlFhjptZqu6uj3ppKqt3kRlTglUZ7R_7AMZkp_GCvQqaPGTH55OXSTMMpqg-SDDplDiL4WlWDMqbSHmYDlwGzdHBM4FoIlNYb-sJoAEwHg8Mc9I45jNrxraAEZn1FY6agPJ426fJIuaiusnJ2mV6dvCKpnwfKWRWRjhQ0oO-R3QZBwshkulZzUnTgz0Jf_i-uSR1mRBnhwKoRynA3HJkKUeXPt4unktMMZb1j3z7yTJUe9bwq6hVh-wqBRndLiI1pFSqNLUJ90rkyktqSQWocX4Y5FEOqHIp2aUYexl2nDMjfIfdyMbpD_1xjmtk-ZUM0QB_bT1MV507jS-RZdoQEHlTmH21zRNecxWPdxQOH1M4kkr9B729PWfu_ThNkUTxapnC5H5_gXi6Oo9D51pjL2ApbGXFNeVFHGrT-pYogD7deNnVVPY1st23KVjENCQiWn3_Tz2oGQDP11yhKvf-OVluccC9GwNRlGvV7idb4Tt-lHTEEktvnmF9pxcERae-6KMpHDVkarb5XJQbVWzrcSUZzKtUWteqqW0bqqo-g1K0lI1oUYE3-7MUG4LBMSPESJfagoO7vpRrRjM6ZegA3tAedTY5PmTTg_QQcMDhNE1IZxOfL_R7z7O7QwL6MuSuw5VoB-aUMjB5OgnjBZeNyR1sRdKg5RMdP-escSw4ZAmCxWQKym42bH1IxgXcHMzAdPoDlL_kEBONkEcv5ACJqzqA2wazPraAQhNOtAUZcUMR06MjpoXLI_ASo2szlP7LJ-gA0w884gNNROQrZ33uTXf6G0iNdUx5UQOIZOZMGpsMTnCdSPuCb40PApqg" style="width: 100%;">
                        <p class="caption">CLIP, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 아래는 좀 더 자세한 학습 코드입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Ga8tHgUgnkKL_v7_ZMOty0JDtJC8W6RrfpJ40ZTjaZZxACdPrjbFldxeneIZB41vDgLakPZu_u8wWb7LIlgJumvpw9K7nrdT8Vu6rEq1adXiJS_GhUQp4ND5g8bzS4O9T2dBj5QTfKF4AuKf9pYPLrCyrHBSLmI_5eOv7UTaqSHETscBkQVVwS82hkxbW0AnyClN6Gqil5IueIFjO3sxsOlsh3vNa0Cj_nIxepVg51mQgBHIaQKTQQfc9n383fNs8eWk6yK3AaTt6jeKz-EesQU-gHJ7nBgX6JqgO-wn8nNvyyG9LD2uIt4n6i_v8FEQQ2W8p7mab-lPEaaktXnqtdwwC_JHSyrj2r8yw6qDk9A9nqQBXJqEMuG_rkCbM7VW7Rc3KxnIeOMKhOjwVFJF6HrEu3GgjwJHkWh7X1zJ45u8TcZCdMZh1wsll0cl-MWuFLtEYYYSiztwzcNxkCehlKDw-S6voEJQHt1IlJWlR9PbqGn_EAzadsvhGN-uj3Wt3aS1M_Z7URTL7_UcB1NouEA8Mxs-AjqIlKXslQ9S2gjHA9VpMb2gfHUwaAMfMxcKNu6w1EpaGKTMYv6J6-FCsYVSL4Cr4HMVrNdUItGpGgwi_FOdu26br6zKb0R0zIHl3Z8RlWGd7scLhL5bO3n9x6iLx1Y8HeRzCJlfnDmWYePNbGAkuF189A3715BBN_Ou0-RtK-8Gw0sYC_QfHF3_e2vn7l1p2XhrN7x0tir1yZBYh4e8WcOgbcm_eK7OvkIt5enbdRwyY-TIepvkQYjCbO3Vn1tnWoKUzjSmkX2J0aqeZjJSkqsAQdFAyA9fVdNNbd4273MDIRG3Spa71hpwFhsTqngL3hT1RTuZaO6BD4s1jm5_liC8iOIKNJ7l0_9LeOJ8EiXEt8kvYvP4BpKNZROCA2AoFG9EYQsyZr_COV3ttEMurQdNplef4DpqDqtJZ_4Xyh480rertFAjOcyEpwzXVdfS8sOY15KNHMh0w9k14CbXQtVkL47ItUCARUfdsIgv_63SGC655W684gae_icGeec11Prr0jylj6Q3Qds9FWlpX8Mji6_JmK3WkBP4OfV9BOrKgIRk0QbEITYysUxxvn3RfqbjSlQFqQduJczQxgLy4xr67svvUGXFrzHwGBh4VSjVNbiZU16HMtYOZ9_O8N5-2Dtw69BjOtS8mv8jTaJ-9t72Ddiz28z81hjkdINfgH7QqVDswc5hjQGZWmqclq8V4pYP5M5VRY48YEWEa5JpDyLEdNCnrt8fheiR5t3zjBrrpgdaJwtqcVCi1iVuDP-TbyqH0ewmkI6gU-K3Z33qedBldtPdVXJ_CqTtxgzi9M71RhuT6ECm87OZMz4nX43puC4w4PsqBPemPOw_MM2fAt4V8y80hU2htCG2KquR3uTXPffBjQ9GpoQHic-fTetfmau1b-UcrmPxNeyGOUxTmemo1z07VtlqWd2iLyA9juhvRC31E5JaKX7NE5Vo_4C7lcZuvqgfvkRUPG2MduxNORANkavCj1YgUMIBHGJTlTsXv2" style="width: 80%;">
                        <p class="caption">CLIP 학습 코드, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>먼저 이미지와 텍스트의 인코더로 어떤 모델을 사용했는지는 좀 더 아래에서 살펴보고 어떻게 학습 되는지 살펴보겠습니다.
                        <ol>
                            <li>이미지와 텍스트에서 인코딩된 feature들을 추출.</li>
                            <li>각각의 feature에 가중치를 행렬곱 해준 후, l2 normalization을 수행.</li>
                            <li>이미지, 텍스트 feature를 dot product를 수행하고 학습 파라미터 \(e^{\tau}\)를 곱해줌.</li>
                            <li>그리고 모델 그림에서 보았던 곱한 \(n*n\)의 dot product 수행 결과의 diagonal 부분만 1에 가깝게, 나머지는 0에 가깝게 학습.</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">이러한 방법은 같은 쌍의 데이터의 dot product score(코사인 유사도)는 높게 나머지는 낮게 학습하려는 과정인 것입니다(이러한 과정에서 이미지의 label은 필요 없음을 알 수 있습니다).</span>

                        <br><br>저자들은 이미지와 텍스트 인코더로 각각 아래의 모델을 선택합니다.
                        <ul>
                            <li>Image Encoder</li>
                            <ol>
                                <li><b>ResNet-D</b>: 기존 ResNet 모델의 global average pooling을 attention pooling(QKV 연산을 진행)으로 바꾼 모델.</li>
                                <li><b>ViT</b>: 기존 ViT에 layer norm 추가한 모델.</li>
                            </ol>
                            <li>Text Encoder: <b>Transformer(L: 76 ([EOS] 포함 77))</b></li>
                        </ul>
                    </p>


                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>CLIP 훈련</span><br>
                        <span>CLIP Training</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <br>실험에 사용한 모델은 아래와 같습니다.
                        <ul>
                            <li><b>Image Encoder</b>: ResNet50, ResNet101, ResNet50의 4, 16, 64배 연산량에 해당하는 EfficientNet-style 모델(총 5가지 모델) + ViT-B/32, ViT-B/16, ViT-L/14(총 3가지 모델)</li>
                            <li><b>Text Encoder</b>: Transformer</li>
                        </ul>
                        그리고 아래 결과에서 후술하는 CLIP 모델은 모두 가장 좋은 성능을 냈던 가장 큰 모델
                    </p>




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>CLIP 결과</span><br>
                        <span>CLIP Results</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>1. Zero-shot Transfer</b></span>
                        <br>저자들은 모델이 representation을 잘 학습했는지 확인하기 위해서 zero-shot task를 수행합니다.
                        Zero-shot task 수행은 아래와 같이 분류 task를 수행하는 과정입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Hgm_LU2iToKrdt4FdRohBv9GpbKiRf5qDr5IxkChQ_FPIETv9JWfxE-Y_WjRfqalk0-juYNLDeArEq5rt3DrmDJ69RFdSmVTj2hY8cRed7kMim7xUXyPUR5J2RuvP9ybjBuuKgEwrFnKMEcUGG3PYhm_Cfh_m3YzPEQENBli3x0m3-jRC-CdYpFbYcO3alvvPG9cZDipYnKBrYwtUlQskAV4DacfQBlSqulqv9WDI9jzNHodtkN1lykfHoq83yH-2bPPj_DdDfo-aCB8UFGfEkGBkILbo8ANlK1K6HMne7O06FS_CQXjEnRvnnQIM001NNQuIIZ6Yrd8KMsPmHevgFPakjSqb-Iiy0t2FeE07PBFvbSU4ZxAr1eeK1u0mQbVv1_b70O14UGccMD21nNzN16fAq0qqSBudA4yCjq2pRgDH71i7ZjLmaM1JZTXfwgGaFsjLSv3npxuFH2lJbtBktzebCSPc8Dq_-2qMDM10JTkAM27b91La0mPnPh-DIyMjXwlrWchd0IeBZqiUzjNV96oW4qpGkfX4BhY4JGLZWi-9HUkTSZCyPkxd44HtC89QYIqyZPYZnRmkkDphPqfR3ijvn2P39sOApK3Y2bKjL4-B9xxOOtxIkUy47J0s3xIVLZmtrqJ2IM80GnIQX8KQvLSlfuepxa3FSJG4eMWpG6LShkyb7NluOBGKMc3dNIlRt6ovGpMzqwhPmHSW2UY1No7Idn6983myzTBhlDH4DK1iFOScPubnGCRMT7ODhODD9pWRVvt8LUpg30CiMKw7FAX8-6Cpr4Aoyoh0f9dhILw66A1lCTHkj2bNPhSWOpO4bZIHuKxCtkh5Q299jQQmp3zWRcUhMuxCIhYdyxMP-rVqhYJlfh8Lmo7sY0ZPOsFSllM6FOaaoVqgxTJWO4sycM-e8KuhmDIqIc0yiRMm-zMP7Jz0ClhyvpmDoMpJiAOh2LhEaR-JtE52kJa6Oh1vDTQQe7wsI8g3OA0ldLGjvLGH1Lscx50_i55p5xl-VRz6jFLAhEkeQITHiUPbgJDKpalt6ukcowsM32JA_0fHZq9gGxl8cWzJsDSvmuD15u2fqi-9bbkD1BQZxEbKT7YpL9Wx947M79ZKpy1MvORE5FddPKSAPNDvKN7uBK5JbHUoZej3w8C79vtPCyn_8GBOjCR1wZVhz4qDNkk-2xefoEyj1-j7Ae2FBqyPOkP2p0IhcXfCeQQgqsjPsHFe4EPbOpehbNWUgDUcaW1eRz7SsNYRMXRxt5_W4nKH09kFzgvPpfHdAWQH0gX--YFDcqDDs_0GQMRemprvWpaBkHq0GO_WKdFaNkU6lim9QJm2oe_PnnQT6rKCbJYymyERKgZ1WOtRrtxhB8n1Ndh_2y3CwcUKcJPFUcbVe-ko3ygNo4Rmu1CVN8taGImCs4HHfxA0m2hjDUm0OUtzuk6McPfgDmZluAnNK-3NQNJI9vQlirY2vt5nYk3yNA1OurxacH588NGtOvEPv1P_F2kDKmb-KlhNvez2aiI1eMcKSkwAAkzV8s9Joxg" style="width: 100%;">
                        <p class="caption">CLIP zero-shot task, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">CLIP은 분류 모델이 아니며, zero-shot이기 때문에 한 번도 모델이 참조한적 없는 데이터를 이용해야 합니다.
                        이 과정을 수행하기 위해 CLIP을 이미지와 더불어 class를 문장(프롬프트)으로 넣어주는 방법을 채택한 것이죠.</span>
                        그리고 이 task를 27개의 데이터셋에서 수행한 결과 baseline 모델인 ImageNet pre-trained ResNet-50의 linear probe(pre-trained ResNet-50 이미지를 분류하는 top layer만 학습시킨 모델) 결과에 비해 16개의 데이터셋에 대해 더 좋은 결과를 보였으며 STL10 데이터셋에 대해서는 SOTA를 달성하기도 했습니다.
                        하지만 일반적인 분류 이미지 데이터가 아닌(e.g. EuroSAT 등은 인공위성 이미지 분류 데이터) 데이터셋에 대해서는 baseline보다 성능이 떨어졌으며, 이는 어찌보면 당연한 결과일 것입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Hc6erAwOTBOyGufn_EzY5ys4W1_GRtF9xuuHRixDc0UFaWjOidEP2GKANsbz3CPKUNGAf2HSgEaXfp4kdm6pKXI9F4croxPbEcUQGsXZ9bSer5wFAKv2KmW9FIe572xchdt3FaM0ilEspUNrjPOHPchRw-HwjqFZuN5GtUixC0BvZAqgNbqVxG72g0_PeTC-rI-0Z1pUvB2AFilMnPEtVGN0AQJEtKpb-4toi5VvSQ13MfdxB97Bf67reJgEYA4M67Suyh5mQTHlENLF4CjFuEEB9-mD3iU0mY7HqgmkOburUWtxYT5X-rKZ5XoPdy9hvjeNX_5XIe6T5ATC1hyMoPNDUbgf6B6otTdxuM7pGdpc3wNJP4b49mN_LdbBBmtN_kEyAosMsNj7KVPIYjp2joaUQBswMKfsVpQ2Id7_EoLfp7EY47nxyeC1EUH3jrrryHAa63WqlLnVdjs6LCLbG1WRcXz5WGYLsa4FsbwJ2ySaHt_pTbVtR2XThb4cS_8CgQUy5p0NjkCj9FDi-xU551Plo_3LWJ8NT8BZEiFcWCkSoQplFtEjos5LfDnq1yfWmxWPCCITEWMX9Q0nx_tjJrqZlJekCqWpoL_clJ8g6JKXgERSAE5GIk6aYCRSe2wSZn5xBqNZVc3kiuMjJFTM79RET7MwA5mrd4LFDVGqo2CuMBGxjiXqjdOVBWvOiWzMQT070i9PfopY3N80Aj9Xuw0IOeaADqfLZBz-GRSAQGiiqb6UspKFdsI3FSkLnHhOppwl1Tq7tHiOHLDJWkKXOHd2DMmhmGy8dy-5qfeFWMsBstn7KxoG-ORszkXNHz7D1JI0wK3-NPOvBhtJt6jKMGvgTqryulp0tcMmAHzt682FNah0mC6MYqps_DtOfKYj7lQfgChZSccdhq8oLCxhIjcAXaYF4OpyyGXspQg5K1BJuvOjpRZgAr8OQLBISTtdqJJJDSPByKz9-wheSSFAvBLkSGQYnSQEB9N9F2LOnOpX2vuf4a-FaJjvXtJZxNkhhgRkP6N15-SpRobHQGB9CyPX6jyYgD_y90oTGU4i3bEaAzcgnGe2RxymLC1QzN0wiGeICCPiShr-vztAy2wZgdEIHTf-v_uuvubxqGQh58cqDn576tBeo3hJLwCVTKvBInCFyBeBN_o7vi3AEg599_Ba3-8j0jvIn863ukuAtqZ5YiUSBqI7ypBIdaMAtxZlvMwuIPt1_lYT6IRCOLEvmR3oG9MistagfMLyr943ZvAiWB7NMfAvmZpV7aPFBO5m2wCl3M3ZlYMdex3cAd5oSYT0izQOziiV3inEwPnVrMMdhsl8Vr8WFlsgA20sBpDIYgtroNxLiiYDKKqmPDKrbUD6ukP-bPEDmHk1XIcUFojMToYo1xgQyMndun1fS70ZWsTW8KRCobKRR9YqJJ0HvvFbwJP5I25DbBtCQhvUMgd24sHdYJyQkE-KvNsApAbo37As9-iW_lzk8elCuxWkqX73E6rItUmS7bisaxB9Af-WrjATPeal2Nhw5_tZ9OUn_OYSmmmyx8" style="width: 80%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 놀랍게도 CLIP은 다른 모델들의 few-shot 결과보다 좋았으며, CLILP의 zero-shot 평균 성능은 4-shot 일때이며, 이는 다른 baseline 모델의 16-shot의 결과와 비슷한 수준으로 아주 뛰어난 성능을 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_H6pQyXqUnG6IIk6drb2qJKAhb-KFQDBDRYtPx8rXwsJ5ylCO8-F98i5EtuUlC1f8Bn_yD4RjP94nfb3NRsroXx3ukeMNPi-ROLwFG-fESSRmYNWM3QlkmYmJmq2eJ8n-VaqeQ1Ro3SbYVxDjk3lcc7HYx8Pny2zm9ppfjTbdYYg-9qnLYmKE0LztwdqAN4jk0cdKweJdfWlOU8D76oRNSXxzC0jJH4ymBLvHd1QrgdClWn0gjWJEA9rR8J4mA4cB89cNY-7hzmqeL83JBaOEjcP8aD3c_ptw_0cAQZlXG1xYSViektdTJ_TbqRPUkzZ7h-3mUNEiDTEG4SdetVVMHt8IcCLIoEd6InizmQA3WoS1hi5OEkIApiDpAPTw40Lna78Yaz172wTHl5VEld6eU1FWNfsXJOoM1xp5kgeXWIzd1XGdRMWJ-qpNF0RhtIQtu9rz4w3OTlMa5uvWZwXWfdqdfrjdbpNf_sVqbNLx1oMtxFbqryXZqUvpx7uw2xdg5eOtSH4JviNQE5cn5b4sKmTd8k-0EbV32rTfCPMvk6zcnv4Dze3rNyPALYzifBRJM6nxJc-nTYvpr01km2vekKVekP3l_SJu6MDvFdbicp48cCPBj2u5vR_hLz_aw6OzhhfBv2LLVbIRWvK-D9gcfYnBUwq6idF57bvpR-AmsSErw9Col1w5ulU78uzC21KiDDow8VCmn4WcZfY_dWaQL3r_T-WvsDPzffiSbkO1xxk53GOhRq18g2g_RLJZs8uNEZm1vAdnZxTixVDODULSYf1Jb8f-490zUNZQsQI9Z5Jh1w-s90oxghtsQUHExvhyPtBFUx-5RBqgsWUTWfnGE5-ETwgtYlCfJMtxtNkSwKYy8aVlq__x-lgMIwqVhhFfsaDl-d2BNAk6Nhna8qoh63ua-jmv3cIHwOpGs_U393nfzncAASxxshomfLZOYvUFntyS9Q0yxNHVBdSjuJO9krHpqbpUld-IqBtAJcWEz4_g_ZfXL_YoRyCO5aot3mPkJ1N08h99vwVStpgxAYbFSWJJt2sUkWcHAeVhDCZcyX1Mhmv9sePWXZKeOYmchgTuG2ZrB1pN7aymAWBDwCbsKE57PBHL0P7QUw1tgxUK3TjuhpjkemJTwAtLa3ArZvjRMIbO4OZMEguKkhkR39nid92DViU16RZT8sKlYL4SBc-ITRR1Vzs1ex9MS1aJNgut5enymodqTKK70YSwpddfU04AA3paevhxVLS-amFrKmyUaqPk85rbQByE68PF8W32BYDvQtV3-raAP7p9j2oNa6tWl6FsFZyZWvfJ_Z46ldbMGt5JmemLJgdsSXINUBBvoFSqyT2TFtfhgqRsb2MV4OhWq0Lw2-hpk56xd5M48fjUl5Y-c7Gj4Ieeupy-2jkP9mchgp1Fav-fJJVjyY9J56J26DuohotS54_0IOeNLoPQEoMICbsXuO2X8-r-57qnGfo9rQweoXFQSeHipS15Bo9JfwRYyanDCBYzjCnPCes6vKEgGahcJcET_Y8Y80KFO6Ki0NWw" style="width: 80%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>또한 Visual N-Grams 모델보다도 zero-shot 분류 성능이 더 좋았지만, Visual N-Grams 연구가 나오기 전에는 transformer 모델도 없었으며 학습 데이터 양또한 CLIP이 많다는 부분 등을 언급하면서 이 모델과의 비교가 완전히 공평한 조건에서 이루어지지 않았다는 부분을 언급합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 요즘 ChatGPT에서 프롬프트를 이용하여 다양한 task를 수행할 수 있는 것처럼, 당시 논문에서도 일찌감치 프롬프트를 이용하여 zero-shot 성능을 더 끌어올릴 수 있고 다양하게 적용될 수 있다고 말합니다.</span>
                        아래 결과는 실제로 프롬프트만 바꿔줬을 때, 4배 더 많은 연산을 한 모델과 같은 성능을 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EgEzxJc1pQO7_mvmLQ1kSLHhSkmeUWHMnHDC--CJJLGZKA4f-O27EfyqwZKvi2mGXlGnXB3yTS3mHig33ehU7xvldsX4drIFcFsNZEKqfVkjeqJ8Ia4na5I8vPtxBEwni7nEyvWowYo0vRlt33prFFloS_ZU9zqiQd3rXxPX0yk5sUFqT5tSjuop5GfJLGoFxVvcZJE9_GgAs4VhuB4DbMdHC5k0FUyFBtsPrPGS7KdiIlkfBo7PbUapIvOmM1e3vpLUyq6oJYAj-2Bhhyzd0slRyrKJR_6fmPmXjli1HNOWZnHAH0L9ylQ46OeqOGXcOdPHxGqAJ7g3pP7Xc5MUJ_ZOOZ9Kws7_Ci2sP1siz0yjcj12S6kv5qLeUo03LiEsS9kKKiWK3LgBTNiIp422HTSNpw2tzoFY7e2I5asXEfHALNaGhe5lrd2Lp7d44ybotcI88tHndwTNv_bgvKpopMx2EbRojhvKo2dZaX6V5kYeIWXQvr4LjYvzOHwWyDVAlvv-fEKoZEnAiwLsUZZ-7KFycoIDwMeMlvM4m3TxPJIucxcsbvJEujpIQ8oYXQZ1GyN8mTfbHvKlxwpdeghhvwkagKaUSzyb2XgDaw4VJa_7sJy1mw1p7RtKi5JqxGfhPLSA_ru1DJ6sPilgomUN7yRpHSBmxFMq6ORYU9tqXZ8hVoksHzHHg_72eIz2EiRjlraNAYEy89t5XhHNK9vpwH84HXTf01dk4GqaM08XEx-7oiUKlJ4uxDDAtdo1m33qi-cZ9FmK2OaOPCCZKCkkt1IpV5q5_RzIfhyRk9ZKkoDQ3jKLvVMTF2RNEagQBXsqrie3SnmGY13kUphVBciuLRPZ_Ypa_UFUqTnGQkbpzA_nuMYYYSBdBXkKXNSepJM3CwzSbBITK98c_loGi0WenCOQUxpMCTc0kBgFrZtWQafF96YGzXRuv-DjWRvIziIHwGXzo6iD0xw3hxEBsbkhsp7Dlch0Ekam96B54AcI02OOo75kV8TujeaCfxBmVKQKuwGW_kjf3rv29cVNARQ6zYrHagQYEimxbz90yOA3Qyv7FTiarFqYdELiDO-EqG6hhr8aSj58wDn0xdl472KK141o-dkhun7ATVwditM-o9ev7yArRyQMXF2HyCGV73UY3I8O1HCz4j87MFd73nzybIzxPcVqcnlI7WjmDCZX2ZHcqa9TJC_GPLnfA9nB5X2amDFh60f7fk8UjXLMHN7IYyDaV7pt_xUTnUZNMgEBqA8bYqZm8JWuK-X2mMK49fSOPd2rSJ02Oz3RIYGSs-nSJlgJM4fED4S5gnoks1lb6tv48hNVdusWi0pqSmtpaA4Sgc9nZqo8s_ZhXI7L-XlZnfyuUPcIww6wKXodx6TX0O1ueWNT6I454eJ_GYmj2Jn7_VHRoWgS2DI8MDsS-qdXp5oJyLGZZ55RD5sn3qBDGszKznKgHHTm6CRJNIQgq3Yn7tptuBYNLCRuMsBC-7uxbe41KluR00ovtIyDBqJ6WO5kg-AyvStF90owUsNsvMuHwmdsGKVC3g" style="width: 80%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>실제로 zero-shot의 결과를 보면 상당히 괜찮은 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_GFHYM872t6Z1HOp2lRBrwmwrwEwR10JXcTRrHaCbb8Bqav7Qz34L5k0wQJD2X-MHEm6BowdmdFBmMp9bg7KZC2Gpgu6yCHZbVw2hPl23pMgcZ7rtcDO3GnbBWdcT5vbQjp9IppCu2CLU9N-kg4c1nEPDwR2KLYgYGt7kBL6OLJCJ-z8rAT4j_1yy4QOhavzrdZquEyfnGTO_mYrO3JzgKPKDi0lPlsc7kdOR4me80CQWumlgrIUedFjcPguiKWHt38gnoU4qNIZVVDBUWTRHwG_yEPPbmzAmZ-RgWopy-4Mrgr9RFasWZ79gZ3msdAyGvn_IQ9LC0XFuQ58yYE1sVBUvP2EqUcYRmwoDWYfb7k3rNnf0ekr83O55GFLiK-uVMzEnErGFrL_kU4YDsz91dmN70chlOKf8e2xWsAlGBk_ebkPo6DYSevSpaTQ4tGygpNGRE-FRJXSk2ZoyMrMmBob7ky8PrPF9cW-7p3KrsU0V7SUzFzsl1Z-sucT_Q7B3wDUbY1zq5tXwcN2KUhzHU0MWTX0uHc0q1SgcIbVLag-pO-bxJpbOS8x62-F_8CqMzvdu37Bp7klFFSRqdjuhorO3AGD3oBt5zHHDun17xAqj8YP-sJR4iBU4eUgbaJbbOupsBdgmKJwRfPNv6ryb45YI6tfxKAhOpbuiHs6vHOnMlyHDWSOY499t2r4btjAA6HQB22-SnIVp3dt31Xkj5qJS6-ShszPJdnKqvllqUKL_yWPCov7pCbF2zXIfwNbgnLkCsFnb9_ogzfh6q-ISQhUiPIw3hRQzqM7tMNBvHP4KU9G6kQVtrAEk_3X5D0nNx4eKa7jS3iQsZlGxsHOkZHOH6Isx0l1yHAKsH8wrZnLQHF4MRYgXocZn19HPhqMB4D9nKBFSJ3Ljs-oLUuOyDbR0dgSaoj6nr4pDtKxL-u7oxYeAZf6CATi7Q9qoIkuL7CL-LrPxh1EhnftOH2uiI1Q2PZm4e6_QLJGrFRFVF1C9BVKgeqZyFwo_QCTai7kuO5fZPd54StSEsVT5pKoPEX0LeCZeOZ6aEuc54lcVfcq0yvcyh5uyAVkwegGGuapugPX4wmUNZmcJpOrPDlX7HximyGPRuARQMXl1NmQmgZ4qN6Gmor0uUg0Ezm34lJeifUOTDDFetGwE-EnlMiEwhDtKmn0zNjWA9zL9meefyA-733KgoWrGbO4awVuyUmj0RRCoU8Bd6h0xQrqKf7LcqKyZIFejcwGDS4z9miVwxU-RfQYlpyPKfbUChkFLLeyC8_y2GUu7PbQr4uztLQrlO8EuwaITDbYiph92r50vUzEaqjit9h-zaRklz4oUsiOo9bDSa4Jt078UNZG6kFY2u7_oKi8Upe_NLuCkJpqcE2d35jMxG63IUYppZpzX1DqWdojo2w8DvtPZvTQz3BM9CZio6e0CQlNZqLF9c2g4jSeqZbOskvoZMoMLNeafOSByLdtE8FE0N-Dcs8kB2Q4OM8jzB5QmtU8TFa2MBWheiGhOuqWIgfTKziABfyF6aAQePsGSvXV1Yd" style="width: 100%;">
                        <p class="caption">CLIP zero-shot task 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>2. Representation</b></span>
                        <br>CLIP은 궁극적으로 feature representation을 위한 모델이기 때문에 저자들은 feature representation 결과를 보여줍니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">먼저 CLIP을 포함하여 baseline 모델에서 나온 feature를 바탕으로 간단한 logistic regression 모델을 통해 이미지 class를 분류하는 방식으로 실험을 진행합니다.</span>
                        그 결과 ViT를 사용한 CLIP의 결과가 가장 좋게 나오는 것을 확인할 수 있으며 SOTA를 달성하였습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_GUEieP3wE7hFoGmJZdhEUlVzVneT35wBBYOlxDFAtOdxZGzUCx6qzktfBd9XqX-_zoKGNPgPcG9pDEdsc7ipXPo-y62xsl4g6pfe8A1P9wM7Z4C9wjCC5CQaF_B0RdJ5fin0S_tSKh7S_CkX7OYSJqReSU7HJqmJkImo8O0xRxoWOO67i28C0HU6QvOvt2Dxv049bIWxGOG2pokb6lyuZbr54-APfsGWIg4WRpS1PvLXWHW0d173CTjNxOG47vjNSDFo541QVUClN2WykYMSSCsayKtcF7UJaulce9SPyCsnuSg9_B54nRI44nlbj8WhLXtc-d_kF-wJ-YOtMe3BYc5JYSRRZtaRU30B3X98f5rT50ppzOCRDTwbl4d_WfOKMKhB22iASMW0FnJpLQyTAvBa3_OdYByTAjSeIRVlxtZAR-p_EkmPQ-rUPUMvFErfz1BJdmuBu8P7-7gft8KZUfjICyKjzbWlXTCziLY6ZcBsvLba60Oo0PBPN1PJ9dIgjq70axV-dVRM6E0yhKazLUH2E6bfx_3HKn7i5DUb09nFghLsBaLNGPMfus3T3EQ4_wxoHKuCjUAcRU14n8-MhfZY1ZB_4X5sAiK8j349O7ArVdU47bez-ntU9auXfOFwXhf7hu0rEZCkB1n09G6l2enjLWiRuVCS2DPNn2ySl2pkT5LXe83Om3NLoT9hxKLRxqPkqy0QIyofrBiUeNiaVvJ1ZuKrE6Bzg_FMp8l3tlFxZgopQDSoNjPRcXj2qzVvYKUE-jDvQm7t2FgWMvA5MR1tq5ACHOR-TadFXgcEZX77qwoDwX4FUBBoFOSaHViU4bzDpXWehF3ntOpp2Nl0An4qZimoFiC8tYO7ZgKD_XgWOpXwd8IFhaYPGmT2I03714O-k4ySWJ22VBKzpmHbRM5inm5Lipda8EJOeON18aU8wiQaib7VIfK5R6bL8TCTW-ir--_rWyCXKH5PJju_sMUi63L6dqdFUzA_BkA-grUog72fmRllFo6ZXB5jzqHMeXnogu27LDsH0Jl5mPHfg7-h2CHEoMdKqMBiHGBF0ArM2EJc4MGZ7tdY3rHkfxvZJiN-87qlELz7EAHRTFaXhI5RLD7sshITApSKztKIhabPXLWjuX5aWgYbCNwuEwGZN2dRK2cdwg3BV9dXttGCJSyaUbE3cgxhashFW-Kj-g6DzmlbV993fOqzma4MeAgn4Kd5oj5k-WZPD8hrRIwiwrAAgCKGuF1BBqP58x8MYuZ-9L8UnwBmD2PVuHyagxRoZJ78cq19HJYoOi9pjE6oNueRPEG8EqrrKpaj9Hnp8hggRSSXpkifUjk4solq7uOROCypAAnj6Q7zoZlpl6Q01uB_oM_ACesjVk8iHv60vL-PsWXFgWIDFxsPTzO8ysPgVp7p5weM5OGfI2bht56iXUnlAzasHhCO_zuagChW4cqpVgH1_O2tzbtn2vsXik8Ur8fL3pMdGYxvQkKDqoN7EUXSY9SPd_cdI7dzBUfoXsEhcZfyHgx624inJ0TLOgsgGcSYd_5nPs" style="width: 100%;">
                        <p class="caption">CLIP feature representation 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 CLIP의 feature를 바탕으로 27개의 데이터셋에 대해서 분류한 결과 EfficiNet의 결과보다 대부분의 데이터셋에서 더 좋은 성능을 보여준다는 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FQnoPIHnhROKTRCeGthkKwek4FEh3kOexFn9AhlPlC0J19gH8VCPMFjEqjYLiEhJ1C-_k3JxLC0djPXRYEeC27as6vL3VU66LS1hC1TPXf6ab9fLifPcG62VJBdK2ofJA-Iz_LPT2EoZkhdTd2l0iDmcjhASYjHdrm36D-vE4SIBzBt1z6L5W-9GQvED45v72bXJvZBe04BqGrynitzRhGK7bbNk6zFZ5fDXqHqGl5LLSu01TKfimGFJACGgyUqMf-ezIEoLQvCpa3NdcKy0tbmMezVs3teUtVfG60ZC9G6ESaIzGn_Xa7jWQbefoGDTVUWCUWquDVSfE6OYdAt27is04wrs7IVnFDEpX9U0mzVDJDM7Q1GjuC8DzGGUP5Ye2H3MNtwr3LT8ZkSrI1ohAWQnd5MTBnDTQp3lb9WbUuSXkaGHOWICFi4L0a1fCkuT-yq34iJaDGHFHlRBBexc0WOYBVN7q7MGeKQP2dWdTjFU9tXeO5zOsdym-hjQkCJ7kqq28a6mj5lglgiJktxxWrHsnfbRwuPwc8r7YVxrn30tabygOuQeSxkuFf06B3q5JrdWTzW4OOP_6TvYHlNoyZcAuc26aV9DbMR15tnl15Z4t1vwwmyevnv8acoZ2L5C_gaMtw4ODKWG5yguFF8JanQMBTPao0-UBfI9q0wB33rqG1Z5zxtNtXkTfKI1veT59yS8wZJ5-3mOxifmXfJ_i4WIx_nHGxJdBHif4BVPZB9Ti97RZJFb7DGwHl5QNf_MlCU9LMDUK74ax7BnUCqczufbnbZrm1ijBvkYTM4eehS_VpiM00egA040U9v-TUM-D2YDoBO_dp4nIUujStCYPPj6iFOMUDKfhPm45qiWAbfmzpZvRe-Ag95CATzeAc3eCLPz-m9bnJzdEFHFz-_B5TwGtNHlSluVhZTWOagq4jOf89UmAkIiSdx6AlfUVDSuR3f_B4LSKizmx0_FkapaSLfRfp5hhmBOsALVrtxoFbVOAvVnBDJVfVh4dFhC-8SRgYgY42x9m-VSAth-eesr5MKC9tUs354Xy-FlCuYrn-7ek_AtCsAw-wL_3h70GiZ6whurmc-FLGN2z73g4dckYvpmqGJDYlYGuPRpoa7fTC5SO5YbnNny5RKp5dX716KxqOLXdOgub4A8r5EyM_dRZv_ggaYWP1ol8hSjP0W_BQ_zCF8x8G_TCk6qkA9PC4OgFHlXc45EkrRaZIfhYTIbT6JB45O5sCrD5BoX8qjttfHquubL3p8NXkK-ETn3mxcftrS99038um8Gtm93vCJe8YhCq6J6hTLGo0Ro7lZmt_zE_RCjg6D2YuEMcAkcHWZ9zANywQKk05-41MVgfK-wCQbrRiWOlQD2pRMJRxeCqC7oKq8EdHx137NGaFnSlIQ7TA7NKCpUI1lntifY9MPopEVJdRaw0wfeRJZ69-3KBNVvG2VheAQnFdp0REwhfFywHBVJaHgwKi49Rd6fQwK6d2tltjlqoV69Jt1FYV-0IQvN4mIUGo9zi0tuHW97nLvfDZNGSj7WMs" style="width: 80%;">
                        <p class="caption">CLIP feature representation 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>3. Robustness</b></span>
                        <br>CLIP은 다른 모델들에 비해 그 결과가 robust하다고 주장합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림의 y축의 결과는 위의 Representation을 설명하는 처음 그림의 결과와 같습니다.
                        다만 x축을 ImageNet의 분류 결과로 바꾸었습니다.
                        CLIP을 제외한 다른 baseline 모델들은 ImageNet에서 pre-trained 된 모델이라 다른 데이터셋으로 평가하였을 때 점선 아래, 즉 ImageNet 결과보다 낮은 결과를 보여줍니다.
                        반면에 CLIP은 데이터셋의 상관 없이 똑같이 높은 결과를 보여줍니다. 이는 CLIP이 robust하게 학습이 되었다는 것을 방증합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HfM7752cSe9MZYNc5wILDokLgfNSL5NHnrUz8ZXlEK9YOeVVGF7FuhmPuyLWYCGa5uaZt2ZsFNTs3EBETWBYWS2ngDarRH4j_Bjs3sC11FfBW5BNAoYyRADmYQKxM-T6CxVnfsVqdEvkwnVPlmD0siYKXs6pPBW6_midVbHwuYMKOMueICohj-4Foae98Yn-xLLdsihAWC4-LFnMZglN33pjTopmQicYcrX5JtLcpYFIOooDSYf51v6Cew2skyTeepoz2zMy_RnwI0N2zmxaWeBxXtNTmfbQDOUVGJDsRhOPJC9EF77R8z_qlw5AYms6bvBvGlvCn9Z4KYVOXO10g81pmKJRX3SjT9wdcDFZVHkgJyeo18BvqEqcBOe7hCjCSDFxqs49ktGp59_a-IIfQRGTc-WkCLWSUC-0Poc4q_osyZcnzzuBCGAR8GN3h1R2quQYDy4HV2DGIEsm9SlmDShPOjFFuals5sRNU1favOYBB3E2jQBGIWuZeC7b48NOooLXJdqNUK94keADsf2j-xfIDTedgDEuYHPwsHLvLVhPDG4eg6zQKtDk7F9vyoJU1fC0sF6IzQ5cdFMvIOkkiEe9KaE04JfsFcqXy2Z-v1F8ULIyBpc8G1Xn_AJv6xrthK1q4i-D7OmmY3WP_636Z4exvN2yyD0z0r4Zo_2jPIHKXqjT-xe8PbLKNuCs1jB4NEFUH3m3nd69J44NKKUzpQeTO7FwGeZsk8qEyDrRutoayT7y-gW-rnPSrx2rgDVRZ9WjhZSRHQtvte31B-JXJ3WG2AJOf1hVnWEtRta3saygxwn3n84qCDKDg9yTKGe7iuca7wSW-v-sLtzsznIh4e0fp_Z9pThDzDMY13_fMu812msXWLQZEZb-lRqCLLgX7qA2sumjP3KdoIFx8jFIpvKohmlzKCVClw5yANXMQpbtgY59fbpR7gyp80CS_71AcvEqNZASmAirHjw-XGXEjMpX3XD8pDiF5TktrTAkH1z_QGrXRHfq9unt2pW9mh7PEcZ2u6-3I6R_K68WeDIw9VcRGzzrptkTISo00W2_ndGJ5mSQXts6NGQ5FCYyQy-d8y45f7UJOIE458N01nTM3iyP6cOcOYp6yCPP8ochNMv17FozHKGoNffd5CTdN60auZnEmzkSljIwD2uTIwnmMNucpVzbGCTAjribLCvmoQL9d_0vGp_TnrVLdo38YUi2cIBEP1sg8J6wKcp4vGTjO78Gfq0SYoISFTZl1ePmCCYDKlyNm5rRNqkbdMedOjEyCYrptqtItPy_4gmGhUew7T4M1D2O1xUYMuVnyoyf_bZh37mlgNcCOXA5MhWbaFcSRo5f9ViHkyy_96AIqwACXeO0yVjf9P_9Nsrn7IOTp8vCKTy8ymX9A8LLFysrEXnje5a-D9eJm4vlEO1-UXT24fMi9UaL4WzSfi3HH1hyBwqQ8aAAL35cO_bkeakwSODZ-sxxFaprkorcYG3LGIXIrEVRnMz7bBvFgL6AJKNvvDqN0-rZpL7M4VpcNyXMxZEQGLQif77dIw" style="width: 100%;">
                        <p class="caption">CLIP robustness 결과, 출처: CLIP 논문</p>
                    </div>
                    <p>
                        <br>그리고 아래의 오른쪽 결과에서 보면 CLIP 모델은 ImageNet으로 학습한 ResNet101보다 다양한 데이터셋에서 더 좋은 결과를 보여줍니다.
                        그리고 왼쪽 결과에서 보면 CLIP모델은 이상적인 robust model과의 gap이 다른 모델에 비해 차이가 적은 것을 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Fe7zen4bNCDe16bkPPW-Y36thikHX8TeC4Iso07_BQ75lsU4HnxAKba4oVbvKZh625fUfpoH_zTbAW6hqer52M5-gvQ77Bo2g-zGS_7E9umZ382UHeGaWPThQW5K0ndmLnahmywlXlLgyl1cFXHcyFHV3lXEuff1uGKZeJRUIKyDetk1iVl2mEPmXPQPZd8jzezyj3eaO2jGZYTLi8r0Jjo4mLo-GzyHvye5GXfXIBiVjmc6XIcjxgnr8xcNfluQwkd7DAdZID2oQJzBd40pwd8Sln2nm-ccYaNIfR7NP4i1il1JywIN06xgN09OgY7wT_vaCh6eneQcKkuy3OKdy1tZpK-dVUR6XhSsdfFkAH-TC25gFkb-_TPP8Y9nuVGiKuhaS-CTyezj8sKADYnPxF9dnkHHY5bu7ybFsNNgJX2D2r3JP3Xn2DpJ-GsL-iUO6B5m7tmHBXogSBSTns4h8hQTKNd4wBuGnl-9pM18N-xM4c3dQjJ7PAyhbEHqHW_tHm7k32PLqBy5NIWqGYnhtqlhQV4oDhgRfgSr0ugL3Y1SbQsryF5raSO45FWE9tXB6STLATA-CIW7__SSYkERmsNmLFNlzSyebvRQdZdFbGNKveb8uQF1nn9NkSpAdGuMa279fu2RF_zCmT9UGEKEJ31gSAbOVP05oBBHonvqIUl3BIuW3FV2COKHL3FZb38Uos4R7ZJi6A3t7wnFWkA0gSYobw41Gyg40zwI580RFkUAhTz6gvFj-G44orl29P-YMcIMc3nMgleqO3qlo52c9-p_wxumdQrmsejGYFKuePO2CdRABZS_iz2Oj1Pi2123SLefemHEy5twiyDOaqmo1SMxt54CUdbXFchcER60lLhp86grhM5KP3c9eqkDeCK6N4p1SGeoM5oy1jmDW8Tf9ljwRkYLHAO2b7PRkIC-b_BYgk8uiP-DC_gbv5vwxN5cRFj3pvxm0KSUqEXW1KM5p-VsqidYCPjy5zdqQQRtUTozcUKhIzSF3a94ZDkz87U5S3iNNSFlXkQatkujtYjiYzgAtiAoYravFjnqrY55QnO6AyT4NF_7Xht1Mg6iY4FluayF4g9l3W2Ua6Tx0m_4lKih0dvujhZoScWwKqNrbwmT95R5EbLXNpDdBjnMwbkkr7y54kXr7BHWLIaQK0PA1GCtqy0-iw81mvK8tb6KkomqcUw0QVrq53KzUFN3T1fpcTod6-BA10lgIElZ0xE7c4FZjVGJk4WnSUKfyW9Id9Rmn6RCqgmjXI4FWEX1jkQQAi6q8fS0Bl5T75qdPjsYsNgl0mAb3Z0MszdBaEDctBVur7uoftX-n1VybCG7NFCyXeqMSvKO0v2iqpuzqwBKkz505SNcOoBAyBAlIElqebu871_76RXErx3dy3_VxPWiI1-SWwzijxLtvFhsdL0ot3vwp1LYcMVa2EqiF1SH_i1eVj-39KJwgsIx93-vEq9svuHRrm9sluiRi6NWgpUG6Q1BqvbtDlVgeQ9x6g-bMHgBE1xhJGHmrH4hDfaH9gKwblYacwD6iB" style="width: 100%;">
                        <p class="caption">CLIP robustness 결과, 출처: CLIP 논문</p>
                    </div>


                    
                    <p>
                        <br><br><br>CLIP은 OpenAI답게 4억개나 되는 image-text 쌍 데이터로 general representation을 학습한 모델입니다.
                        이 모델은 추후 다른 모델에서 많이 활용되었으며, 그 성능도 엄청났었습니다.
                        
                        <br><br>다음에는 image-text가 아닌 text-text 질의 응답 쌍 데이터를 통해 주어진 query에 대해 적절한 answer를 데이터에서 retrieve 해주는 text-clip을 제작한 프로젝트에 대해 이야기 해보겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#CLIP&emsp;#ContrastiveLearning
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('CLIP 첫 게시물 입니다.\n\nThis is the first post of CLIP.')" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('clip2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>Text-CLIP을 이용한 질의 응답 retrieval 모델 학습</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>