<!DOCTYPE html>
<html>
    <head>
        <title>Transformer를 이용한 WMT'14, IWSLT'14 (En-De) 기계 번역</title>
        <meta name="description" content="Transformer를 이용해서 WMT'14, IWSLT'14 번역 모델을 학습합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/transformer2.html" />
        <meta property="og:title" content="Transformer를 이용한 WMT'14, IWSLT'14 (En-De) 기계 번역" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="Transformer를 이용해서 WMT'14, IWSLT'14 번역 모델을 학습합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/ALs6j_FyI_4pqugl4czV3FXegOJClD4AbqJb0bTpMhRwvIVQw4_rNW-G_Kkw2NDVkspohHLQ24oqz56W4jyfmKVsE3fKFNFfq8N-HcLSFXGT-o4RWT02O9lngaIRITb6Zl_h5dW-i0BMZX8fjpaNvhzfvTMcFYQhRzO1TbpYE0tMgTylSqGz2XFW9gSQTT6qR5FNvpuyGAShXGD7HIPhHZr-vTkJadkV2oWmn1lXUy5yGZafKLY5CJDsjZS14emwEFUDlwZk-tRoA_XK3qmWj-Zu6-pkQiS_iNxdLxnalPhPF-mfPJoGPHeKvWtB3HRNOrBhaziUETGBzZws2DubKdPEsyX3SLFhnqRhEKy3XalWMhZuHjExTcpd_6v_jZt7Yajtao_XYz3cGz7NmGdQrxAOooOSRKHwb1ZebxW9aN3LBHL2mvEhA4p1k3CrLXvv2MBpeyYX5z3jghnVkUqYit8-Fzer8wgEjfjKLQCDo9VI5A0W_iHlBhnnV5TFjvG7PC2ySALLexg4yDQi0UeReDPT-7QuLEsGiOPLuwTJgR4uR67V7tHyDnVVYJOot2QNSqmPbcti-3pc-Y0a95LySf7tVN6v9Z0U1Jg8oHaxc1WSCGtiOs786LcCxyX79PzYebrrR8gx0WCIMZa4rp5Yziw19uTwJyGCu4JhHx_8cU7QC1wRr1M5gDZffvxELnfhfusK2YtCfUzTNThSP4P25FOspLbgrNp7nebPWF1QvDnzWZmmNIiMDaTSYsx8bogs3Ni3h14hXQJVa3pf5lr5HZCKBf_ghX5DSw2jjijiVJteBp55QcKa08GF5HayhbStgFhWRYaoICmCpnMN9vLXcf0dEaiodNQEWRZ6HSHp3QUn0IO6B1Z4Q_rfKtJvH8J803E6BWszzliitGPYGA4D8o52b9A6AkrhniP7szMeEzOhBP4pfEzfvBXmUXY_yQDHboujvh10lClfLy_yObJfufBy3GmnCLqD73om6CtIMENZyEq__OaOgR66vTAKbYsNUBdkUuExbmCcWjzg2_cyu5ny1r1fRcGbcGeISgi_v6b2N8WftuqC2ooISi7IyiBSknY0wsTXL6GfUjKi2ogDKVrb-nAkUXe99JjZ1SJvK_8n4Rjtb2SNegFQlQ9yatCANNOSCKn-ux_wc0zvtKW16ERAhz09gyx6B5EkRMVVARb3JzeUZU9RhP5QHEXx5j7Rn_NitNOCQjTlzLB7-8OZVSbdbz1UmUselX4etTaQWNmuHRzUaWq2qlba9aSwjn_wWZ8L5UB4ig9d9gcXDhTnAXGygIhS0P8G8NEB1tz3gBy8tCKvjisFv6KYazob8dC3usjK4vBpZiztiE2OpRNTX7kqjfvQ2y-k3qSj7xKCWv__cOgjkp_-u5xRJXSrv9r_em2nxTjz_ibKXXZVMsJhHzXoyAG6vleDmdcKJXDIpNbw8QtNJhl5BKLG_09IinmWyaZCFRuHSzx_m5xlY6awQDj2FJD7qOAZ_uhrmr7bHrNzXJnCx1REf7L3N648NfolwVdLhb9ikNilvA" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Transformer / 2. Transformer를 이용한 WMT'14, IWSLT'14 (En-De) 기계 번역</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/ALs6j_FyI_4pqugl4czV3FXegOJClD4AbqJb0bTpMhRwvIVQw4_rNW-G_Kkw2NDVkspohHLQ24oqz56W4jyfmKVsE3fKFNFfq8N-HcLSFXGT-o4RWT02O9lngaIRITb6Zl_h5dW-i0BMZX8fjpaNvhzfvTMcFYQhRzO1TbpYE0tMgTylSqGz2XFW9gSQTT6qR5FNvpuyGAShXGD7HIPhHZr-vTkJadkV2oWmn1lXUy5yGZafKLY5CJDsjZS14emwEFUDlwZk-tRoA_XK3qmWj-Zu6-pkQiS_iNxdLxnalPhPF-mfPJoGPHeKvWtB3HRNOrBhaziUETGBzZws2DubKdPEsyX3SLFhnqRhEKy3XalWMhZuHjExTcpd_6v_jZt7Yajtao_XYz3cGz7NmGdQrxAOooOSRKHwb1ZebxW9aN3LBHL2mvEhA4p1k3CrLXvv2MBpeyYX5z3jghnVkUqYit8-Fzer8wgEjfjKLQCDo9VI5A0W_iHlBhnnV5TFjvG7PC2ySALLexg4yDQi0UeReDPT-7QuLEsGiOPLuwTJgR4uR67V7tHyDnVVYJOot2QNSqmPbcti-3pc-Y0a95LySf7tVN6v9Z0U1Jg8oHaxc1WSCGtiOs786LcCxyX79PzYebrrR8gx0WCIMZa4rp5Yziw19uTwJyGCu4JhHx_8cU7QC1wRr1M5gDZffvxELnfhfusK2YtCfUzTNThSP4P25FOspLbgrNp7nebPWF1QvDnzWZmmNIiMDaTSYsx8bogs3Ni3h14hXQJVa3pf5lr5HZCKBf_ghX5DSw2jjijiVJteBp55QcKa08GF5HayhbStgFhWRYaoICmCpnMN9vLXcf0dEaiodNQEWRZ6HSHp3QUn0IO6B1Z4Q_rfKtJvH8J803E6BWszzliitGPYGA4D8o52b9A6AkrhniP7szMeEzOhBP4pfEzfvBXmUXY_yQDHboujvh10lClfLy_yObJfufBy3GmnCLqD73om6CtIMENZyEq__OaOgR66vTAKbYsNUBdkUuExbmCcWjzg2_cyu5ny1r1fRcGbcGeISgi_v6b2N8WftuqC2ooISi7IyiBSknY0wsTXL6GfUjKi2ogDKVrb-nAkUXe99JjZ1SJvK_8n4Rjtb2SNegFQlQ9yatCANNOSCKn-ux_wc0zvtKW16ERAhz09gyx6B5EkRMVVARb3JzeUZU9RhP5QHEXx5j7Rn_NitNOCQjTlzLB7-8OZVSbdbz1UmUselX4etTaQWNmuHRzUaWq2qlba9aSwjn_wWZ8L5UB4ig9d9gcXDhTnAXGygIhS0P8G8NEB1tz3gBy8tCKvjisFv6KYazob8dC3usjK4vBpZiztiE2OpRNTX7kqjfvQ2y-k3qSj7xKCWv__cOgjkp_-u5xRJXSrv9r_em2nxTjz_ibKXXZVMsJhHzXoyAG6vleDmdcKJXDIpNbw8QtNJhl5BKLG_09IinmWyaZCFRuHSzx_m5xlY6awQDj2FJD7qOAZ_uhrmr7bHrNzXJnCx1REf7L3N648NfolwVdLhb9ikNilvA);">
                    <div>
                        <span class="mainTitle">Transformer를 이용한 WMT'14, IWSLT'14 (En-De) 기계 번역</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2022.12.04</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이전글에서는 transformer에 대해 설명하였습니다. 이번글에서는 transformer를 이용하여 기계 번역 모델을 학습해보겠습니다.
                        본 글에서 설명하는 코드는 기본적으로 학습을 진행할 때 모델 저장의 지표로 사용되는 BLEU-4 계산을 NLTK를 사용합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">본 코드는 추가로 학습이 완료된 모델에 대해 번역 모델의 benchmark BLEU score 계산을 위해 많이 사용되는 multi_blue.perl을 이용하여 계산할 수 있는 방법도 제공합니다.</span>

                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">학습에 사용한 데이터는 영어, 독일어 번역 데이터인 WMT'14, IWSLT'14 (En-De) 데이터를 사용하였으며, 구현은 python의 PyTorch를 이용하였습니다.</span>

                        <br><br>그리고 transformer에 대한 글은 <a onclick="pjaxPage('transformer1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">Transformer (Attention Is All You Need)</span></a>,
                        LSTM을 이용하여 학습한 기계 번역에 관한 글은 <a onclick="pjaxPage('RNN4.html');"><span class="highlight" style="color: rgb(0, 3, 206);">Seq2Seq 모델을 이용한 기계 번역</span></a>를 참고하시기 바랍니다.
                        본 글에서 설명하는 transformer 학습 코드는 GitHub에 올려놓았으니 아래 링크를 참고하시기 바랍니다(본 글에서는 모델의 구현에 초점을 맞추고 있기 때문에 데이터 전처리, 토크나이저 학습 등 학습을 위한 전체 코드는 아래 GitHub 링크를 참고하시기 바랍니다).
                        
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>Transformer 구현</li>
                            <li>Transformer 학습</li>
                            <li>Transformer 학습 결과</li>
                        </ol>
                    </p>
                    <div class="link">
                        <a href="https://github.com/ljm565/neural-machine-translator-transformer" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Transformer 기계 번역 모델 GitHub 코드</a>
                    </div>


                    <h1 class="subHead">Transformer 기계 번역 모델</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>Transformer 구현</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        여기서는 transformer 구현 코드를 살펴보겠습니다.
                        먼저 encoder, decoder를 살펴보기 전에 embedding layer 코드부터 확인해보겠습니다.
                        한 줄씩 자세한 설명은 아래를 참고하시기 바랍니다.
                        <br><br><span style="font-size: 20px;"><b>Token Embedding &amp; Positional Encoding (Embedding)</b></span>
                    </p>

<pre><code class="python"><span class="reserved">class</span> <span class="clazz">Embeddings</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">vocab_size</span>, <span class="var">hidden_dim</span>, <span class="var">pad_token_id</span>):
        <span class="clazz">super</span>(<span class="clazz">Embeddings</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">vocab_size</span> = <span class="var">vocab_size</span>
        <span class="var">self</span>.<span class="var">hidden_dim</span> = <span class="var">hidden_dim</span>
        <span class="var">self</span>.<span class="var">pad_token_id</span> = <span class="var">pad_token_id</span>
        <span class="var">self</span>.<span class="var">emb_layer</span> = <span class="clazz">nn</span>.<span class="clazz">Embedding</span>(<span class="var">self</span>.<span class="var">vocab_size</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">padding_idx</span>=<span class="var">self</span>.<span class="var">pad_token_id</span>)


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">x</span>):
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">emb_layer</span>(<span class="var">x</span>)
        <span class="return">return</span> <span class="var">output</span>



<span class="annot"># positional encoding layer</span>
<span class="reserved">class</span> <span class="clazz">PositionalEncoding</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">max_len</span>, <span class="var">hidden_dim</span>, <span class="var">pos_encoding</span>, <span class="var">device</span>):
        <span class="clazz">super</span>(<span class="clazz">PositionalEncoding</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">max_len</span> = <span class="var">max_len</span>
        <span class="var">self</span>.<span class="var">hidden_dim</span> = <span class="var">hidden_dim</span>
        <span class="var">self</span>.<span class="var">pos_encoding</span> = <span class="var">pos_encoding</span>
        <span class="var">self</span>.<span class="var">device</span> = <span class="var">device</span>

        <span class="var">self</span>.<span class="var">pos</span> = <span class="clazz">torch</span>.<span class="method">arange</span>(<span class="num">0</span>, <span class="var">self</span>.<span class="var">max_len</span>)
        <span class="return">if</span> <span class="var">self</span>.<span class="var">pos_encoding</span>:
            <span class="var">self</span>.<span class="var">pe</span> = <span class="clazz">torch</span>.<span class="method">zeros</span>(<span class="var">self</span>.<span class="var">max_len</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>)
            <span class="return">for</span> <span class="var">i</span> <span class="return">in</span> <span class="clazz">range</span>(<span class="num">0</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="num">2</span>):
                <span class="var">self</span>.<span class="var">pe</span>[:, <span class="var">i</span>] = <span class="clazz">np</span>.sin(<span class="var">self</span>.<span class="var">pos</span>/(<span class="num">1</span><span class="num">0</span><span class="num">0</span><span class="num">0</span><span class="num">0</span>**(<span class="var">i</span>/<span class="var">self</span>.<span class="var">hidden_dim</span>)))
                <span class="var">self</span>.<span class="var">pe</span>[:, <span class="var">i</span>+<span class="num">1</span>] = <span class="clazz">np</span>.cos(<span class="var">self</span>.<span class="var">pos</span>/(<span class="num">1</span><span class="num">0</span><span class="num">0</span><span class="num">0</span><span class="num">0</span>**(<span class="var">i</span>/<span class="var">self</span>.<span class="var">hidden_dim</span>)))         
            <span class="var">self</span>.<span class="var">pe</span> = <span class="clazz">nn</span>.<span class="clazz">Parameter</span>(<span class="var">self</span>.<span class="var">pe</span>.<span class="method">unsqueeze</span>(<span class="num">0</span>), <span class="var">requires_grad</span>=<span class="reserved">False</span>)
        <span class="return">else</span>:
            <span class="var">self</span>.<span class="var">emb_layer</span> = <span class="clazz">nn</span>.<span class="clazz">Embedding</span>(<span class="var">self</span>.<span class="var">max_len</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>)


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">x</span>):
        <span class="return">if</span> <span class="var">self</span>.<span class="var">pos_encoding</span>:
            <span class="return">return</span> <span class="var">self</span>.<span class="var">pe</span>[:, :<span class="var">x</span>.size(<span class="num">1</span>)]
        <span class="return">return</span> <span class="var">self</span>.<span class="var">emb_layer</span>(<span class="var">self</span>.<span class="var">pos</span>.<span class="method">unsqueeze</span>(<span class="num">0</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>))[:, :<span class="var">x</span>.size(<span class="num">1</span>)]
</code></pre>
                    <p>
                        <ul>
                            <li>4 ~ 7번째 줄: Token embbeding을 위해 필요한 파라미터 정의.</li>
                            <li>10 ~ 12번째 줄: Token embedding을 지나는 부분.</li>
                            <li>20 ~ 23번째 줄: Positional encoding을 위해 필요한 파라미터 정의.</li>
                            <li>22번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">pos_encoding이 1이면 positional encoding, 0이면 positional embedding을 수행</span>.</li>
                            <li>26 ~ 31번째 줄: sin, cos을 이용한 positional encoding 정의.</li>
                            <li>32 ~ 33번째 줄: Positional embedding을 정의.</li>
                            <li>36 ~ 39번째 줄: Positional Encoding (or positional embedding)을 거치는 부분.</li>
                        </ul>
                    </p>
                    <p>
                        <br><br><br>이제는 multi-head attention 부분입니다.
                        <br><br><span style="font-size: 20px;"><b>Multi-head Attention</b></span>
                    </p>


<pre><code class="python"><span class="annot"># mulithead attention</span>
<span class="reserved">class</span> <span class="clazz">MultiHeadAttention</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">hidden_dim</span>, <span class="var">num_head</span>, <span class="var">bias</span>, <span class="var">self_attn</span>, <span class="var">causal</span>):
        <span class="clazz">super</span>(<span class="clazz">MultiHeadAttention</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">hidden_dim</span> = <span class="var">hidden_dim</span>
        <span class="var">self</span>.<span class="var">num_head</span> = <span class="var">num_head</span>
        <span class="var">self</span>.<span class="var">bias</span> = <span class="var">bias</span>
        <span class="var">self</span>.<span class="var">self_attn</span> = <span class="var">self_attn</span>
        <span class="var">self</span>.<span class="var">causal</span> = <span class="var">causal</span>
        <span class="var">self</span>.<span class="var">head_dim</span> = <span class="var">self</span>.<span class="var">hidden_dim</span> // <span class="var">self</span>.<span class="var">num_head</span>
        <span class="return">assert</span> <span class="var">self</span>.<span class="var">hidden_dim</span> == <span class="var">self</span>.<span class="var">num_head</span> * <span class="var">self</span>.<span class="var">head_dim</span>

        <span class="var">self</span>.<span class="var">q_proj</span> = <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">bias</span>=<span class="var">self</span>.<span class="var">bias</span>)
        <span class="var">self</span>.<span class="var">k_proj</span> = <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">bias</span>=<span class="var">self</span>.<span class="var">bias</span>)
        <span class="var">self</span>.<span class="var">v_proj</span> = <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">bias</span>=<span class="var">self</span>.<span class="var">bias</span>)
        <span class="var">self</span>.<span class="var">attn_proj</span> = <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">bias</span>=<span class="var">self</span>.<span class="var">bias</span>)


    <span class="reserved">def</span> <span class="method">head_split</span>(<span class="var">self</span>, <span class="var">x</span>):
        <span class="var">x</span> = <span class="var">x</span>.view(<span class="var">self</span>.<span class="var">batch_size</span>, <span class="num">-<span class="num">1</span></span>, <span class="var">self</span>.<span class="var">num_head</span>, <span class="var">self</span>.<span class="var">head_dim</span>)
        <span class="var">x</span> = <span class="var">x</span>.<span class="method">permute</span>(<span class="num">0</span>, <span class="num">2</span>, <span class="num">1</span>, <span class="num">3</span>)
        <span class="return">return</span> <span class="var">x</span>


    <span class="reserved">def</span> <span class="method">scaled_dot_product</span>(<span class="var">self</span>, <span class="var">q</span>, <span class="var">k</span>, <span class="var">v</span>, <span class="var">mask</span>):
        <span class="var">attn_wts</span> = <span class="clazz">torch</span>.<span class="method">matmul</span>(<span class="var">q</span>, <span class="clazz">torch</span>.<span class="method">transpose</span>(<span class="var">k</span>, <span class="num">2</span>, <span class="num">3</span>))/(<span class="var">self</span>.<span class="var">head_dim</span> ** <span class="num"><span class="num">0</span>.5</span>)
        <span class="return">if</span> <span class="reserved">not</span> <span class="var">mask</span> == <span class="reserved">None</span>:
            <span class="var">attn_wts</span> = <span class="var">attn_wts</span>.masked_fill(<span class="var">mask</span>==<span class="num">0</span>, <span class="clazz">float</span>(<span class="str">'-inf'</span>))
        <span class="var">attn_wts</span> = <span class="clazz">F</span>.<span class="method">softmax</span>(<span class="var">attn_wts</span>, <span class="var">dim</span>=<span class="num">-<span class="num">1</span></span>)
        <span class="var">attn_out</span> = <span class="clazz">torch</span>.<span class="method">matmul</span>(<span class="var">attn_wts</span>, <span class="var">v</span>)
        <span class="return">return</span> <span class="var">attn_wts</span>, <span class="var">attn_out</span>


    <span class="reserved">def</span> <span class="method">reshaping</span>(<span class="var">self</span>, <span class="var">attn_out</span>):
        <span class="var">attn_out</span> = <span class="var">attn_out</span>.<span class="method">permute</span>(<span class="num">0</span>, <span class="num">2</span>, <span class="num">1</span>, <span class="num">3</span>).contiguous()
        <span class="var">attn_out</span> = <span class="var">attn_out</span>.view(<span class="var">self</span>.<span class="var">batch_size</span>, <span class="num">-<span class="num">1</span></span>, <span class="var">self</span>.<span class="var">hidden_dim</span>)
        <span class="return">return</span> <span class="var">attn_out</span>


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">query</span>, <span class="var">key</span>, <span class="var">value</span>, <span class="var">mask</span>):
        <span class="return">if</span> <span class="var">self</span>.<span class="var">self_attn</span>:
            <span class="return">assert</span> (<span class="var">query</span> == <span class="var">key</span>).all() and (<span class="var">key</span>==<span class="var">value</span>).all()

        <span class="var">self</span>.<span class="var">batch_size</span> = <span class="var">query</span>.size(<span class="num">0</span>)
        <span class="var">q</span> = <span class="var">self</span>.<span class="method">head_split</span>(<span class="var">self</span>.<span class="var">q_proj</span>(<span class="var">query</span>))
        <span class="var">k</span> = <span class="var">self</span>.<span class="method">head_split</span>(<span class="var">self</span>.<span class="var">k_proj</span>(<span class="var">key</span>))
        <span class="var">v</span> = <span class="var">self</span>.<span class="method">head_split</span>(<span class="var">self</span>.<span class="var">v_proj</span>(<span class="var">value</span>))

        <span class="var">attn_wts</span>, <span class="var">attn_out</span> = <span class="var">self</span>.<span class="method">scaled_dot_product</span>(<span class="var">q</span>, <span class="var">k</span>, <span class="var">v</span>, <span class="var">mask</span>)
        <span class="var">attn_out</span> = <span class="var">self</span>.<span class="var">attn_proj</span>(<span class="var">self</span>.<span class="method">reshaping</span>(<span class="var">attn_out</span>))

        <span class="return">return</span> <span class="var">attn_wts</span>, <span class="var">attn_out</span>
</code></pre>
                    <p>
                        <ul>
                            <li>5 ~ 10번째 줄: Attention을 위해 필요한 파라미터 정의.</li>
                            <li>11번째 줄: num_head에 따른 head dimension sanity check.</li>
                            <li>13 ~ 16번째 줄: Query, key, value 및 attention 결과를 각각 mapping하는 linear layer.</li>
                            <li>19 ~ 22번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Multi-head attention을 위해 행렬 차원 바꿔주는 부분(B x L x hidden_dim &rarr; B x num_head x L x head_dim)</span>.</li>
                            <li>25 ~ 31번째 줄: Scaled dot product를 하는 부분. Encoder의 경우에는 pad mask, decoder일 경우 mask에 causal mask가 들어옴.</li>
                            <li>34 ~ 37번째 줄: Multi-head attention 때문에 바뀌었던 행렬을 차원을 복구하는 부분(B x num_head x L x head_dim &rarr; B x L x hidden_dim).</li>
                            <li>40 ~ 52번째 줄: Multi-head attention을 수행하는 부분.</li>
                        </ul>

                    </p>
                    <p>
                        <br><br><br>이제는 postion wise feed forward network 부분입니다.
                        <br><br><span style="font-size: 20px;"><b>Postion Wise Feed Forward Network</b></span>
                    </p>


<pre><code class="python"><span class="annot"># postion wise feed forward</span>
<span class="reserved">class</span> <span class="clazz">PositionWiseFeedForward</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">hidden_dim</span>, <span class="var">ffn_dim</span>, <span class="var">dropout</span>, <span class="var">bias</span>):
        <span class="clazz">super</span>(<span class="clazz">PositionWiseFeedForward</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">hidden_dim</span> = <span class="var">hidden_dim</span>
        <span class="var">self</span>.<span class="var">ffn_dim</span> = <span class="var">ffn_dim</span>
        <span class="var">self</span>.<span class="var">dropout</span> = <span class="var">dropout</span>
        <span class="var">self</span>.<span class="var">bias</span> = <span class="var">bias</span>

        <span class="var">self</span>.<span class="var">FFN1</span> = <span class="clazz">nn</span>.<span class="clazz">Sequential</span>(
            <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">ffn_dim</span>, <span class="var">bias</span>=<span class="var">self</span>.<span class="var">bias</span>),
            <span class="clazz">nn</span>.<span class="clazz">GELU</span>(),
            <span class="clazz">nn</span>.<span class="clazz">Dropout</span>(<span class="var">self</span>.<span class="var">dropout</span>)
        )
        <span class="var">self</span>.<span class="var">FFN2</span> = <span class="clazz">nn</span>.<span class="clazz">Sequential</span>(
            <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">ffn_dim</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">bias</span>=<span class="var">self</span>.<span class="var">bias</span>),
        )
        <span class="var">self</span>.<span class="method">init_weights</span>()


    <span class="reserved">def</span> <span class="method">init_weights</span>(<span class="var">self</span>):
        <span class="return">for</span> <span class="var">_</span>, <span class="var">param</span> <span class="return">in</span> <span class="var">self</span>.<span class="method">named_parameters</span>():
            <span class="return">if</span> <span class="var">param</span>.<span class="var">requires_grad</span>:
                <span class="clazz">nn</span>.<span class="clazz">init</span>.<span class="method">normal</span>_(<span class="var">param</span>.<span class="var">data</span>, <span class="var">mean</span>=<span class="num">0</span>, <span class="var">std</span>=<span class="num"><span class="num">0</span>.5</span>)

    
    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">x</span>):
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">FFN1</span>(<span class="var">x</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">FFN2</span>(<span class="var">output</span>)
        <span class="return">return</span> <span class="var">output</span>
</code></pre>
                    <p>
                        <ul>
                            <li>5 ~ 8번째 줄: Feed forward network를 위해 필요한 파라미터 정의.</li>
                            <li>10 ~ 17번째 줄: 첫 번째, 두 번째 linear layer 정의.</li>
                            <li>18 ~ 24번째 줄: Feed forward network 가중치 초기화.</li>
                            <li>27 ~ 30번째 줄: Feed forward network 거치는 부분.</li>
                        </ul>

                    </p>
                    <p>
                        <br><br><br>이제는 transformer의 encoder 부분입니다.
                        아래 코드의 config.의 부분은 <a href="https://github.com/ljm565/neural-machine-translator-transformer" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">GitHub 코드</span></a>에 보면 src/config.json이라는 파일에 존재하는 변수 값들을 모델에 적용하여 초기화 하는 것입니다.
                        <br><br><span style="font-size: 20px;"><b>Encoder</b></span>
                    </p>

<pre><code class="python"><span class="annot"># a single encoder layer</span>
<span class="reserved">class</span> <span class="clazz"><span class="clazz">Encoder</span>Layer</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">hidden_dim</span>, <span class="var">ffn_dim</span>, <span class="var">num_head</span>, <span class="var">bias</span>, <span class="var">dropout</span>, <span class="var">layernorm_eps</span>):
        <span class="clazz">super</span>(<span class="clazz"><span class="clazz">Encoder</span>Layer</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">hidden_dim</span> = <span class="var">hidden_dim</span>
        <span class="var">self</span>.<span class="var">ffn_dim</span> = <span class="var">ffn_dim</span>
        <span class="var">self</span>.<span class="var">num_head</span> = <span class="var">num_head</span>
        <span class="var">self</span>.<span class="var">bias</span> = <span class="var">bias</span>
        <span class="var">self</span>.<span class="var">dropout</span> = <span class="var">dropout</span>
        <span class="var">self</span>.<span class="var">layernorm_eps</span> = <span class="var">layernorm_eps</span>
        <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span> = <span class="clazz">nn</span>.<span class="clazz">Dropout</span>(<span class="var">self</span>.<span class="var">dropout</span>)
        <span class="var">self</span>.<span class="var">layer_norm</span> = <span class="clazz">nn</span>.<span class="clazz">LayerNorm</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">eps</span>=<span class="var">self</span>.<span class="var">layernorm_eps</span>)

        <span class="var">self</span>.<span class="var">self_attention</span> = <span class="clazz">MultiHeadAttention</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">num_head</span>, <span class="var">self</span>.<span class="var">bias</span>, <span class="var">self_attn</span>=<span class="reserved">True</span>, <span class="var">causal</span>=<span class="reserved">False</span>)
        <span class="var">self</span>.<span class="var">positionWiseFeedForward</span> = <span class="clazz">PositionWiseFeedForward</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">ffn_dim</span>, <span class="var">self</span>.<span class="var">dropout</span>, <span class="var">self</span>.<span class="var">bias</span>)


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">x</span>, <span class="var">mask</span>):
        <span class="var">attn_wts</span>, <span class="var">output</span> = <span class="var">self</span>.<span class="var">self_attention</span>(<span class="var">query</span>=<span class="var">x</span>, <span class="var">key</span>=<span class="var">x</span>, <span class="var">value</span>=<span class="var">x</span>, <span class="var">mask</span>=<span class="var">mask</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span>(<span class="var">output</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">layer_norm</span>(<span class="var">x</span> + <span class="var">output</span>)

        <span class="var">x</span> = <span class="var">output</span>
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">positionWiseFeedForward</span>(<span class="var">output</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span>(<span class="var">output</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">layer_norm</span>(<span class="var">x</span> + <span class="var">output</span>)

        <span class="return">return</span> <span class="var">attn_wts</span>, <span class="var">output</span>



<span class="annot"># all encoders</span>
<span class="reserved">class</span> <span class="clazz">Encoder</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">config</span>, <span class="var">tokenizer</span>, <span class="var">device</span>):
        <span class="clazz">super</span>(<span class="clazz">Encoder</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">vocab_size</span> = <span class="var">tokenizer</span>.<span class="var">vocab_size</span>
        <span class="var">self</span>.<span class="var">pad_token_id</span> = <span class="var">tokenizer</span>.<span class="var">pad_token_id</span>
        <span class="var">self</span>.<span class="var">device</span> = <span class="var">device</span>

        <span class="var">self</span>.<span class="var">enc_num_layers</span> = <span class="var">config</span>.enc_num_layers
        <span class="var">self</span>.<span class="var">hidden_dim</span> = <span class="var">config</span>.hidden_dim
        <span class="var">self</span>.<span class="var">ffn_dim</span> = <span class="var">config</span>.ffn_dim
        <span class="var">self</span>.<span class="var">num_head</span> = <span class="var">config</span>.num_head
        <span class="var">self</span>.<span class="var">max_len</span> = <span class="var">config</span>.max_len
        <span class="var">self</span>.<span class="var">bias</span> = <span class="clazz">bool</span>(<span class="var">config</span>.bias)
        <span class="var">self</span>.<span class="var">dropout</span> = <span class="var">config</span>.dropout
        <span class="var">self</span>.<span class="var">layernorm_eps</span> = <span class="var">config</span>.layernorm_eps
        <span class="var">self</span>.<span class="var">pos_encoding</span> = <span class="var">config</span>.pos_encoding
        
        <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span> = <span class="clazz">nn</span>.<span class="clazz">Dropout</span>(<span class="var">self</span>.<span class="var">dropout</span>)
        <span class="var">self</span>.<span class="var">emb_layer</span> = <span class="clazz">Embeddings</span>(<span class="var">self</span>.<span class="var">vocab_size</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">pad_token_id</span>)
        <span class="var">self</span>.<span class="var">pos_layer</span> = <span class="clazz">PositionalEncoding</span>(<span class="var">self</span>.<span class="var">max_len</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">pos_encoding</span>, <span class="var">self</span>.<span class="var">device</span>)
        <span class="var">self</span>.<span class="var"><span class="var">encoder</span>s</span> = <span class="clazz">nn</span>.<span class="clazz"><span class="clazz">Module</span>List</span>([<span class="clazz"><span class="clazz">Encoder</span>Layer</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">ffn_dim</span>, <span class="var">self</span>.<span class="var">num_head</span>, <span class="var">self</span>.<span class="var">bias</span>, <span class="var">self</span>.<span class="var">dropout</span>, <span class="var">self</span>.<span class="var">layernorm_eps</span>) <span class="return">for</span> <span class="var">_</span> <span class="return">in</span> <span class="clazz">range</span>(<span class="var">self</span>.<span class="var">enc_num_layers</span>)])


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">x</span>, <span class="var">mask</span>=<span class="reserved">None</span>):
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">emb_layer</span>(<span class="var">x</span>) + <span class="var">self</span>.<span class="var">pos_layer</span>(<span class="var">x</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span>(<span class="var">output</span>)

        <span class="var">all_<span class="var">attn_wts</span></span> = []
        <span class="return">for</span> <span class="var">encoder</span> <span class="return">in</span> <span class="var">self</span>.<span class="var"><span class="var">encoder</span>s</span>:
            <span class="var">attn_wts</span>, <span class="var">output</span> = <span class="var">encoder</span>(<span class="var">output</span>, <span class="var">mask</span>)
            <span class="var">all_<span class="var">attn_wts</span></span>.<span class="method">append</span>(<span class="var">attn_wts</span>.detach().cpu())
        
        <span class="return">return</span> <span class="var">all_<span class="var">attn_wts</span></span>, <span class="var">output</span>
</code></pre>
                    <p>
                        <ul>
                            <li>2 ~ 28번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">하나의 encoder block을 정의하는 코드</span>.</li>
                            <li>33 ~ 65번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Encoder block을 레이어 개수만큼 쌓아 전체 encoder를 정의하는 코드</span>.</li>
                            <li>5 ~ 12번째 줄: Encoder block을 제작할 때 필요한 파라미터 정의.</li>
                            <li>14번째 줄: Self attention 정의.</li>
                            <li>15번째 줄: Feed forward network 정의.</li>
                            <li>18 ~ 28번째 줄: Encoder block을 거치는 부분.</li>
                            <li>21, 26번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Residual connection 부분</span>.</li>
                            <li>36 ~ 48번째 줄: 전체 encoder를 제작하기 위한 파라미터 정의.</li>
                            <li>51 ~ 52번째 줄: Encoder의 embedding 레이어 정의.</li>
                            <li>53번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Encoder의 레이어 개수만큼 encoder block을 쌓아 정의하는 부분</span>.</li>
                            <li>56 ~ 65번째 줄: Encoder(모든 encoder block)를 거치는 부분.</li>
                        </ul>

                    </p>
                    <p>
                        <br><br><br>이제는 transformer의 decoder 부분입니다.
                        아래 코드의 config.의 부분은 <a href="https://github.com/ljm565/neural-machine-translator-transformer" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">GitHub 코드</span></a>에 보면 src/config.json이라는 파일에 존재하는 변수 값들을 모델에 적용하여 초기화 하는 것입니다.
                        <br><br><span style="font-size: 20px;"><b>Decoder</b></span>
                    </p>



<pre><code class="python"><span class="annot"># a single decoder layer</span>
<span class="reserved">class</span> <span class="clazz"><span class="clazz">Decoder</span>Layer</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">hidden_dim</span>, <span class="var">ffn_dim</span>, <span class="var">num_head</span>, <span class="var">bias</span>, <span class="var">dropout</span>, <span class="var">layernorm_eps</span>):
        <span class="clazz">super</span>(<span class="clazz"><span class="clazz">Decoder</span>Layer</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">hidden_dim</span> = <span class="var">hidden_dim</span>
        <span class="var">self</span>.<span class="var">ffn_dim</span> = <span class="var">ffn_dim</span>
        <span class="var">self</span>.<span class="var">num_head</span> = <span class="var">num_head</span>
        <span class="var">self</span>.<span class="var">bias</span> = <span class="var">bias</span>
        <span class="var">self</span>.<span class="var">dropout</span> = <span class="var">dropout</span>
        <span class="var">self</span>.<span class="var">layernorm_eps</span> = <span class="var">layernorm_eps</span>
        <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span> = <span class="clazz">nn</span>.<span class="clazz">Dropout</span>(<span class="var">self</span>.<span class="var">dropout</span>)
        <span class="var">self</span>.<span class="var">layer_norm</span> = <span class="clazz">nn</span>.<span class="clazz">LayerNorm</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">eps</span>=<span class="var">self</span>.<span class="var">layernorm_eps</span>)

        <span class="var">self</span>.<span class="var">masked_<span class="var">self_attention</span></span> = <span class="clazz">MultiHeadAttention</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">num_head</span>, <span class="var">self</span>.<span class="var">bias</span>, <span class="var">self_attn</span>=<span class="reserved">True</span>, <span class="var">causal</span>=<span class="reserved">True</span>)
        <span class="var">self</span>.<span class="var">enc_dec_attention</span> = <span class="clazz">MultiHeadAttention</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">num_head</span>, <span class="var">self</span>.<span class="var">bias</span>, <span class="var">self_attn</span>=<span class="reserved">False</span>, <span class="var">causal</span>=<span class="reserved">False</span>)
        <span class="var">self</span>.<span class="var">positionWiseFeedForward</span> = <span class="clazz">PositionWiseFeedForward</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">ffn_dim</span>, <span class="var">self</span>.<span class="var">dropout</span>, <span class="var">self</span>.<span class="var">bias</span>)


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">x</span>, <span class="var">enc_<span class="var">output</span></span>, <span class="var">dec_<span class="var">causal</span>_mask</span>, <span class="var">enc_dec_mask</span>):
        <span class="var">dec_<span class="var">self_attn</span>_wts</span>, <span class="var">output</span> = <span class="var">self</span>.<span class="var">masked_<span class="var">self_attention</span></span>(<span class="var">query</span>=<span class="var">x</span>, <span class="var">key</span>=<span class="var">x</span>, <span class="var">value</span>=<span class="var">x</span>, <span class="var">mask</span>=<span class="var">dec_<span class="var">causal</span>_mask</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span>(<span class="var">output</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">layer_norm</span>(<span class="var">x</span> + <span class="var">output</span>)

        <span class="var">x</span> = <span class="var">output</span>
        <span class="var">cross_<span class="var">attn_wts</span></span>, <span class="var">output</span> = <span class="var">self</span>.<span class="var">enc_dec_attention</span>(<span class="var">query</span>=<span class="var">x</span>, <span class="var">key</span>=<span class="var">enc_<span class="var">output</span></span>, <span class="var">value</span>=<span class="var">enc_<span class="var">output</span></span>, <span class="var">mask</span>=<span class="var">enc_dec_mask</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span>(<span class="var">output</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">layer_norm</span>(<span class="var">x</span> + <span class="var">output</span>)

        <span class="var">x</span> = <span class="var">output</span>
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">positionWiseFeedForward</span>(<span class="var">output</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span>(<span class="var">output</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">layer_norm</span>(<span class="var">x</span> + <span class="var">output</span>)

        <span class="return">return</span> <span class="var">dec_<span class="var">self_attn</span>_wts</span>, <span class="var">cross_<span class="var">attn_wts</span></span>, <span class="var">output</span>



<span class="annot"># all decoders</span>
<span class="reserved">class</span> <span class="clazz">Decoder</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">config</span>, <span class="var">tokenizer</span>, <span class="var">device</span>):
        <span class="clazz">super</span>(<span class="clazz">Decoder</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">vocab_size</span> = <span class="var">tokenizer</span>.<span class="var">vocab_size</span>
        <span class="var">self</span>.<span class="var">pad_token_id</span> = <span class="var">tokenizer</span>.<span class="var">pad_token_id</span>
        <span class="var">self</span>.<span class="var">device</span> = <span class="var">device</span>

        <span class="var">self</span>.<span class="var">dec_num_layers</span> = <span class="var">config</span>.dec_num_layers
        <span class="var">self</span>.<span class="var">hidden_dim</span> = <span class="var">config</span>.hidden_dim
        <span class="var">self</span>.<span class="var">ffn_dim</span> = <span class="var">config</span>.ffn_dim
        <span class="var">self</span>.<span class="var">num_head</span> = <span class="var">config</span>.num_head
        <span class="var">self</span>.<span class="var">max_len</span> = <span class="var">config</span>.max_len
        <span class="var">self</span>.<span class="var">bias</span> = <span class="clazz">bool</span>(<span class="var">config</span>.bias)
        <span class="var">self</span>.<span class="var">dropout</span> = <span class="var">config</span>.dropout
        <span class="var">self</span>.<span class="var">layernorm_eps</span> = <span class="var">config</span>.layernorm_eps
        <span class="var">self</span>.<span class="var">pos_encoding</span> = <span class="var">config</span>.pos_encoding

        <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span> = <span class="clazz">nn</span>.<span class="clazz">Dropout</span>(<span class="var">self</span>.<span class="var">dropout</span>)
        <span class="var">self</span>.<span class="var">emb_layer</span> = <span class="clazz">Embeddings</span>(<span class="var">self</span>.<span class="var">vocab_size</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">pad_token_id</span>)
        <span class="var">self</span>.<span class="var">pos_layer</span> = <span class="clazz">PositionalEncoding</span>(<span class="var">self</span>.<span class="var">max_len</span>, <span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">pos_encoding</span>, <span class="var">self</span>.<span class="var">device</span>)
        <span class="var">self</span>.<span class="var"><span class="var">decoder</span>s</span> = <span class="clazz">nn</span>.<span class="clazz"><span class="clazz">Module</span>List</span>([<span class="clazz"><span class="clazz">Decoder</span>Layer</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var">ffn_dim</span>, <span class="var">self</span>.<span class="var">num_head</span>, <span class="var">self</span>.<span class="var">bias</span>, <span class="var">self</span>.<span class="var">dropout</span>, <span class="var">self</span>.<span class="var">layernorm_eps</span>) <span class="return">for</span> <span class="var">_</span> <span class="return">in</span> <span class="clazz">range</span>(<span class="var">self</span>.<span class="var">dec_num_layers</span>)])


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">x</span>, <span class="var">enc_<span class="var">output</span></span>, <span class="var">dec_<span class="var">causal</span>_mask</span>=<span class="reserved">None</span>, <span class="var">enc_dec_mask</span>=<span class="reserved">None</span>):
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">emb_layer</span>(<span class="var">x</span>) + <span class="var">self</span>.<span class="var">pos_layer</span>(<span class="var">x</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var"><span class="var">dropout</span>_layer</span>(<span class="var">output</span>)

        <span class="var">all_<span class="var">self_attn</span>_wts</span>, <span class="var">all_<span class="var">cross_<span class="var">attn_wts</span></span></span> = [], []
        <span class="return">for</span> <span class="var">decoder</span> <span class="return">in</span> <span class="var">self</span>.<span class="var"><span class="var">decoder</span>s</span>:
            <span class="var">dec_<span class="var">self_attn</span>_wts</span>, <span class="var">cross_<span class="var">attn_wts</span></span>, <span class="var">output</span> = <span class="var">decoder</span>(<span class="var">output</span>, <span class="var">enc_<span class="var">output</span></span>, <span class="var">dec_<span class="var">causal</span>_mask</span>, <span class="var">enc_dec_mask</span>)
            <span class="var">all_<span class="var">self_attn</span>_wts</span>.<span class="method">append</span>(<span class="var">dec_<span class="var">self_attn</span>_wts</span>.detach().cpu())
            <span class="var">all_<span class="var">cross_<span class="var">attn_wts</span></span></span>.<span class="method">append</span>(<span class="var">cross_<span class="var">attn_wts</span></span>.detach().cpu())
        
        <span class="return">return</span> <span class="var">all_<span class="var">cross_<span class="var">attn_wts</span></span></span>, <span class="var">output</span>
</code></pre>
                    <p>
                        <ul>
                            <li>2 ~ 34번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">하나의 decoder block을 정의하는 코드</span>.</li>
                            <li>39 ~ 72번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Decoder block을 레이어 개수만큼 쌓아 전체 decoder를 정의하는 코드</span>.</li>
                            <li>5 ~ 12번째 줄: Decoder block을 제작할 때 필요한 파라미터 정의.</li>
                            <li>14번째 줄: Causal mask를 적용한 maked self attention 정의.</li>
                            <li>15번째 줄: encoder-decoder attention 정의.</li>
                            <li>16번째 줄: Feed forward network 정의.</li>
                            <li>19 ~ 34번째 줄: Decoder block을 거치는 부분.</li>
                            <li>22, 27, 32번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Residual connection 부분</span>.</li>
                            <li>42 ~ 54번째 줄: 전체 decoder를 제작하기 위한 파라미터 정의.</li>
                            <li>57 ~ 58번째 줄: Decoder의 embedding 레이어 정의.</li>
                            <li>59번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">Decoder의 레이어 개수만큼 decoder block을 쌓아 정의하는 부분</span>.</li>
                            <li>62 ~ 75번째 줄: Decoder(모든 decoder block)를 거치는 부분.</li>
                        </ul>

                    </p>
                    <p>
                        <br><br><br>이제는 encoder와 decoder를 합쳐 transformer를 구성하는 부분입니다.
                        <br><br><span style="font-size: 20px;"><b>Transformer</b></span>
                    </p>

<pre><code class="python"><span class="annot"># transformer</span>
<span class="reserved">class</span> <span class="clazz">Transformer</span>(<span class="clazz">nn</span>.<span class="clazz">Module</span>):
    <span class="reserved">def</span> <span class="method">__init__</span>(<span class="var">self</span>, <span class="var">config</span>, <span class="var"><span class="var">tokenizer</span>s</span>, <span class="var">device</span>):
        <span class="clazz">super</span>(<span class="clazz">Transformer</span>, <span class="var">self</span>).<span class="method">__init__</span>()
        <span class="var">self</span>.<span class="var">config</span> = <span class="var">config</span>
        <span class="var">self</span>.<span class="var"><span class="var">src</span>_tokenizer</span>, <span class="var">self</span>.<span class="var"><span class="var">trg</span>_tokenizer</span> = <span class="var"><span class="var">tokenizer</span>s</span>
        <span class="var">self</span>.<span class="var">device</span> = <span class="var">device</span>
        
        <span class="var">self</span>.<span class="var">hidden_dim</span> = <span class="var">self</span>.<span class="var">config</span>.hidden_dim

        <span class="var">self</span>.<span class="var">encoder</span> = <span class="clazz">Encoder</span>(<span class="var">self</span>.<span class="var">config</span>, <span class="var">self</span>.<span class="var"><span class="var">src</span>_tokenizer</span>, <span class="var">self</span>.<span class="var">device</span>)
        <span class="var">self</span>.<span class="var">decoder</span> = <span class="clazz">Decoder</span>(<span class="var">self</span>.<span class="var">config</span>, <span class="var">self</span>.<span class="var"><span class="var">trg</span>_tokenizer</span>, <span class="var">self</span>.<span class="var">device</span>)
        <span class="var">self</span>.<span class="var">fc</span> = <span class="clazz">nn</span>.<span class="clazz">Linear</span>(<span class="var">self</span>.<span class="var">hidden_dim</span>, <span class="var">self</span>.<span class="var"><span class="var">trg</span>_tokenizer</span>.vocab_size)


    <span class="reserved">def</span> <span class="method">make_mask</span>(<span class="var">self</span>, <span class="var">src</span>, <span class="var">trg</span>):
        <span class="var">enc_mask</span> = <span class="clazz">torch</span>.<span class="method">where</span>(<span class="var">src</span>==<span class="var">self</span>.<span class="var"><span class="var">src</span>_tokenizer</span>.pad_token_id, <span class="num">0</span>, <span class="num">1</span>).<span class="method">unsqueeze</span>(<span class="num">1</span>).<span class="method">unsqueeze</span>(<span class="num">2</span>)
        <span class="var">dec_<span class="var">causal</span>_mask</span> = <span class="clazz">torch</span>.<span class="method">tril</span>(<span class="clazz">torch</span>.<span class="method">ones</span>(<span class="var">trg</span>.size(<span class="num">1</span>), <span class="var">trg</span>.size(<span class="num">1</span>))).<span class="method">unsqueeze</span>(<span class="num">0</span>).<span class="method">unsqueeze</span>(<span class="num">1</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>) + <span class="clazz">torch</span>.<span class="method">where</span>(<span class="var">trg</span>==<span class="var">self</span>.<span class="var"><span class="var">trg</span>_tokenizesr</span>.<span class="var">pad_token_id</span>, <span class="num">0</span>, <span class="num">1</span>).<span class="method">unsqueeze</span>(<span class="num">1</span>).<span class="method">unsqueeze</span>(<span class="num">2</span>)
        <span class="var">dec_<span class="var">causal</span>_mask</span> = <span class="clazz">torch</span>.<span class="method">where</span>(<span class="var">dec_<span class="var">causal</span>_mask</span> &lt; <span class="num">2</span>, <span class="num">0</span>, <span class="num">1</span>)
        <span class="var">enc_dec_mask</span> = <span class="var">enc_mask</span>
        <span class="return">return</span> <span class="var">enc_mask</span>, <span class="var">dec_<span class="var">causal</span>_mask</span>, <span class="var">enc_dec_mask</span>


    <span class="reserved">def</span> <span class="method">forward</span>(<span class="var">self</span>, <span class="var">src</span>, <span class="var">trg</span>):
        <span class="var">enc_mask</span>, <span class="var">dec_<span class="var">causal</span>_mask</span>, <span class="var">enc_dec_mask</span> = <span class="var">self</span>.<span class="method">make_mask</span>(<span class="var">src</span>, <span class="var">trg</span>)
        <span class="var">all_<span class="var">attn_wts</span></span>, <span class="var">enc_<span class="var">output</span></span> = <span class="var">self</span>.<span class="var">encoder</span>(<span class="var">src</span>, <span class="var">enc_mask</span>)
        <span class="var">all_<span class="var">cross_<span class="var">attn_wts</span></span></span>, <span class="var">output</span> = <span class="var">self</span>.<span class="var">decoder</span>(<span class="var">trg</span>, <span class="var">enc_<span class="var">output</span></span>, <span class="var">dec_<span class="var">causal</span>_mask</span>, <span class="var">enc_dec_mask</span>)
        <span class="var">output</span> = <span class="var">self</span>.<span class="var">fc</span>(<span class="var">output</span>)
        <span class="return">return</span> <span class="var">all_<span class="var">cross_<span class="var">attn_wts</span></span></span>, <span class="var">output</span>
</code></pre>

                    <p>
                        <ul>
                            <li>5 ~ 13번째 줄: Fully-connected layer, encoder, decoder 등을 정의하는 부분.</li>
                            <li>16 ~ 21번째 줄: <span class="highlight" style="color: rgb(0, 3, 206);">pad mask, causal mask 등을 제작하는 부분</span>.</li>
                            <li>24 ~ 29번째 줄: Encoder, decoder를 거치는 전체 transformer 부분.</li>
                        </ul>

                    </p>




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>Transformer 학습</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제 기계 번역 모델 학습 코드를 통해 어떻게 학습이 이루어지는지 살펴보겠습니다.
                        아래 코드에 <span style="color:rgb(86, 155, 214);">self</span>. 이라고 나와있는 부분은 GitHub 코드에 보면 알겠지만 학습하는 코드가 class 내부의 변수이기 때문에 있는 것입니다.
                        여기서는 무시해도 좋습니다.
                        <br><br>그리고 아래 학습 코드는 실제 학습 코드를 간소화한 것입니다. Scheduler 등 전체 학습 코드는 <a href="https://github.com/ljm565/neural-machine-translator-transformer" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">GitHub 코드</span></a>를 참고하면 됩니다.
                    </p>


<pre><code class="python"><span class="var">self</span>.<span class="var">model</span> = <span class="clazz">Transformer</span>(<span class="var">self</span>.<span class="var">config</span>, <span class="var">self</span>.<span class="var"><span class="var">tokenizer</span>s</span>, <span class="var">self</span>.<span class="var">device</span>).<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
<span class="var">self</span>.<span class="var">criterion</span> = <span class="clazz">nn</span>.<span class="clazz">CrossEntropyLoss</span>(<span class="var">ignore_index</span>=<span class="var">self</span>.<span class="var"><span class="var">trg</span>_<span class="var">tokenizer</span></span>.<span class="var">pad_token_id</span>)
<span class="var">self</span>.<span class="var">optimizer</span> = <span class="clazz">optim</span>.<span class="clazz">Adam</span>(<span class="var">self</span>.<span class="var">model</span>.<span class="method">parameters</span>(), <span class="var">lr</span>=<span class="var">self</span>.<span class="var">lr</span>)

<span class="var">self</span>.<span class="var">model</span>.<span class="method">train</span>()

<span class="return">for</span> <span class="var">i</span>, (<span class="var">src</span>, <span class="var">trg</span>) <span class="return">in</span> <span class="clazz">enumerate</span>(<span class="var">self</span>.<span class="var">dataloaders</span>[<span class="str">'train'</span>]):
    <span class="var">batch</span> = <span class="var">src</span>.size(<span class="num">0</span>)
    <span class="var">src</span>, <span class="var">trg</span> = <span class="var">src</span>.<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>), <span class="var">trg</span>.<span class="method">to</span>(<span class="var">self</span>.<span class="var">device</span>)
    <span class="var">self</span>.<span class="var">optimizer</span>.<span class="method">zero_grad</span>()

    <span class="return">with</span> <span class="clazz">torch</span>.<span class="clazz">set_grad_enabled</span>(<span class="var">phase</span>==<span class="str">'train'</span>):
        <span class="var">_</span>, <span class="var">output</span> = <span class="var">self</span>.<span class="var">model</span>(<span class="var">src</span>, <span class="var">trg</span>)
        <span class="var">loss</span> = <span class="var">self</span>.<span class="var">criterion</span>(<span class="var">output</span>[:, :<span class="num">-<span class="num">1</span></span>, :].reshape(<span class="num">-<span class="num">1</span></span>, <span class="var">output</span>.size(<span class="num">-<span class="num">1</span></span>)), <span class="var">trg</span>[:, <span class="num">1</span>:].reshape(<span class="num">-<span class="num">1</span></span>))
        <span class="var">loss</span>.backward()
        <span class="var">self</span>.<span class="var">optimizer</span>.<span class="method">step</span>()</code></pre>

                    <p>
                        <span style="font-size: 20px;"><b>학습에 필요한 것들 선언</b></span>
                        <br>먼저 위에 코드에서 정의한 모델을 불러오고 학습에 필요한 loss function, optimizer 등을 선언하는 부분입니다.
                        <ul>
                            <li>1 ~ 3번째 줄: Loss function, transformer 모델 및 optimizer 선언.</li>
                        </ul>

                        <br><span style="font-size: 20px;"><b>모델 학습</b></span>
                        <ul>
                            <li>5 ~ 16번째 줄: Cross entropy loss를 이용하여 모델 학습하는 부분.</li>
                            <li>14 ~ 16번째 줄: Loss를 계산하고 모델을 업데이트 하는 부분.</li>
                        </ul>
                    </p>





                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>Transformer 학습 결과</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제 WMT'14, IWSLT'14 validation set의 BLEU-4 history와 validation set의 최대의 BLEU-4일 때 test set의 BLEU-4 결과를 살펴보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 test set의 결과를 낼 때는 NLTK뿐 아니라 많이 사용되는 multi_bleu.perl을 이용하여 계산한 결과도 보여줍니다.</span>
                        multi_bleu.perl을 사용하는 방법은 <a href="https://github.com/ljm565/neural-machine-translator-transformer" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">GitHub 코드</span></a>를 참고하시기 바랍니다.
                        그리고 각 데이터에 대해 번역한 ground truth (GT) 값과 예측값도 비교해보겠습니다.
                        
                        <br><br><span style="font-size: 20px;"><b>IWSLT'14 결과</b></span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FyI_4pqugl4czV3FXegOJClD4AbqJb0bTpMhRwvIVQw4_rNW-G_Kkw2NDVkspohHLQ24oqz56W4jyfmKVsE3fKFNFfq8N-HcLSFXGT-o4RWT02O9lngaIRITb6Zl_h5dW-i0BMZX8fjpaNvhzfvTMcFYQhRzO1TbpYE0tMgTylSqGz2XFW9gSQTT6qR5FNvpuyGAShXGD7HIPhHZr-vTkJadkV2oWmn1lXUy5yGZafKLY5CJDsjZS14emwEFUDlwZk-tRoA_XK3qmWj-Zu6-pkQiS_iNxdLxnalPhPF-mfPJoGPHeKvWtB3HRNOrBhaziUETGBzZws2DubKdPEsyX3SLFhnqRhEKy3XalWMhZuHjExTcpd_6v_jZt7Yajtao_XYz3cGz7NmGdQrxAOooOSRKHwb1ZebxW9aN3LBHL2mvEhA4p1k3CrLXvv2MBpeyYX5z3jghnVkUqYit8-Fzer8wgEjfjKLQCDo9VI5A0W_iHlBhnnV5TFjvG7PC2ySALLexg4yDQi0UeReDPT-7QuLEsGiOPLuwTJgR4uR67V7tHyDnVVYJOot2QNSqmPbcti-3pc-Y0a95LySf7tVN6v9Z0U1Jg8oHaxc1WSCGtiOs786LcCxyX79PzYebrrR8gx0WCIMZa4rp5Yziw19uTwJyGCu4JhHx_8cU7QC1wRr1M5gDZffvxELnfhfusK2YtCfUzTNThSP4P25FOspLbgrNp7nebPWF1QvDnzWZmmNIiMDaTSYsx8bogs3Ni3h14hXQJVa3pf5lr5HZCKBf_ghX5DSw2jjijiVJteBp55QcKa08GF5HayhbStgFhWRYaoICmCpnMN9vLXcf0dEaiodNQEWRZ6HSHp3QUn0IO6B1Z4Q_rfKtJvH8J803E6BWszzliitGPYGA4D8o52b9A6AkrhniP7szMeEzOhBP4pfEzfvBXmUXY_yQDHboujvh10lClfLy_yObJfufBy3GmnCLqD73om6CtIMENZyEq__OaOgR66vTAKbYsNUBdkUuExbmCcWjzg2_cyu5ny1r1fRcGbcGeISgi_v6b2N8WftuqC2ooISi7IyiBSknY0wsTXL6GfUjKi2ogDKVrb-nAkUXe99JjZ1SJvK_8n4Rjtb2SNegFQlQ9yatCANNOSCKn-ux_wc0zvtKW16ERAhz09gyx6B5EkRMVVARb3JzeUZU9RhP5QHEXx5j7Rn_NitNOCQjTlzLB7-8OZVSbdbz1UmUselX4etTaQWNmuHRzUaWq2qlba9aSwjn_wWZ8L5UB4ig9d9gcXDhTnAXGygIhS0P8G8NEB1tz3gBy8tCKvjisFv6KYazob8dC3usjK4vBpZiztiE2OpRNTX7kqjfvQ2y-k3qSj7xKCWv__cOgjkp_-u5xRJXSrv9r_em2nxTjz_ibKXXZVMsJhHzXoyAG6vleDmdcKJXDIpNbw8QtNJhl5BKLG_09IinmWyaZCFRuHSzx_m5xlY6awQDj2FJD7qOAZ_uhrmr7bHrNzXJnCx1REf7L3N648NfolwVdLhb9ikNilvA" style="width: 100%;">
                        <p class="caption">IWSLT'14 BLEU score</p>
                    </div>
                    <p>
                        <ul>
                            <li>Test set BLEU-4: 0.2579 (NLTK)</li>
                            <li>Test set BLEU-4: 0.2580 (multi_bleu.perl)</li>
                        </ul>
                        <br><br>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_H_XFxWfn1h91XX9ogXq63wIYFeYT62uM3jUOItf-7OdDMlk3nSVgQmgi6ZyneFGlB7DVglhSF5OyQ4PbFNxQPAuEU8QjiE3WPDatifDp2s_EObl8THQHT1BMVt0hGqcAy8JH9wtHV1CVmbpQ0PGsaFFfa23wAR6ROmg8B8mciH_ZSqL5EAkigz9M1VqMcKAYcMd1sYVA-bflblCxqNA0AoTOST0ANO85H65NwEP3AvDUQQUQHS9RIVgGw3VjJkD97P6UAknJUus-_IkSNsQvRkpsmcnv1VkSEkQ_Mgp10eU8KP36-BX8juJLaksrksDRoELTwafNbxkjod4UqhGFv5O3bdjHFx8gxVeDsDHmhABG024NA4xyJ9hEotF4oWqKIIyMMpTkk-k35_yQHLgbhP1VU7B_sBOvLHR-PEPV6jzK3LFzZGZXVa58YMyBUZsUhzL6v8gNItYYm9grsol4lhg_D9EhNF3UTXTY_lD3IYJ0k4Y1_n7qmN26rENKPgSbTCTLqnBuFd_AY4u2-yuL8udiMbLgM4vFeyvVfyaIIfw2rPZxagx6etUqdtLqhpL0e7LvJOjgzVgvqz8mSOqwNyh0dSUowBG37bi12jToP7PfIGETXaGQ6pqGwFw1lwFGS190cEeEMf7wWLpDwlUD96rXiTwNHsLhk4ztGk0HnaF_dvwOa8e7lgHY1ZH25R_D-gUgTUkvfGpbBPf_Q8w3rZPRheAAT9ULBvT9vftOolmd4dFerv4uL6TbRgMLhlDYVyWx3TQzCQHeefSR99HXYD1sbXtxsME8_GoaeQrhz-XEFXiHuTpmj3y2vwIub40lYGN_hzLPdz2o9BfiY47aUfv4dka1TPN2t0m3WswgdwpVQtMMd6wFVKYiAONHwp14IyH9v8jgJ3VTxSMSLXZl0fsN-4XU7geBE-ceiz1kXbnyZqycHrrghAAk6ssN8wZX4Ow9DVp7pVyx-SkJux7cbXaT9CWUZVF7-OBOVD2oKoIeifw3gU4KmQ7GGNUEFzVybN_amtsFh1I1xCLwJ10oTeAeEVZHiAq_M5VDeRg549yFy6xufJu4XNDU2t9S3qQsnbPktl4OSZyjGwJIAKpFkGjHOhs3nGBD1WpwC0eiRwJmdL8QbCeCO0uIvlc0vmDFmbunOOsTsBnWHm1oOR-mgzSwUn-EQfQ5kXAvTTy_L2LQYE6BHf6aPL-q0z2HAvtB_z_TwQ_jrH_UdO0ZynpT0XyHax3u7QDzhujD_sK2q38Aek1tQKw_Bl4-V_fK6q-0Al9MJCMZGLEEWuWUMMFLxCMl9JSGBHVlHKOgFyzz4_qI8Egxq7gmaY1MezDf1FiB5iLR2UeaIvTD8jQTIzJR43We9SpGEIs8J_azh8W1SBU4DOSroNSBg0Z73CDi3f5z1RrO0LCxXewIJAh2O9xgOO2l0VJ5XdF4olD90qdJKe9xIkVxtUKY4R6gjXlT27NG5n8gfnQ6SE51fR9Gv0p1IUg6wY_m163o4HXagV9i_jjIhIQdBZ2Bo5K4eQaYpLrgicAbSHJgZW" style="width: 100%;">
                        <p class="caption">IWSLT'14 NIST score</p>
                    </div>
<pre>
<div class="codeWrapper">
<div class="code">
<pre>
<span class="annot"># Sample 1</span>
gt  : ein sehr konk ##rete ##r wunsch , dass wir diese technologie erfinden .
pred: es ist ein sehr konk ##ret wunsch , dass wir diese technologie erfinden .


<span class="annot"># Sample 2</span>
gt  : wir durch ##laufen initi ##ations ##ri ##t ##ual ##e .
pred: wir durch ##laufen den initi ##ations ##ri ##k .


<span class="annot"># Sample 3</span>
gt  : vac ##la ##v have ##l , der große ts ##che ##ch ##ische politiker , hat einmal gesagt :
pred: v ##la ##v haben ##l den großen c ##ze ##ch - anfü ##hrer , darüber gesprochen .
</pre>
</div>
</div>
</pre>
                

                    <p>
                        <br><br><span style="font-size: 20px;"><b>WMT'14 결과</b></span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_F3TUNCP2R9R1PBUfvUffZothX48vdUracS-C-Ua0llua1RPQZpMRKeNnerlf_cr_HRC-p6yd5psiK7hvfrmn0hrkVgNpcN55F8lgTrXXJZaDgWoHEPD3tqxPrRWb-c10rytDbW7N2IKiurCuWOG9XfcP28Qtnwx04X3qw4YDp1TXmu-IwHa7jradUCDACzcmL5X64FoiCBkX-PWmlhE1zIaz9qb5vYfy1zOLxSdH8XcyhvnP1X2COtZMHw4jYxi1_gqkXr9N-wbVzY7Pml3KmKm3fjNzN4BDqUQs3pqwdbXkyBxpWwgCIwZy4mSMof5NsdqlF_D939dYC-sN3lge19wBsh4Qxp_m2BNAFiWgUvRsGOrrteTlZoPNAY2rB_mRAzvwXZcVsJD-8i2olUQVSUh3lAsGXs7nhU5fsQowb8ZT7H8ffYqIF_ChaAr33EHtbXv7UVOhV8koHF3BCNZa-S5cjaolJJVxke8SnArZ4wTVpP0KBdIQommX9GbOfsqeOkS38SAjrFfAWo7OuHgvtjV8btjo2EyAfiqxHvwBWEbz38Zdyx3czLyjUpDQyqAUw1RG8r7k3ml1fuecYpbG3bJRm22EBB5JwDlQau_W5t5N0ZMRQr7oLm1ctxq4xZVJKVC-WnDix5QP_3AKA0-w58SSCL2lI4pQ5JLZxV-YfyVDAZfqFDN1q4XErSLQEPUfsAoo_IOBZCA3lWbXbYq6mKgzdZ6ctgLMCfF-8MMsVK24e8jzL854FR_swmtFH9nIrLqeK-1ppBbsPUrVXqUEjfIrI_h5oAYlUTTdV2VIJHcIkOFIrcd-Go_v34RLZUM5WkCrBw0klRkTOXBjygDH1CsyjgpiohETjdGKlW2h90VmsYqX9GeGkZktdmfUebYbdjiuIwNlbgrGj-AHeaAOtcj3DEl0txf_yRl2CrnQqrgK6PFs20ooPjCWhzFochfV8dlCjp6hQQAMGaZ-DP-crQxhxEo70wG5yury5wamQh-qnieZOSE_aooy5_rlNCv5VZ1gz4OYlaJK42qgKibRbugxpMdG4MASrgtWv7PuFM4EE-0Be1e78ZWasc6xjZdkvNYv3TciZiPB0Mpdf3TxuCTgCetzOqC3l0ykPntb8Lbho8qzbApmFppJ9FWvk4YDJeB2BjWUhyNeu7jb_qezc6AHMAhKOMygkYvg7JLUaR3qL4zbWIbJQiz6Pu0Sy4ZWbhTFEp89Zi46rhBxbID8kC4V_nu-dkldyRmEvI4WpjP2hi8I65I8w4j2cr-BhTN825u4XbSKsfV22XC_vv8XhoG5kC8LJ5wgqApkCQONBcNRJpyapp68XekEuJXSnHb_lyX7bZqH8U84Tu2EpPGpNzHR5abIjeqvev2KJj0oKbVCnlVY6HGQfNEhanFq5OCm7sMV2cpnT0uNcZXAeokj0ROuBOPVYtozTst8QPks6IR2JjWPuucivcjhWtETSnnEH8ZXdzCq7caVhMqC_cslvJmiBnn7gGC5yoA53ujDpCF6ExkK91EeSi_3tE5qthgeEoKGSnfbRa" style="width: 100%;">
                        <p class="caption">WMT'14 BLEU score</p>
                    </div>
                    <p>
                        <ul>
                            <li>Test set BLEU-4: 0.2803 (NLTK)</li>
                            <li>Test set BLEU-4: 0.2803 (multi_bleu.perl)</li>
                        </ul>
                        <br><br>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Fg17Z77EDihdgeP9vXJ06Jp4NIEACmIFCuwbdRt4WSBkbqHn3KcnBuFoI8bQ8m5jU5uP7iUia-n9wDBdKkVNQuxNSitvaUR3dCT_P6DURTWFuZo1LGAbm4tcHB0U7B9CoSK7_0WmaFQI01ol9Drbu37HWbgOLG3ghfqD2xx4yKnNVA5BMeOIKQsIadodEX2UBJtBZSnjl3Auz9iCjgw-GEQ8RUHHOD3kjySZCMSCn3bE8nU2pAd75MJ15FfSxjIOTOWYr9JTyyAQhyOOPZPuQGK9K9EmfZZAcXXJ8QIPMnjXxwDZl2oTiwDtugGIQtSBAgGA8exfB15LEXDenr3HaP29VxXUfNpXhPqcY27ChYrvZxRujFqSd3PYoxLcZya4TLDmHNnUakj3nhUzZr-5gS3JlJCRF3UZ0s5g8uTghESIxHoN-BYhCdBLTunBPZbyC51mAg9EuxMXXIFNY0HGdHv5Skjt-HY5uFMHGrUzQXrjC204UXkT4tdYGQrP1Yt6aYQbMNLEpZn73gQ6bGxHKA35LXrRuH06pQA9cFFWS42PM3cLOtyHS9X2briNRbyPqmpXmhrgZitwqQ9SGLAczTVkTRd01uDprxZyB51AeaZbmCznSof_kpR0z9_h0ETJUlRSFwr7usRwKKSUiRN2O8wZKnQs0VWtsLPZ_1scMNrBPIoV15QBxEiTny2BSKJHwFUgRT4noEwS3RrVyl9_jhnFcVptzNzQcF_01mbBQehIirELpkGxJozZYySsJPKsVjKRNwTkQs-Q3ivIRUygEWkCa7ooSBfUf_vJENYNoah19zxBAtGozg2Mqd8ZxO78lCdLLpWoAA-RrGCy9UldAQ7VvPCRYaBEEkO4Ntlf5YqmjEZWH6z0SE3Dbb0dizuuez7upsrWMSrsv5ZFaECPEE8J7y7KY79HQI06mUvdkU5a5KIbx4t-GF07DXUqwx-i98K2CZ1hH6akND7Bu6GVi87d0Wi6Ffh4xfGMfnrQOZN37QQGSdJHJJzLkrVgkNHoN6jaLZi_Sp5OIGx0l_1NGQL4duCdGf-Vgf5xnvsiGR-RBLCJkLaV5Ir7GuwMhZm0p_IHKEyUXBioKyY029ILYobolRHiamuvkfWDKIONyHh_9_PWf3i3yuiDzeBj_ET3xIghL0eZ3BZKHiPKzDCsH6vOZmTdFpVFcd1ibimMMcZx2gq-qUSs0Z4ZjFqpPDBU-5uTVylKTzgKFcoog_ogBgyLrxTAPcZwo-bCdmj_34Q-fOJrleUpHy3HvRhI5JMBLmsMkZaEBIIUmBjx1UprFF9Wih0zaiYTcOlXHjA-X4QGnglqlM8f4p6FZExUfbQFWFpcb5YIiy5yUJmSkVT8p8-5j8ZJnH_gAJhWw9oh2v0d0C7FNyGIJ3tjP6GbSIEtL9St6-qXMbeBJViY7AY-45pH379VTQIvpws9ubcUKk978mWS2rEY_id9az7xg2y6xDfFXzDJKzafoheUNqC7soGWvBt2S0G3rol8v4FvUFUhefLSx5kGuC-fnT2eMCUyw1w4vNuA" style="width: 100%;">
                        <p class="caption">WMT'14 NIST score</p>
                    </div>
                    
<pre>
<div class="codeWrapper">
<div class="code">
<pre>
<span class="annot"># Sample 1</span>
gt  : „ erwachsene sollten in der lage sein , eigene entscheidungen uber ihr rechtliche ##s geschlecht zu treffen “ , erklarte sie .
pred: " erwachsene sollten in der lage sein , ihre eigenen entscheidungen uber das legal ##e geschlecht zu treffen " , sagte sie .

<span class="annot"># Sample 2</span>
gt  : insgesamt seien vier verkehrs ##schauen durchgefuhrt worden , auch ein kreis ##verkehr wurde ange ##dacht , allerdings wegen der enge in dem kreuzung ##s ##bereich sulz ##bach ##weg / kirchimportant .
pred: insgesamt wurden vier sicherheits ##kontrollen im straßenverkehr durchgefuhrt , und auch ein kreis ##verkehr wurde berucksichtigt , jedoch wurde dieser gedanke aufgrund der engen linien abgelehnt .

<span class="annot"># Sample 3</span>
gt  : austral ##ische flug ##pass ##agi ##e ##re mussen auch weiterhin tablets und smartphones bei start und landung abschalten , trotz bemuhungen in den usa , die regelungen fur derartige gerate
pred: flug ##gaste austral ##ischer fluggesellschaft mussen trotz der bemuhungen in den usa , die bestimmungen uber die flug ##pass ##agi ##e ##re zu locker ##n , ihre flug ##table ##tten
</pre>
</div>
</div>
</pre>
                    
                    
                                       
                    <p>
                        <br><br><br>지금까지 transformer를 통해 IWSLT'14 (En-De)와 대용량 모델 WMT'14 (En-De)를 바탕으로 기계 번역 모델을 학습해보았습니다. 
                        학습 과정에 대한 전체 코드는 <a href="https://github.com/ljm565/neural-machine-translator-transformer" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">GitHub</span></a>에 있으니 참고하시면 될 것 같습니다.
                        GitHub에는 IWSLT'14와 WMT'14의 sample 데이터가 있습니다. 전체 데이터를 받을 수 있는 링크도 GitHub에 나와있으니 참고하면 될 것 같습니다.
                        <br><br>다음에는 transformer를 이용하여 chatbot 학습을 해보겠습니다.
                    </p>

                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#Transformer&emsp;#기계번역&emsp;#WMT'14&emsp;#IWSLT'14
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="pjaxPage('transformer1.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br>Transformer (Attention Is All You Need)</span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('transformer3.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>Transformer를 이용한 한국어 대화 챗봇</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>