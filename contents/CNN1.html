<!DOCTYPE html>
<html>
    <head>
        <title>Convolutional Neural Network (CNN)</title>
        <meta name="description" content="CNN 원리를 설명하고 여러 종류의 CNN과 사용처에 대해 언급합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102" crossorigin="anonymous"></script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Convolutional Neural Network (CNN) / 1. Convolutional Neural Network (CNN)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/AAWUweU58Y44Yu9FlVOW_-fNAiY9Hc-0IBDAUpspf08fmPdV0c3Die5Mwte2J2XxKYb7vIFkOe7QW320pdT1uZLaGXcT3pUgTbCEfIA7bGyehXXHFAgpZ854zhBV6Zn2yieoy-Jqrz_3dypIBLQksQ5DI0j8xfcXTYxbX4eXAFH1e8cGjgNORxCkVfGJtM3qvojeLN6_WPBK01FHEShrxYg2wXF6CPbxvNNMmE0Ps-lOGzHTOZx3kCtduvWpWp-V1EitMDVb7l7uJc85PBbhO0iV0zKZiR2gGQ3_L0W1BCr_--pjGBGmQFHbgCX16biRCIqe-LQIoxgPnxrPfUx2OKRylxoW2AbiJph_49mn5hZpwnfsopejmqfgAQGuRXXruHqUwQ8PtDc3yBkgpQGs2QCMe5PD0muXoIOIizXJbKUzorRlcWj0Rvgp3TuaAD2WZl8cwncF7Gy_cCxGGgx3FUwWXkloI8_Rnj7FywVjwCadzaG5s93yskyPUfxdjzqtvPLyYmFi1aDSu2pdNWMEJblbVLSAout9zC7ZM6JoYVl2yrfMimzU1R-JgBGgI3iw2sXjhF3rMGj-eKVUrXRsN_NUREXViLCbkaka0bJvUIfPQvrxaMcymg7E80SfovCPbiUpbk6BA2Dt7M0VX4l2i53hKURtEqbIyZHne9J_hQNdf9CsWZwgKjSck1uhvoWdS3iYuMnalctppRjjFAmNFiQ5IwITpSbl0dLyxWxFc3UzTEg5abggVQ3MzM7iJzXN-qtAeIasqn1b_i8RF8VaD8lFNqV9Pk_NS_0o5FPYT3THJ05lV_yADzWoM2rveF0U4T1NwkYe7vUp7AsRrORk6bPZkDOcB0rcseVgsBgwsK5oPVXoo534IJVXDBYyjqw7yel7zV8z0J0Dwl4E3XhYaz35cfIsj2mHyasNRpFukvdyW-TKPMsdgwB5sL_c7OJCuAu0z4gsTBLduDZaMcuQa8BV7WAAUcuThuCrfkHNmZAtc4Bf9K4Pwtfx6jWmxh0q7eNxP63AEk_4TmECwnc614aiQxpuko-w--4tk-0XBxbT8OrnhSD510GdyMhkfxucc_ZbY_4pr1Fynr-6uoXwWHIaR8ZOhXXu1KqnzpjSus3XGPfCCIZManrtbwwb9-cTygXxpgqS3E9XfygFkftqRqWub1FN-IyOiD0ye3pB5ImdRF2HWw44ITnbSShI9VPCnf4QbJ4G-Ekz7qogeV4uAsBjaSdcxusWKP4cGhQ117UAACZeHQmewrgff6DTpW25urv1fngd);">
                    <div>
                        <span class="mainTitle">Convolutional Neural Network (CNN)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2022.04.16</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>딥러닝 이야기의 다섯 번째 주제는 Convolutional Neural Network (CNN) 입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">CNN은 현재 가장 많이 사용되는 모델 중 하나입니다. 특히 이미지에서 그 성능이 우수하여 비전 관련 연구에서 많이 사용되는 모델입니다.
                        또한 이미지 뿐 아니라 1D CNN 같은 경우 자연어 처리에 쓰이기도 합니다.</span>

                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">이 글에서는 CNN의 원리와 종류, 그리고 CNN이 작동하기 위해 필요한 stride, pooling 등에 대해 알아보도록 하겠습니다.</span>
                        그리고 CNN이 어디에 사용되는지, 유명한 모델이 무엇이 있는지도 간단하게 살펴보겠습니다.
                    </p>



                    <h1 class="subHead">Convolutional Neural Network (CNN)</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>CNN의 원리</span><br>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        딥러닝의 가장 기초가 되는 레이어는 바로 linear layer 입니다.
                        Linear layer를 이용하는 방법은 매우 간단합니다.
                        데이터에 행렬 계산을 진행하고 non-linear activation function을 거쳐, 비선형 변환으로 데이터를 변형시켜 우리가 하고자하는 task를 수행합니다.
                        따라서 대부분의 데이터는 (batch size * hidden size)의 2차원의 크기를 가진 데이터가 대부분입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 이 글에서는 batch를 제외하고 생각하여 이 데이터들을 1차원의 크기를 가진 데이터라고 생각하겠습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweXgJSwMHhXa8lMIy0u63g7kkoDmjV_FPwdV1lNCOxTbVNRCJO9KLdK12oYMLH6nmdeir7slv23nvlNj7BfjHv2GXAfGCl7naIQS_f_Q0AItRmSNZkcXPhEKL7yIIOv71YFE0bZPcJnAQuQf83mE0HP9VkNCF-Y8z5b4hl7v7T2_Z00PgJmCjt7sUblAu1MkeI03lFAI3fdERfIw2lduK3ZY_8FmgP18uGWjIP6OCtxxlxRc5nxqmRcZuuiHv9mJrvRz48iKQFU8wWMJDp1q4b22vbj2wBiH_SlkgRnpvQGum5h2rERV5_akYU6NAkZ-ess_AA4L7TWFCf5wjFfHlZELGJeRK_SMvkHrAqFKInp_22lRKxaDv9WjTtLoTj19Bbmu58NIcSh3VRqBLoeT7Gz6-331LaxtOPwarrBWuGSMn53GHhdE5RACkD0K6bb6Cr1L7O6zOCDaWb_4e8vjWfuUvU3bdeF02YWF6arJgocrUp2PWFlehO_1tnN9Cpt8AKGaRBRVrKWKFhchWoS-tHw8YKEnOlL76r_l8kIiw5gueeZoPv_aRJZ6tCVRrm31XXwG0iyYCvVwRMo8GkASf65I5UZqNJIs7088v2SG08EGbIzVSUpGEQPKv0XoBIzGogzudWBM1vVWdrvFnWGPZRBGIgE5vG0Iz281iZWUPid9puUz7zXyf1E-ASlZeHBRok7xzD0ceM-KC3MY8k6r2f6u2VdGPz2flO6MSoesFQ9TuX1gwrhRou4507K0y7IjMfOkSL84onp3ln3NdgP5ZB3WEZxDwcNsngiJzpV2rjDYj5xK5L3mIWipSxrohPjDwuyCDFydkMfuW6gaH7JG53y5SP2Si0wdfTbI-kQv5GAqyjjBHuA8plmswSzycwyZ3e5Bor3iM6t_j5iyeUCYST_6aGHpbleCVJ3OKqDiLEcnr7rf_PlwU5PrdJQieaXw9oEh76QVdWDeuoz4aksg3YKE0wWOlUwpFvB2a4m3HbntS4BcKok1IhuEyK5Cw_exKsgMRDeQ5aNROqKZfoS_vLE0ug6eLUwf_3A5kpLvC6Kn7hlBLwy-OyNx_Gv9zJA9WBLHVEr0cCq3ekuVScMm6Z38zo4DHRxvwYxmP-FLJD7s815-aGubQxUw70PUwdtZmkWqBp-WNMzwV5GFzXHBOiaPEi48Upj7TSb0bEjLHOmyUXWq9tbAnCcdzCX78eeiFIP3UIpCybgVCqVTrqguw137b0_Ymqk1vRWvOLsVgao-OXVwkbAnVDh2RNiuqKKFKD-v" style="width: 60%;">
                        <p class="caption">Linear layer를 이용한 학습</p>
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">하지만 CNN은 linear layer를 이용한 단순한 행렬 계산이 아닌, convolution이라는 합성곱을 데이터에 적용합니다.</span>
                        그럼 합성곱 방법은 무엇일까요?
                        합성곱의 방법은 아래의 그림을 보면 단번에 이해가 됩니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweWOfcOc8ire0Xisgsra2joW_iecwiqBX2MwNw1lz5HLgCErH-NJVysIE4gezFl9lijAXCOAnbsBVRjXftFOxTlSgkRV78yNHEg5XQXKSbv9zBjEhUMWiBzajUz2l-MTDF4Xe1LZ3JwQTzRVklwA1-kxWn7UQG669cWks1K06-Vkc-MzloJQ6UG_JOErHNu8PV6FdoqcxxwXwiWiwrFIYWOgtXj60utOlgoFkPXHWymIKCZdmUHmeo-xiWFG9O4r9XmxBMxLiXEOCYPT0Kn_5iC950SMyI4HAKjZDzUq7Pw2isbm8xZTrqN-lfNr6P_pvt0d5TkM40i1_2k-GyEClxun3SXUooLO4GaPSyUmhs1ixc3EdTibVGdth0sWRo24qsmeFEDB0MaNFiJnihn5uj-MYKCEE-ws4V6gTqB63NAH42DQkvHyGpBVRclTWF-Dv-6kJLlTRwqpw9MC9A_TZ6NLxNg1PYOnEUMm-pGKsfXLs6hkUayBBE0te7buxazBHyXnCEfhlyG77aSxkof5NMOMg7I9CPOx09LgOGa06dBq-OYD_2I8Xd_7kULMAVqXc7XU9LLmFl_Y6gUdQsAggRM4Ys6S4egE4OlVCv7nZAnfwpr8ssrTXhSJ2yHnLwdRR62QmUEp2RgINXrdWe1sO_u-8ZF60hL18FPIq0YMZ1GxvxQinh00ewN7tptBXrnMrcRIbVQaIoDOcQyPeJcKRmPgyFL3_4D8R9ztofMjsW58-YcRFjZYEQFKYma3AmEKTVavF13pmIA3TfIXf8FfVVQFfjSvGRuS-7TDM2BBbCmhTX_uyf599DLCOoW5YTwKrJDD3FzET5AFyDvwdAVJGN4fnXls7KuQY7bVhoJkNWbsQBoCc8K9HcWx1kQYnDRGqX8ulwgYA2UD67-JhMuGpWILj-cUxCeksV-6LW1HQGEv6RdG8_s9ZO49gj10Yemri59eDIIjRzla5PmF4fIaczlso31-db75e4zQ3O8rnzcIKdfSu8m4pP0eZTaqMP--XgwFabiuFhMKrrmcVwIzlQw6TWSYvBtMS2xiQB-qo-VqHEndeRm_r_xPBIuZEjnY6sezWeN2kN5KzzpalcE2Texwo7aYZL6-w_4GRQ9rW-1U4yFpahZ1kyznVR1Js1W857F-gzQ4kNHyFr38NWkcDqKs3GCaXVaZ_0FdATOmBneHLpdseKehmwgJZACFkmqqIIu1LZ69KmLUWjZ4oTS3ClKTNpkPDp_5v27mPzLSWp43476pJkTklA9ChMjd2omq_ij-" style="width: 35%;">
                        <p class="caption">합성곱을 이용한 학습 (input: 파랑, output: 초록),<br>출처: vdumoulin github (https://github.com/vdumoulin/conv_arithmetic)</p>
                    </div>
                    <p>
                        <br>위 그림을 보면 4*4 크기의 데이터에 3*3의 영역이 가로 세로로 한 칸씩 움직여서 2*2의 크기를 가진 결과를 내어주는 것을 확인할 수 있습니다.
                        그리고 위 그림을 보면서 우리는 한 가지 사항을 예측해볼 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">바로 기존의 한 줄로 연결한 데이터가 아닌 batch를 제외하고 가로, 세로의 2차원 데이터에 대해 특정 영역 범위(위에서는 3*3 범위)에 있는 값들로 계산한다는 것입니다.</span>
                        <br><br>그리고 왜 CNN이 2차원의 데이터 계산을 위해 만들어졌는지 아래 그림을 보면 이해가 됩니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">우리는 2차원의 이미지 데이터를 1차원으로 변형시켰을 때 그 특징을 바로 알아차리기 쉽지 않습니다.
                        이는 컴퓨터도 마찬가지이며, 이러한 문제점을 해결하고 2차원 이미지를 있는 그대로 보아 특징을 파악하기 위해 만들어진 것입니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweVfBgnRzSg2BU57WkVwnVlBBsrRKF1vkITszs4HSBd0M9pg4fdBLl2P3EwkxQXZYll49Res4VtIDtTTOboYbK3xybXTJwd0S7iYibYgeLdQfjOlDooxo44q6n2DvdgPs_TQgJURA5TAABxmWMk7PfaSHURRGl1foELzqtdCVLsdsJpiL4fX6uMJZv5_oSuPPs8Wai5FxT3YHW0uS1WR4jrcIWAbfEQEdAyrTdDGs_VHmd4Np4WP9qI0UY8UhP0qJt00-HyLYNm2TtP2uANL4Yw39608uxA9Rc61SR1pxzIhqZ0j05cW1OHO3ufK1POsr7s69XGDtkc9Np-04oqDVJKAt52Ctkxw_-4tBCHho6tdclxQyaFTrPyVwm9G3Xx-AWAQxKxd9PlmruZOdTmVY05bMQDlOx7FIjoETDkTmlunHbQH5-AktxHs7p1DAv-rhypq0GYb5jo8fCiD5WONYvnhgsLu2bxWJISFIv_NhrDXthZZHy2gUgcm2Kflt6Of_-WPhNSTkjTlVFAZgjcDqWYSh0uX8m0M0vEYGWJ8XwYnRY9oiLx7npyqZF-3stssn169OWz7uhUxB7y8t-_ZD6B4Ti87OHfsZW_oYmbLrHNTuylZ7EU4m2w2FK8w2cVW10j3vmizsEktiFFoRFCkZ2d-gcfqCG6fCYXMkpQmdiyxlz7GSafpgNTs6QDSP65w-BIWbL00rJNiLkmcmhywjWulezWju57Yr-oLM50xiHtNCJMZJjs0yG2M84tWC4TlTMx-AI2yjHAijkAI2V4_Q3aeWDI5KwTaxFod0J3JUU9PjwpnixvyjeD6_kfkHd9JqA3Ehb_F7H8MqFFRPTJKj3Y6Q2l5HPrD9tyX6yV1xZmQpwcWLNMg1JYZyRVqg9oGngWckAMqdticvugPT9JWDz8nuvSTYxtafV194VTwb5FPNtjG_j1kA-lJiS4yDqRe6rvy0e9JZggIFOYZekPckfPYbOUJ5p5m7A1trPFiL9ZP_BiG-rA3jKZ91wJB1Ez8KLPzg0P558bCOhi_WBFdXtmVO9okWCkxDCzgQXY0DOfUQHPYAUapTzyC3hDRtvQgEvDlJzTUCiyEbW7v3AEgM-yxPdG6SR38F02Z4kx_Xy3MBYKIdc5BcdA8Q8GgEBmARf_9NYMtTdnWXOvqgZ_G8r8bsksIVHqon9VOikcsgZnmC-oaRn97vDJvzodnsHUL_rJpaODyx7XNeYBgajzHsX1lan3v_DoYf7EwJ-YXBoHbymbXoFJ8dvBgp5_8hKsr4md7" style="width: 100%;">
                        <p class="caption">1D, 2D 이미지 예시</p>
                    </div>
                    <p>
                        <br>이로써 우리는 CNN이 왜 가로, 세로의 크기를 가지는 2차원 데이터인 이미지에 많이 적용되는지 알 수 있습니다.
                        바로 2차원 데이터에 CNN을 사용하여 특정 영역에, 즉 국소 영역에 대해 연산을 진행하기 때문에 이미지의 특징을 잘 파악할 수 있는 것입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림의 사람 이미지를 예로 들어 눈 주위의 영역에 대해 합성곱을 계산하고, 코 주위의 영역에 대해 합성곱을 계산하는 식의 방법으로 데이터를 바라보기 때문에 이미지에 많이 사용되고 효과적인 것입니다.</span>
                        즉 3*3의 영역이 왼쪽 위 부터 시작해서 한 칸씩 움직이면서 오른쪽으로 한 줄 계산 후, 한 줄로 아래로 내려와서 그 줄에 대해 또 계산하고, 또 한 줄 아래로 내려오는 방식으로 오른쪽 아래 까지 계산하게 되는 것이죠.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweXQgZVQbpq_Lcc9Ucj7VyoE8Cosh7lXYtQMhVcvPFVYguXAMEUJrqetSXmzo_OXZg1IsLT3J9xy-sC7ZSdzZyEdbtrvjpwNOg5jhlcE83bKP89Pu3a90AU_TdC58Usy2Rdr7gB8WN53bcOf03jbV0Ou-6cLoPYadU2HBc5FNmp7KRgzKFYhovXCzKZJBsiNoXZZrMgT-Ogt53-QBWUJexsDxd361kf2d650x3colntFhoqZWubXlF30-EwE_p0wZrXkAb-XR8TwtR7G_E-3H-b1ESBvmIC4g0w8L-LkjT31rNvkOtyxUGBp3oTAStGdkBOfQGbxo7OMhh0Uc3HTrVn1IgG7EvKyBus1uOofMOYGJeiq2aMn4CyALELQb3jgnHO07oMakPeki7QW7aahkLncj1ShEY_Ok2e5OVekrUzNzBsqr8tMHKpTB3QJ9aUUM23pVGOzA437XN2Tngn5FuXLL1d4JXFdyLPNur3X3oZAanI0rk6LF2MIFt9xzTPiRATY-nse95H7ZlZGxRP9RJfmJi1E1uNMB0H7gaZkcylQ30tNFd6P1OdE8j-EOBgS1MuTsGOGxVlcBdOa4IAKtHtExpJc-f5IbBxPxpfl_VOk7IE3jo9aqGZB59t6SS-fRLdicmiSrjxeMcOvSnxWceioITt2y9NyJAhMFCP8tNfFFYPEXKynAbmdKddzhg56cvBuA1IWYoP7DoOJCSL1lSgQoItwFKzMQONtkAQHt10CO6XxxIb5guLRY7WyUDqV1hxNZImewtH21Y6drQcKVdvLn7TGTiRltbAOO4NClJzBKhIZ_IB4HQgGd69OG9Y8rYT-NkOeahp35We3D7RpjRMJBpV8FpKZ6__xYzoGb0ukp5WKz-FaAJNkhrEcaeuT4z8AooI8T5mVMPXWyZAmwqq95rU97YTtDlNhQ1k3F0AQlBQD9oZllaN6gCGoW38YOX2iNpcg2yhoNpw3ah7uJ-xnH8J4zu-aGEd2AuCrQEcP5kC1tdKIquu_QeRmY-GCzoNuPXXW1pbC8Z5RYX0yCj3M_4mfPAi2TtVk8iH9yWUdTv-Vhlxl-39fRXm1VCIRhJzKEVgLeXl4HqicX3psdbEAYAFV_KkyBw5rYECg2uOljot-bBHxGOfDzLGdWNCS1zOhERcNVgwwkTc0Fu4DltERNBHVk87h7KvM3JNWB4PHEVqjaIwSTMLDXJUMqXSjzmU7QHKxPhhbfa5AAMjlXgt4llLnjNqHGsjlQkjaEc66UziRkjG8PKGssCLHfMHXVcGE" style="width: 45%;">
                        <p class="caption">합성곱을 이용한 학습 예시</p>
                    </div>
                    <p>
                        <br>그럼 CNN에 대해 아래와 같은 의문이 생깁니다.
                        <ol>
                            <li>합성곱(convolution)은 어떻게 계산하는가?</li>
                            <li>CNN을 계산하기 위해 필요한 것은 무엇인가?</li>
                            <li>CNN의 파라미터 수와 특정 데이터에 CNN을 수행한 결과의 크기는 어떻게 바뀌는가? (공식)</li>
                            <li>CNN 계산이 가능한 데이터는 가로, 세로의 크기만 가진 이미지 데이터 뿐인가?</li>
                        </ol>
                        위 의문들을 하나씩 살펴보겠습니다.

                        <br><br><br><span style="font-size: 20px;"><b>합성곱(convolution)은 어떻게 계산하는가?</b></span>
                        <br>합성곱의 과정은 덧셈과 곱셈으로 이루어진 아주 간단한 연산이기 때문에 아래 그림을 보면 바로 이해가 됩니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">커널 혹은 필터라고 하는 것과 데이터간의 곱한 값들을 더한 값으로 결과를 내어주는 과정이 바로 합성곱 과정입니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweXwxabhrCRrVaMdlGecvjSPGeZGtfKxtk42ipZhM3kmKUAMVeFZHT-ECZIOvbQFxYawdGdBxYaMp9sibHIlcrfHRIxtOCyCTlyvX-2aScE6ZkdNvnisAzjKAMbFqEblGJFscsUL-bRFAQnIL6JovMKkHS9eLFRRCSoZOlr6EgHjeVe-VUDVaN4UMMCSpMDPdiVAtbjq9WOhm4Gas7uLXAh8jf6SLHKg9QBe8wb2OzyGyYLfWMVLRaouSHZOAipsxEuuyAYhFv1XBjkCzkoQ7Q7270wphFOIK-ET0YNLvnZA-bWGeXfntBcpyXr7z6xUcy7gOFChaKnc2r_7VL4U4tCLU_Fi83TJvHr6MuL_93hchPeNBI_VHR-R1_JbXBI357pprQGmYoHwg9D0KVXC1-g4Igba7GIud2nGyCmtEcUs-WQXcGbEWVF0OvA8xX83rDAzNneG7jGfjT_EXUIN6NQEk7K0Idnl4GG-zUeHumjZMtTi3zUDaqIx1uNz2Iqkg0Zhs0udABoDnIiqGKlsNnTGcBioo3de08l9-bVoOiSJ5i3leHRbPreljEllGODI05aE7ro7F2NFkUj7nBPjdgODyrSM474QlIG-7VVU2lXU7KX1SvKePc-1xxq_QyxfxOumMK3PPTqOp4qIPTE2qPvzJTKksU_HHzDAOVTNIwf0aWSrfTMEqlAo_DEPVhQa0fX1dZWn-hlkmddg5lrZjPQbLobUWzY1eXpg_WFWhbsWW1yeMMn7qGz9ca-_APK-W1d5Q3KojliKPLIZXWyma6mm6jSDiRkb_aLCYo2LhiTrNCQNx_EzpPfNW3waIFWKj3TP4YyYqqw50HOSIXqVG6Je-Q9j1QrKgZKgD6rVlLu5tdXKifA64M1NhseuKWxYa4HSLFmJoVeEHSZpt4tZYOF6lSZb0552NmcUl5jrBVDhBshyP8yWTfn7IOsK_ywCuYIkKbHrA0wIfCZf14_gof-qWX4N02FIBWUSis647Ovoig8mhn4OsuKjt2tHjBf182b06d_chfWvk3kJLo3XGQK0PhgrMTuJ3gxXqyQEfcQFXgsiM_wrXxOjw037a2xGo1a2mgRY3qmDiRQrD6SOLLoXe9xbhwoDBcEJsrcPIFlaYAX0KWzI9OzhKPt4FFRXFHW3KQjxZWqISLx0Z20No6yzHayhAR-eKvMY3xPM-yYzo2zl6uoGQ7NLBu1PORZajq1reUAmwJImoKqIi__9LPQ1xQqHsW82LBHTfKeD_j6b28E_9lFfh0lSEW2apSChp62D" style="width: 100%;">
                        <p class="caption">합성곱 연산 예시</p>
                    </div>
                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>CNN을 계산하기 위해 필요한 것은 무엇인가?</b></span>
                        <br>이제 합성곱 연산의 원리를 알았으니 본격적으로 CNN을 학습하기 위해 필요한 것과, 파라미터로써 사용되는 것 그리고 파라미터 수는 어떻게 계산하는지 알아보겠습니다.
                        <ul>
                            <li><b>데이터</b></li>
                            먼저 CNN 연산을 할 데이터가 필요하겠죠.
                            위에서 이미지 데이터를 가로, 세로로 이루어진 2차원 데이터라고 하였는데 사실은 총 3차원입니다.
                            <span class="highlight" style="color: rgb(0, 3, 206);">그 이유는 가로, 세로 그리고 이미지를 이루고 있는 RGB 칼라 데이터인 채널(channel, 혹은 깊이)이 존재하기 때문입니다.
                            따라서 최종적으로 batch 까지 포함하면 총 4차원의 크기를 가진 데이터가 CNN 연산이 가능한 것이지요.
                            즉 데이터는 (batch size * channel * height * width) 이렇게 이루어져있으며, 칼라 이미지 같은 경우에 channel은 RGB 값이 있으므로 3, 흑백인 경우에는 1로 설정 됩니다.</span><br><br>
                            <li><b>커널(필터), Kernel(Filter)</b></li>
                            위의 그림에서 보면, 커널에 있는 값을 기준으로 데이터의 합성곱 연산을 수행합니다.
                            이 커널은 기본적으로 정사각형 모양을 가지며, 사용자가 커널의 가로와 세로의 크기를 정할 수 있습니다.
                            위의 그림에서는 3*3 크기의 커널을 가지며, 사용자가 마음대로 3*5 이런식으로 커널의 크기를 정할 수 있는 것이지요.
                            <span class="highlight" style="color: rgb(0, 3, 206);">그리고 CNN이 학습하면서 업데이트 되는 파라미터가 바로 이 커널에 해당하는 값들이 바뀌게 되는 것입니다.
                            당연히 학습을 하며서 데이터의 값을 바꿀 수 없으니, 학습이 진행되기 위해서 이 커널의 값이 바뀌게 되는 것입니다.
                            그리고 커널은 학습 데이터와 같은 깊이의 channel을 가져야합니다. 즉 커널의 차원은 (channel * kernel height * kerel width)가 되는 것이지요.</span>
                            커널의 가로, 세로의 크기를 4*5라고 정해줬다고 가정하면, 커널은 칼라 이미지라면 3*4*5, 흑백 이미지라면 1*4*5의 크기를 가집니다.
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweUx69q4EGfDbVbn5wFV_PEWYKt9ys6iR5o88VDZzIBkl_Rfa92yWgfJjg87kiutycf5yx57SmrcB8qBkolvqzhPXBdukFHIlCwTJs5MTV4eprum-64xLhCX04Eu-1YK9geCos9PP6z1is2S5XnU1WK0PnWW2puV6d7c8Efoxj3hjhO_A5pQ4WbRIQwAPZJQBMS3_1NzvSpNaeC2Xg_YlIXTW3ndVZEstIbUn5Su4HhM3vByWcm6npz59OIYF4-R-3YElmwEb4SJwBeVInZvvrAkdz3Sme4OH8Sc-Bu2a0gNm2VzKChEuTVYvQt5a1VfyPgTNkYwPwL9D_OgDx_6xH2RIlKgxXpY0b1EHPfch4L8JlegTxA0MyZIstQtp9q30DHWgtDCj-gtGLV1xwdbg7uMnZhv2o3IOqvaDgBjd6Z8eucKc0h9mSZmAPf44fXtkn_pULmo7yDiR8_-oDIsGExlneRATXDZRnAKccvz2nkNqztXGtUqtTDg2dBJTCmODuYUgposgrkEWBFFZKRr7axFALzjRkb1kDbz1xoDMe9_ZXS42Jp5zbEbJ6_Zq2m8VN3gQXkpMoFCawJXsRG5DqnPdVC5a2i1SHMkIas5uPnpFo1zWj2aDOaUqiVUb_rf9J1ZWzNtGN5mHPZEgt3cwYvkDUTxaQ6ImIbqZFVcnwYJZiUnUmujC9lVZ8-GlXCJT_aiAnBouuObdaIBDpMmR_a64Nz0qOrlIQMHl697CJCOulV6qqmEGr5doDu9A_DCAwI0CoHxYDrWBrugabW3JZz46nlbwqUXtsFrI0TjPhZ50y07Bj6JXW7-XzEN-2gm0oOQONM4Bzhd9kbQLF0p5KgumlxvD4HxpZpSA56kZdHs1df4UBQcBIoGp2d1v_bibCevV2ky48TE-4PKh8kHZGYj2lYivy1pBneh_ZtaEu7LVS1y_o6FIHj516zDeCLsbgxvbrQ0y1KIbV9w7JhyIvvLn23VD_50d2iOu9Y9xYv0BHcMGdJ-8FU4Iza7u9U3sxzFtW2eiVrQjZmdrvLp4noU0y2hvItFN6ep1PCsS5vJrHVkiociHQ89uLb8uUrnuJFuYFW3y34YKRXU3Puwp4Z4zC9b2vgfl3bDrDD9xAMN12rV2u3tOUmdzbeRsgXkBR3AjaCGhmIN2oLZMejb1TmEeq3iz8H9c6L0AVlLlWpXPvyUAoTGPHOISmyvAbT72CQSLgM7yoMNxoFk8hDeK5-VK_5GRvRX6Oxs_Hmm2GRrM8esELEep7J_HmBnxih550TQ" style="width: 70%;">
                        <p class="caption">데이터 칼라 유무에 따른 커널의 채널 차원 변화</p>
                    </div>
                    <p>
                        <br><ul>
                            <li><b>스트라이드(Stride)</b></li>
                            <span class="highlight" style="color: rgb(0, 3, 206);">스트라이드는 커널이 데이터 위에서 움직이는 간격을 의미합니다.</span>
                            위의 움직이는 이미지와 합성곱 방법에 대해 설명할 때 사용했던 이미지에서는 가로, 세로로 한 칸 씩 커널이 움직이는 것을 볼 수 있습니다.
                            이 경우에는 스트라이드가 1이라고 볼 수 있습니다.
                            만약에 스트라이드가 2라면 가로, 세로로 2칸씩 옮겨가면서 계산을 하게 될 것입니다.
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweWOfcOc8ire0Xisgsra2joW_iecwiqBX2MwNw1lz5HLgCErH-NJVysIE4gezFl9lijAXCOAnbsBVRjXftFOxTlSgkRV78yNHEg5XQXKSbv9zBjEhUMWiBzajUz2l-MTDF4Xe1LZ3JwQTzRVklwA1-kxWn7UQG669cWks1K06-Vkc-MzloJQ6UG_JOErHNu8PV6FdoqcxxwXwiWiwrFIYWOgtXj60utOlgoFkPXHWymIKCZdmUHmeo-xiWFG9O4r9XmxBMxLiXEOCYPT0Kn_5iC950SMyI4HAKjZDzUq7Pw2isbm8xZTrqN-lfNr6P_pvt0d5TkM40i1_2k-GyEClxun3SXUooLO4GaPSyUmhs1ixc3EdTibVGdth0sWRo24qsmeFEDB0MaNFiJnihn5uj-MYKCEE-ws4V6gTqB63NAH42DQkvHyGpBVRclTWF-Dv-6kJLlTRwqpw9MC9A_TZ6NLxNg1PYOnEUMm-pGKsfXLs6hkUayBBE0te7buxazBHyXnCEfhlyG77aSxkof5NMOMg7I9CPOx09LgOGa06dBq-OYD_2I8Xd_7kULMAVqXc7XU9LLmFl_Y6gUdQsAggRM4Ys6S4egE4OlVCv7nZAnfwpr8ssrTXhSJ2yHnLwdRR62QmUEp2RgINXrdWe1sO_u-8ZF60hL18FPIq0YMZ1GxvxQinh00ewN7tptBXrnMrcRIbVQaIoDOcQyPeJcKRmPgyFL3_4D8R9ztofMjsW58-YcRFjZYEQFKYma3AmEKTVavF13pmIA3TfIXf8FfVVQFfjSvGRuS-7TDM2BBbCmhTX_uyf599DLCOoW5YTwKrJDD3FzET5AFyDvwdAVJGN4fnXls7KuQY7bVhoJkNWbsQBoCc8K9HcWx1kQYnDRGqX8ulwgYA2UD67-JhMuGpWILj-cUxCeksV-6LW1HQGEv6RdG8_s9ZO49gj10Yemri59eDIIjRzla5PmF4fIaczlso31-db75e4zQ3O8rnzcIKdfSu8m4pP0eZTaqMP--XgwFabiuFhMKrrmcVwIzlQw6TWSYvBtMS2xiQB-qo-VqHEndeRm_r_xPBIuZEjnY6sezWeN2kN5KzzpalcE2Texwo7aYZL6-w_4GRQ9rW-1U4yFpahZ1kyznVR1Js1W857F-gzQ4kNHyFr38NWkcDqKs3GCaXVaZ_0FdATOmBneHLpdseKehmwgJZACFkmqqIIu1LZ69KmLUWjZ4oTS3ClKTNpkPDp_5v27mPzLSWp43476pJkTklA9ChMjd2omq_ij-" style="width: 35%; display: inline-block;">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweXRHmYajnILafHjs3dLz5C0UCkhtX0XHhAMCD3JEI4MaPo2D9B6GRbQywtbH4ZdSOx_fI4OwtuvHOsKNkobc3tkN2Wz_UtEF10ojRhV4Zanq2NW0G92fvYn7KLkU8ozwQe4nXblYuuUQP7o7bictafu2NdLJCdt7BeMezUGQlQMlf03mFJ7cE9luqsABjTLk_wMgu47_I6Hpw3F7UJds4HAvYGFcxsgg7CMEuewgskTTdZstWZUEK4hQ7BvxIbSK6UECBfo3g1bcgipSoKk1IO9Al-B_skH_QA3eJ5ywjaTEL-ekMGv02hZAEamHVAP-lUtJtiaNJbg6gkxyj-7onihPHv2aMwme2hdT5wQk8yYpnWDEcC8ie2nOwaOtgczSTovo4ZOv3Sk-nZCVkUhHpg5JyFfda8403fQUK9wKMUt3KUHwB1ygK14i7f662nsWWXb9dtBnVEmxCDPmOWgJw-uTT5f8sfMcQr8OnpwcND8ODmOlT9qAPvHVQiU6qn8R_IqpuruI5No9KCbEGH6fxIcBkQqJWUS2bL2cxi2GXYRXdd9VlLmcPrypqAFtnA7tN86mT0m_qEvTGb7zmryqjFEjeHWyGmuA0Ift1YLz5B-r7wcr9jYbTsB89CBQoZvzF6YziHadMuScbNTZEFy7Oic_gnFVXddkGjCUfNNK6orlkdOldH57UakUxu2FfmWMOW_swceQ-DUGTCP1Fx6-Z-28YPgBIdW1P5RE2mn_JEWEdXG7JkQVUJi2OMFswV84TH7MFHbqWK6QkKagChYpSZn7c7i8dPxodhp0VKroLo1X4mGGwMVwFvVHdf4F7u7flVDPgEgdjk1AS3BnXLOQGFbQy3NOehHg_6MZyX1yvYsl6Wk75cOfOSzXpNEpVSm6D7DKZNjsW440xeSJ644jEeA9KUinrkV4fF3u45dN49TpJ_e4NYkueZfX8sikQjomy2yUz_9ahHTWmBh3u_MXzDyIpdcrw3WvcZsEUTU7gnId7HCBCEFe1dLTpqj_e33eOLS2WBP2F5x2yvzM8q6RKQ71J2ZuuJDI85wV7m53uoK4JZ_quHQeCBiaFT6p7hqMIXB97mBw6M7BetURT6-PHzWeKOMOzU2DtZhgbpPqhSwsFd2Hij6QvKUYKo868xgnIbvKh2-hgDdO53YzHznFhBjHtdtjjpTmCDpY2gMGrqBaUukTGj0KMGrDFrA6-Uh69e5iB0PryS2RJ8Wig8smW6rakwyaAs2uKhSWAg-KVRuHiN7u_fp4Db5--I-z6jx09Oc" style="width: 35%; display: inline-block;">
                        <p class="caption">좌: stride 1, 우: stride 2 (input: 파랑, output: 초록),<br>출처: vdumoulin github (https://github.com/vdumoulin/conv_arithmetic)</p>
                    </div>
                    <p>
                        <br><ul>
                            <li><b>패딩 (Padding)</b></li>
                            이 때까지 살펴봤던 내용을 보면, 합성곱 연산을 수행하면 당연히 input 데이터에 비해 output 데이터의 가로, 세로 크기는 줄어들 수밖에 없을 것입니다.
                            따라서 몇 번 안되는 합성곱 연산을 거쳤더니 결과가 더이상 합성곱 연산을 못할 정도로 작아지게 되면 깊은 CNN을 구성하기가 힘들 것입니다.
                            이를 방지하기 위해 패딩이라는 것이 등장합니다.
                            <span class="highlight" style="color: rgb(0, 3, 206);">즉 input 데이터에 대해 합성곱을 진행했을 때, 크기가 줄어드는 것을 방지하거나 덜 줄어들게끔 해주는 것이 바로 패딩이죠.
                            패딩을 함으로써 데이터에 변형을 주면 안되므로 보통 패딩 값을 0으로 설정합니다.</span>
                            혹은 패딩 값을 바로 옆 값과 복제를 하기도 합니다.

                            <br><br>아래 그림을 예시로 들자면, 패딩이 없는 상태의 합성곱의 결과 크기보다 패딩을 씌운 후 합성곱의 결과 크기가 더 큰 것을 확인할 수 있습니다.
                            <span class="highlight" style="color: rgb(0, 3, 206);">만약 패딩을 더 두껍게 씌운다면, input 크기와 동일한 output 크기가 나올 수도 있고, 혹은 더 큰 크기의 output의 결과가 나올 수도 있을 것입니다.</span>
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweXRHmYajnILafHjs3dLz5C0UCkhtX0XHhAMCD3JEI4MaPo2D9B6GRbQywtbH4ZdSOx_fI4OwtuvHOsKNkobc3tkN2Wz_UtEF10ojRhV4Zanq2NW0G92fvYn7KLkU8ozwQe4nXblYuuUQP7o7bictafu2NdLJCdt7BeMezUGQlQMlf03mFJ7cE9luqsABjTLk_wMgu47_I6Hpw3F7UJds4HAvYGFcxsgg7CMEuewgskTTdZstWZUEK4hQ7BvxIbSK6UECBfo3g1bcgipSoKk1IO9Al-B_skH_QA3eJ5ywjaTEL-ekMGv02hZAEamHVAP-lUtJtiaNJbg6gkxyj-7onihPHv2aMwme2hdT5wQk8yYpnWDEcC8ie2nOwaOtgczSTovo4ZOv3Sk-nZCVkUhHpg5JyFfda8403fQUK9wKMUt3KUHwB1ygK14i7f662nsWWXb9dtBnVEmxCDPmOWgJw-uTT5f8sfMcQr8OnpwcND8ODmOlT9qAPvHVQiU6qn8R_IqpuruI5No9KCbEGH6fxIcBkQqJWUS2bL2cxi2GXYRXdd9VlLmcPrypqAFtnA7tN86mT0m_qEvTGb7zmryqjFEjeHWyGmuA0Ift1YLz5B-r7wcr9jYbTsB89CBQoZvzF6YziHadMuScbNTZEFy7Oic_gnFVXddkGjCUfNNK6orlkdOldH57UakUxu2FfmWMOW_swceQ-DUGTCP1Fx6-Z-28YPgBIdW1P5RE2mn_JEWEdXG7JkQVUJi2OMFswV84TH7MFHbqWK6QkKagChYpSZn7c7i8dPxodhp0VKroLo1X4mGGwMVwFvVHdf4F7u7flVDPgEgdjk1AS3BnXLOQGFbQy3NOehHg_6MZyX1yvYsl6Wk75cOfOSzXpNEpVSm6D7DKZNjsW440xeSJ644jEeA9KUinrkV4fF3u45dN49TpJ_e4NYkueZfX8sikQjomy2yUz_9ahHTWmBh3u_MXzDyIpdcrw3WvcZsEUTU7gnId7HCBCEFe1dLTpqj_e33eOLS2WBP2F5x2yvzM8q6RKQ71J2ZuuJDI85wV7m53uoK4JZ_quHQeCBiaFT6p7hqMIXB97mBw6M7BetURT6-PHzWeKOMOzU2DtZhgbpPqhSwsFd2Hij6QvKUYKo868xgnIbvKh2-hgDdO53YzHznFhBjHtdtjjpTmCDpY2gMGrqBaUukTGj0KMGrDFrA6-Uh69e5iB0PryS2RJ8Wig8smW6rakwyaAs2uKhSWAg-KVRuHiN7u_fp4Db5--I-z6jx09Oc" style="width: 35%; display: inline-block;">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweUdzmJhG4XrNh3t7gfLrQHVA9Czh6IXY-OHrDkM3RBLgS3Z7QgjA-hJ6KVeH3paffCC6SJmGh7twBylQQkym3gKtv616dOL9ns__lGwYrB8fN09MgYloL80D2GcJFmw5ueoT5BPPTiiYRK4bWE7KABYZhqmwT3MNZVFafg1wXUInQgvqolQTVXb7J8SolZt479RhNeyrOUStzD6TuvWZpufiNPxbTZO4yosgjyASa1_54LaoTWv6yaJtNNjHre3AseQYGHY0N-FJxJAjUW259D8jFHKxgwykxp3TiUT8FAXZxmJWdbndDgfBme-TUUdbcqQCjZtTtpDfJb1LnfKqiwwl4g29gyiFTBwIapZDKMg4r4riKExIxIrJKD_952tjCHI3HxOz5p3-fuJqNr5StrSm1QGBnIDcu3xUFf0gb0Nl3WRUgrihFO05gGApMCmXJpwB2F-hVmXChd-a97P4a_3ZYk6i9bIcdQtmsX4_rUefNyATTE_4T5c7QFN5OkMEYa3yCEC56hGU8qEOvhTat_9_vtd1hCyPK_tovK0LXngddTJhhHCmY08ubEFpV3hb0Ep_SscLmfqApQTaCjF-5P6O0dAgcppTjwZDotb3c0BSDXBNkXDrRbe_GvtjlP1hmZcL_MzatOGHMXoyRVzWNdiRAGHFNIjA8qwdBOIlBBVNrBATgBWP1xikUXGf0RdvaGzMiZql7qn5ESfJD9DxD87tNo_LTEQcPMuyOQzi7sBCGdL4U8_OjEZ71MEocexoUF1UczArAeujkzo6zidnw2D9NyuYTpxFQw3YO_nV7te66RokI20LSuw_zNZ855pQ5DmRnRBvCNWJEU3Y78cKVDyoZ6hFXFCXcD_HLueIxGrAYce1vVOJa_dqYBNYgXXZzqH_qxY6vIZWq23MYn4FZ1HcX9jBqeytatcpiY0GsuTCkePhQFnD_zK_u8YGl0vsCSeyeeG1b2OuoV0yawxdIwZWXmZPwinD1CX3rqv-HnrPtjJ-4CDHodU7qlfar7BbTw9w7r7fGoG_4l2eEhqGm17BWbgyQ8441TSzlWQu90XpQZuTPLzePALVky-LeJZwf-f2HeSHstIAItXGydtzxyOzj1ymSAbzf6VUuhujldwcf-ksI3LowanxGpE6ixbdiydJFcnUZoLw0IMlW5MLrdsOW9DPvB0pp9agugxo9UPLUKHU_bLz6GDaLYQuMzSric4YAAR5MIDbD3hoSAjwEbnPSBXAseXSYPFKCqpAdRGGjLK50qI-uvetzeEdd3E2byx" style="width: 35%; display: inline-block;">
                        <p class="caption">패딩 유무에 따른 output 크기 차이 (input: 파랑, output: 초록),<br>출처: vdumoulin github (https://github.com/vdumoulin/conv_arithmetic)</p>
                    </div>
                    <p>
                        <br><ul>
                            <li><b>풀링 (Pooling)</b></li>
                            만약 input 데이터의 크기가 너무 커서 아무리 CNN 레이어를 통과시켜도 여전히 output 데이터가 큰 경우가 있을 것입니다.
                            이런 경우 보통 커널이 움직이는 stride를 크게하여 output 데이터를 크게 줄이는 방법이 있을 수 있지만, 이러한 해결 방법은 세세하게 이미지를 보지 않는 문제점이 존재합니다.
                            <span class="highlight" style="color: rgb(0, 3, 206);">이렇게 데이터의 크기를 줄이고 싶을 때, 모델의 사이즈를 좀 작게 만들고 싶을 때 사용하는 것이 바로 풀링입니다.</span>
                            
                            <br><br>아래 그림의 풀링은 2의 kernel크기를 가진 2 stride의 풀링의 예시입니다.
                            아래 그림을 보면 바로 이해할 수 있듯이 데이터의 크기를 줄이는 역할을 하는 것입니다.
                            <span class="highlight" style="color: rgb(0, 3, 206);">그리고 당연히 어떤 범위에 대해서 얼마나 움직일건가에 대한 kernel 크기와 stride를 정해주어야합니다.</span>
                            그리고 pooling은 max pooling과 average pooling이 많이 사용되는데, 전자는 kernel 범위에 있는 최대값을, 후자는 평균값으로 데이터를 퉁쳐버리는 것입니다.
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweVy-itmwmf3jIHyFNVtVEwAVfyV6VFSU56wYyvdejebGOIMkjrgeJ6UcEBwdEQUxb2HCwEOeL64-mIPSkbdq69TqZ04wGmYjaK_uIaQuhnNehhnY2L-BA24-zK2FVHG4vZCN5Fr55Eg2yeMs14L5cW_Y8fpzEgjBL5FKVZmgyEu_UsVS9npM3SA2xRiQlq-hqjS9OyGNkgru5tTYp6aBkr5ePuKjKarGk4L8KOT0tYUKwNES_PFBtBV5eQSUdWvq-kfwBvckss6jAA37OzBNkoz72qZSEodue9sIYAyYC0t7BfZ7OPu1nuAoWZsbrNcZB3PpSC_ukPAbZZPNMHqXkRgclZDZxMm96SFD4d7JLz05TW6nE1Snw1Uqu666IIYQPsyMcoHBtn9_n8u-dOlSZ8ZM_Pxfei0pchbw3YRjNWLsBJzaxUshyQYan7CefPNSxsPgg4rpdBBZpTATrAVVdqyfjlVyqlUJoi_me4OSnxZ7fn1q1tBc2F73k-h06cG1w0bH6R5E4gfbfz_QT6ijEQ_Hq8v8XYKwt9Ma9BQLHcX_uMPa9HC-71OsRcX0nXzGBErYtR3-5NiqmMGbK1gekLW_eScI6q0Y7Jjq26-HjJRXuFqrq3N2qPN74o54J4I8nQLOUudeEMYSt5s0K3bEkoiwlxk7FI0I4e8uLuUHWNlhaBbGchYChqxWvi8XMbavzIYeBxwAzKurH6Ddrfa89D4cQ_DPRJ2BrN5X9Fl8FOjGbSq29AyuxXOPX16Q-zBhX8ND38U1oYORALGHY8YTrYL8QjyINGy6UMzWGUL8fX91MM67uY8UvRPUkZFzB8tNQatti-6wKUZsQWqBoQtCyauoHYVSQlqMP6rqZWu-O7udDUV9Qt4B-MQ6gYGvFoHL2rUMyCrlaKGQbNG6PvlZMmO6W2X5RFjaaxUJpqieYR7l_pi_xGqBTQjvJM1okH0hu9D92lEbrq_dzOgVhF_-nSzzmoQZF-ZKhfe7_eM3RSyklLQhPe-eDLiSc9VNcgrV6n0pugRd_dwPw3Xv6I3028AmC2PeltW9ZmvGuAy2HJ2XQ0Mc67t3HSwyiPYuwz_g3lmHfmUp1N4NE0Ao4SiQ7bVxd75gccOs2of3PeZ_l-dzmHwZXbtQbP8HkbHLBsdJQLBXrvKYYKDi8nek_Gqmx0C26uVAArvuKMWaCHDgCtPGZdfA_8Gja36vTNz_6_bfWE9fRhI_E3RGUkeIGUOz6XVEFweM6jvxnhu07dQbjIgTg_YuoE2ZoPCUK8vw1greDh_" style="width: 50%;">
                        <p class="caption">Max pooling과 average pooling</p>
                    </div>
                    <p>
                        <br>그럼 풀링을 함으로써 얻는 이득은 무엇이 있을까요?
                        <ol>
                            <li>데이터의 크기가 줄어들고, 모델이 가벼워짐으로써 메모리 등 source 절약이 된다.</li>
                            <li>풀링을 함으로써 데이터의 크기 및 모델의 파라미터의 수가 줄어어들게 되고, overfitting (과적합)을 방지한다.</li>
                        </ol>
                        첫 번째에 대해서는 위에서 설명했던 부분입니다. 그럼 두 번째 내용은 무엇을 뜻할까요?
                        <span class="highlight" style="color: rgb(0, 3, 206);">풀링을 하면 데이터의 결과를 줄여줌으로써 데이터의 모든 데이터를 사용하지 않습니다. 또한 학습하는 모델의 파라미터도 데이터의 차원이 줄어드니 개수가 작아질 수밖에 없습니다.
                        따라서 모델은 통해 적은 파라미터와 적은 차원의 데이터를 통해 의미있는 학습을 하게 될것이며, overfitting이 방지 되는 것입니다.</span>
                    </p>
                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>CNN의 파라미터 수와 특정 데이터에 CNN을 수행한 결과의 크기는 어떻게 바뀌는가? (공식)</b></span>
                        <br>위에서 첫 번째와 두 번째 의문을 살펴보았습니다. 그럼 이제 세 번째 의문인 CNN의 파라미터 수와 input, output의 결과 크기를 어떻게 계산할 수 있는지 보겠습니다.
                        <ul>
                            <li><b>CNN 파라미터 개수 계산</b></li>
                            <span class="highlight" style="color: rgb(0, 3, 206);">위에서 CNN의 파라미터는 바로 커널이라고 했습니다. 그럼 우리는 파라미터의 개수를 계산하기 위해서 커널만 보면 되는 것입니다.</span>
                            그럼 아래의 경우 파라미터 개수는 이렇게 계산됩니다.
                            <br><br><b><i>Input: 3 * 32 * 32, Kernel: 5 * 5 크기 10개</i></b>
                            <br>결과: <i>(3 * 5 * 5 + 1) * 10 = 760</i>
                            <br>해설: <i>Input 데이터의 채널이 3이므로 커널의 채널도 3이어야함. 그리고 bias term을 1 더해주고, 이러한 커널이 10개가 존재하므로 760의 파라미터를 가짐.</i><br><br>
                            
                            <li><b>인풋 아웃풋 크기 변환 공식</b></li>
                            <span class="highlight" style="color: rgb(0, 3, 206);">하나의 레이어를 통과했을 때 input 데이터의 크기가 어떻게 변하는지 계산하는 것은 매우 중요합니다.
                            왜냐하면 그 결과를 바탕으로 다음 레이어의 필터와 stride를 정해줘야하기 때문이죠.</span>
                            참고로 크기를 계산할 때는 데이터의 채널 수는 상관 없습니다.
                           <span class="highlight" style="color: rgb(0, 3, 206);"> 채널 수는 사용한 커널의 개수대로 정해지기 때문입니다.</span>
                            예를 들어 위에서 파라미터 구하는 문제의 경우 10개의 커널을 사용했으므로 3개의 채널의 가진 input 데이터에 대한 output 데이터의 채널 수는 10이 되는 것이지요.
                            각설하고 크기 변환 공식은 아래와 같습니다.
                            <div class="equation">
                                \[output\,size = \frac{N - K + 2P}{Stride} + 1\]
                                \[N=input\,size\,,K=kernel\,size\,,P=\,padding\,size\]
                            </div>
                            그럼 아래의 경우 output size를 어떻게 예측할까요?
                            
                            <br><br><b><i>Input: 3 * 7 * 7, Kernel: 3 * 3 크기 10개, Stride: 1, Padding: 1</i></b>
                            <br>결과: <i>(7 - 3 + 2*1) / 1 + 1 = 7, ouptut size: 10 * 7 * 7</i>
                            <br>해설: <i>위의 공식을 대입한 결과이며, output 데이터의 채널 수 10은 필터 개수를 의미.</i>

                            <br><br><b><i>Input: 3 * 21 * 21, Kernel: 4 * 4 크기 5개, Stride: 2, Padding: 0</i></b>
                            <br>결과: <i>(21 - 4 + 2*0) / 2 + 1 = 9.5, ouptut size: 5 * 9 * 9</i>
                            <br>해설: <i>위의 공식을 대입했을 때 결과가 나누어 떨어 지지 않는 경우, 소수점 아래는 버림.</i>

                            <br><br><b><i>Input: 3 * 10 * 21, Kernel: 2 * 3 크기 5개, Stride: (1, 2), Padding: 1</i></b>
                            <br>결과: <i>세로: (10 - 2 + 2*1) / 1 + 1 = 11, 가로: (21 - 3 + 2*1) / 2 + 1 = 11, ouptut size: 5 * 11 * 11</i>
                            <br>해설: <i>가로, 세로의 길이가 다른 데이터에 대해 직사각형의 kernel과 방향마다 움직이는 stride가 다를 경우 따로 계산.</i>
                        </ul>
                    </p>
                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>CNN 계산이 가능한 데이터는 가로, 세로의 크기만 가진 이미지 데이터 뿐인가?</b></span>
                        <br>이제 마지막 의문입니다.
                        이 부분은 설명할 내용이 많기 때문에 아래의 "<span class="highlight" style="color: rgb(0, 3, 206);">CNN의 종류</span>"에서 이어서 설명하도록 하겠습니다.
                    </p>
                   



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>CNN의 종류</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>2D CNN</b></span>
                        <br>우리가 위에서 주야장천 예를 들었던 경우는 바로 2D CNN 입니다.
                        바로 (batch * channel * height * width)의 크기를 가진, batch 포함 4차원의 데이터인 것이죠.
                        <ul>
                            <li>Input 데이터 크기: 4차원 (batch * channel * height * width)</li>
                            <li>예시: 이미지 데이터 등</li>
                        </ul>



                        <br><br><br><span style="font-size: 20px;"><b>1D CNN</b></span>
                        <br>하지만 우리가 이미지와 같은 데이터만 CNN을 적용할 수 있는 것이 아닙니다.
                        바로 자연어와 같은 데이터에도 바로 CNN을 적용할 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">다만 달라지는 것은 가로, 세로 크기 대신 문장의 길이만 나타내는 크기가 있으며, 이미지에서 채널이라고 불리는 것이 바로 각 토큰의 임베딩 된 값인 hidden size가 되는 것이지요.</span>
                        <ul>
                            <li>Input 데이터 크기: 3차원 (batch * hidden * length)</li>
                            <li>예시: 자연어 데이터 등</li>
                        </ul>
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림을 보면 각 단어에 해당하는 hidden의 크기는 6이고 이것이 바로 2D 데이터에 해당하는 channel이 되는 것입니다.
                        그리고 빨간색과 노란색에 해당하는 각각의 kernel의 크기는 2, 3이며, 이는 자연어의 n-gram 방식과 매우 흡사합니다.</span>
                        그리고 stride는 1로 설정하면 문장의 길이 방향으로(위아래로) 1칸씩, 2로 설정하면 2칸씩 움직이게 되는 것입니다.
                    </p>

                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweUlkv17-ydIDzsRhomC-D3aC0RiX7KrBugyw18Iz4tT8gE0yL5p0oxnnmH3GfDKQBGcxIGpJsdo8jPqzHzgtl5ULDhBW54dgy5tZviPyGSUT4ZPANUHIbuxmjXHqbbrksBLtpD9VhYn_R3bTjNjIgmJe-1A-aUDm9MByoO_T7mATMP1spxsFh1cBs-dMRAimgX2Gknhdw17js1GaRj_71Qk9VSsNMTim3upu_rIkk5KWLF0fTJvhswspDwHh--VxFFKGFMSw0Q75BqOG70IaLUVB-l5g6D0Tu0R9T3HVlUaIeStVgEi4csZAHQRYaBv1QSCGp2-xeFdfVhhR3VYBHUDtqjh5NNjFiqCnmxriNr59wFb0YzWkv1oPjrrhmC-qGBNYiMtXVXRGr-f8xHvA3HYlU9IAaIeyE5Gjjnrst1x1O2lld6jZgwCOtlcvbaa2pqDDbid29-6PMqbknp5_IywdzDWKrc0cil-XaE3o-XVgD-Q_qE63AvS9j0W0zZ-E549gHNoxDR4QNwsqLHnJrlxCpT-SAEvE2h3iEwVsDVZu4A4jiaH_zj40XHGW3KtZ-1WxExf8rOc84bmj7kzbJRnQelJyBUoopkJe2LsUiearBJVZ_NLgSu_rxhZHz2UXax6Tb3LwD4xz2d-Lx0uSFeZuv--BZqxunp8gc19fUHW2wzFy-QtSwoUBadYAZNLXSLJeKrdFVWjCPXLqGcGzo4WQ0jdSSjrjQ8go1L-NGkVC5L-vhLDHVPTkY8lw7TTfZoDmXHm2yqVkZqVk0BNY4_juUF12qQpaObRKLrcR9SLCeCCLqe1V6SotMapra0VHuTEc2JdkRq5J_mkbGfWSxjJgZnmx1fXvrMo-MKiAlyahB-Oh0vjMJjk3eDdP0wKQWnAdOFwfNmevE4SzsvS_GaoVuQHxT7af9eCh6F59VTYPIXD8h9-L6NMRBMrPy1y2ai7Dk_M8lALshzw_8-_SQX1vfPdZpeDKZsxuW4CeH7jcbGce7nPGiC75USIXCayl5D4q3g99yBot0mH53rwviSS1YFrL1qSi8arOSHm_Ph9Dou2SHtJrb1yQZSiksTdlj28ZkHgsenReKONioOyyOatyF-yPioM6pvZBmRs5YW_cL816unA1eDarrWX3nD-VyPFVaMKiSMYczWPsE9g5e8qIFNKMplIIqPYSnH1sFNi6fhviKO9zkOldIhNp3QhhyJxV8wWpvdioW3BUi5YNVLxO9gsknK944Mw1h61S1VyF2kTr4mSljc9hCSw3DmUfWO4" style="width: 100%;">
                        <p class="caption">1D CNN 예시, 출처: Kim (2014), Convolutional Neural Networks for Sentence Classification</p>
                    </div>



                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>3D CNN</b></span>
                        <br>CNN을 1D, 2D뿐 아니라 3D에 대해서도 적용할 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이것은 이미지 데이터의 가로, 세로에 새로운 차원이 더 추가되는 데이터에 적용 가능합니다.</span>
                        예를 들어 이미지 데이터에 sequence가 더해지는 비디오나, CT scan 같은 예시가 이러한 경우 입니다.
                        <ul>
                            <li>Input 데이터 크기: 5차원 (batch * channel * height * width * sequence)</li>
                            <li>예시: 비디오 데이터, CT scan 등</li>
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweU58Y44Yu9FlVOW_-fNAiY9Hc-0IBDAUpspf08fmPdV0c3Die5Mwte2J2XxKYb7vIFkOe7QW320pdT1uZLaGXcT3pUgTbCEfIA7bGyehXXHFAgpZ854zhBV6Zn2yieoy-Jqrz_3dypIBLQksQ5DI0j8xfcXTYxbX4eXAFH1e8cGjgNORxCkVfGJtM3qvojeLN6_WPBK01FHEShrxYg2wXF6CPbxvNNMmE0Ps-lOGzHTOZx3kCtduvWpWp-V1EitMDVb7l7uJc85PBbhO0iV0zKZiR2gGQ3_L0W1BCr_--pjGBGmQFHbgCX16biRCIqe-LQIoxgPnxrPfUx2OKRylxoW2AbiJph_49mn5hZpwnfsopejmqfgAQGuRXXruHqUwQ8PtDc3yBkgpQGs2QCMe5PD0muXoIOIizXJbKUzorRlcWj0Rvgp3TuaAD2WZl8cwncF7Gy_cCxGGgx3FUwWXkloI8_Rnj7FywVjwCadzaG5s93yskyPUfxdjzqtvPLyYmFi1aDSu2pdNWMEJblbVLSAout9zC7ZM6JoYVl2yrfMimzU1R-JgBGgI3iw2sXjhF3rMGj-eKVUrXRsN_NUREXViLCbkaka0bJvUIfPQvrxaMcymg7E80SfovCPbiUpbk6BA2Dt7M0VX4l2i53hKURtEqbIyZHne9J_hQNdf9CsWZwgKjSck1uhvoWdS3iYuMnalctppRjjFAmNFiQ5IwITpSbl0dLyxWxFc3UzTEg5abggVQ3MzM7iJzXN-qtAeIasqn1b_i8RF8VaD8lFNqV9Pk_NS_0o5FPYT3THJ05lV_yADzWoM2rveF0U4T1NwkYe7vUp7AsRrORk6bPZkDOcB0rcseVgsBgwsK5oPVXoo534IJVXDBYyjqw7yel7zV8z0J0Dwl4E3XhYaz35cfIsj2mHyasNRpFukvdyW-TKPMsdgwB5sL_c7OJCuAu0z4gsTBLduDZaMcuQa8BV7WAAUcuThuCrfkHNmZAtc4Bf9K4Pwtfx6jWmxh0q7eNxP63AEk_4TmECwnc614aiQxpuko-w--4tk-0XBxbT8OrnhSD510GdyMhkfxucc_ZbY_4pr1Fynr-6uoXwWHIaR8ZOhXXu1KqnzpjSus3XGPfCCIZManrtbwwb9-cTygXxpgqS3E9XfygFkftqRqWub1FN-IyOiD0ye3pB5ImdRF2HWw44ITnbSShI9VPCnf4QbJ4G-Ekz7qogeV4uAsBjaSdcxusWKP4cGhQ117UAACZeHQmewrgff6DTpW25urv1fngd" style="width: 100%;">
                        <p class="caption">3D CNN 예시, 출처: https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610</p>
                    </div>


                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>2D Transposed CNN</b></span>
                        <br>이번에는 transposed CNN에 대해 알아보도록 하겠습니다.
                        그중에서도 우리가 가장 개념적으로 이해하기가 쉬운 2D에 대해서 한 번 개념을 살펴보고 1D, 3D에 대해서 알아가보도록 하겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">Transposed CNN은 CNN의 반대의 연산이라고 개념적으로 이해할 수 있습니다. 다만, 오직 개념적으로 역연산일 뿐이지 절대로 수학적으로 역연산이 아닙니다.
                        예를 들어 (input * 3)의 연산의 역연산은 바로 (input / 3) 입니다.
                        이처럼 CNN의 수학적인 역연산을 deconvolutional 이라 부르고, transposed CNN은 절대로 deconvolutional 연산이 아닙니다.
                        다시 한 번 강조하지만, 개념적으로 역연산처럼 동작한다는 것입니다.</span>

                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">각설하고 기본적으로 transposed CNN은 input 데이터의 크기를 늘려줄 수 있습니다.</span>
                        CNN을 통해서 input 데이터의 크기를 늘리는 방법은 패딩을 무지하게 넣은 후 output 결과를 크게 만드는 것이지만 이는 결코 CNN의 성능이 좋아질 수 없습니다.
                        하지만 transposed CNN은 패딩 없이도 기본적으로 input 데이터의 크기를 늘려주는 역할을 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림을 보면 input 데이터 숫자에 대해 커널 값을 곱한 결과를 사용하는 점은 CNN과 동일합니다.
                        하지만 stride를 옮겨가면서 계산된 결과가 이전 결과와 겹쳐진 부분은 더한다는 연산이 추가 됩니다.</span>
                        이것이 바로 transposed CNN의 계산 방법입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweWQwNF5dRgJ5m8bKHqXKHsD1A5tt-7jsNXtYiFr8I6VfoBipQNj8kmNKET-iyJFwsEwLrvD_f1-YpebsRZDj2XGGMUcan3lpRC7GfMrjoKu4ZNQs1ergIXjXGKesn0F0b9r1aAkFhg_orVq7BhToNFuuRKRoY6ClO40JMzlu0FcJwmGLQDhhl4G052oQbbdFFO9KIjJvoUsJIVSpd4J7bOZAvLaEfOJuK3QlCIs4cdPhM_1NO888UnCGtFjVLkaC62Mt67CUNDDv5istVJrB-Gwgga3PpG0MaDu6u_GQoaHXCfgX-AXo55fPn3c0yFIZx-QrPl60uf6Y9IG-LKCyn0ua9omzRYOsmYNnJ4s6Z9CYMXkhPoYoxM426YbhQ2wMxk1IK2NdKOaIAlwesWj6AoWRRYRSdLDLaRMJ9dZUColwyyriTZQd141pF1P3pm9fDZ2ZeWKK4RBlrLhCURU6EEVCSXiKvahktDLvGjStAt_z6uf-aNa0YRlUu1MHTbh-18Km6h9EjYOssoqr0YK9CB3QCos3BsvQUgAr_zXSSluZWMU1LhrdZdV91OTYaT_29w4oliMw-Ui5v4EsZ-cutonlRxJFrL0-yfAWPEL4dpJBXARxqDVAwEYxXlPpHeePdAPAPqLPumz7oHvyDGjUyQ4Kt82Iv_jjkwupaqiMFrFroctf69si3YROrUKfuz2Mf91sN9vEkmSLQ3cqO34uLPkq9eZjyqn0r9bFonD4GRPWDvIl2E00KWD8ou4Ur0oPYZtMOYNXGwkP1pwPSnmAEmsviiX9GJlMhgLo1naNCjWV8LpI-F5BldsnRUgDdC29aPiHwvnXUiTTYAWs1veNic-9KEXymvD5MZGb55d3Xu0hymMewMqXNjta-au_JxJsY3FiGSruI2vh9UwqTrBJPuNL9E6Z3H-8B4bfaIrJfE6LIUI5orXso3iPnMS1VW5KA-YFVcVlYCfqE3LPpF8zNeeOhJpQEckrYJDBPMZWunYt2ICT_GKSOjGcvLCgtFwA2KDwnNqP85xQ6lVBwCDaP9KNwWTUevtuDCtrRUmpz2EISVQvhDkr_vXrhZ2a2Qvd17Lij_pt-ezI17QpkFWtHVkqmVwsYdpf9qfwQ-9zmen_XsPbiRvIr_I-3tp3gUjCN0pG66nppXahJxY__OnV0xdsc__byEoL6F7xIS6dExDldVJPW4c2RV4oChySPGudeUk0qbczO4vs3xRTrPSisN-FWIt8MAR0l6Zr_5iOIwX2Z1yoTHFmK84dsVON3USb6xv" style="width: 100%;">
                        <p class="caption">2D transposed CNN 연산 방법</p>
                    </div>
                    <p>
                        <br>여기서 주의할 점이 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림의 CNN 부분의 padding은 input 데이터에 적용시켜 그 결과를 냈다면, transposed CNN은 개념적으로 CNN의 반대의 연산이기 때문에 input 데이터에 padding을 씌우지 않습니다.
                        다만 output의 결과가 padding을 씌운 크기의 결과값이 나올 것이라고 가정하고 연산을 진행합니다. 그리고 실제로 ouptut에 가정한 padding을 생략한 결과가 최종 결과가 되는 것이지요.</span>
                        즉 transposed CNN 부분에 보면 output에 padding을 씌운 결과의 크기가 나올 것이라고 예상을 하여 연산을 한 후, 최종적으로 3 * 3의 padding을 제외한 값이 output이 되는 것입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweV4yFp4_GJx7-JDkFJIDaZcin5s40e7Wb_Nst7QCqZKkpUmWlps9zMm-sauk5mxHGTfh_B--1cAvSzoNrBYHR9T3B9ax0rSKQz2QjAfreAvEYCOoncqnN7huuM4CUrP6EyUhQfzDLc4WBMCBM4NHe7FLewshaBWLI7YUBf8OFMm2fDiVVUBNJSjC9nB7Si3C5YTpMMeVnfAmZdQwMYrFWda4vXLuoaAePUyVI5NG3rYf8qifIJ2WhcykEr5R6Tn7o-y-bMZgImfkxulwtokWJj-TOpJ23nFaiwS6Htygc5ekZBnhHjJoNmbzC1GEmKSHoSlMk4jN4iT4pXU7td52ARt51lNQYW4P_PQAkXS4--C18m7ix6tqYMIfSXUc_GZirn7t3ZiJNVGQP9VCf1z1r5yAfMoR-2kPFx3SzjsGXi5EqKMUgyEkaxjFv3b5VnilhG2AkWpVjOiStGw3PRJVV8Vnclvzmpu4PCj35JPGW361pA3W2T-FkkURmZUWX12hmC49kl2DI82FLmtfq90f1B8YFgss_Nx9ND6__hXxQYmMANDtTfgLjeZk3Yz3a2X5QuwfWtmtPzLGMRMRBkIRgoYsrPEGTJs0Dtu4mr5lnzyGnHRWvCuaO6iEeiI4A2wdTkxs4-ecOEonXiKywoqJy6e0W6_DfBTWTXV0UkprtjuEJpBnlj27SAyck0W5v8789tRtgvSYNARd-bcUk_JWfS-Ae3XIwzYvraHhGYnEwfsiD5c78pxL8FrEcwjRHc3ung9Y8tu7vEJ2V3uoZ0IeTk9e_698Aic-SyWq6-wWoKEiC0kZio6q9-fkHjqEvEioSVQcJqtBsY3hgX1tZlkTkR5OzoCgf_KU0qo6JkqfO68sohUJR8nOY2p30ev0oDw6XHe92yWpJlWhUxvWdirqEQhKNl2pT-AZAO4OEwY6Z40e8UX5gnIUiVeQFKt0u-pDk4wG5Y8OBhhz1C30P6XjOiR1iLwI574fCqaZ08jmG-74psHXSI9_MCd_NubmxFnKsXoALyv-MwxcLPTId8IHDj43G7zrR5zszsOlMpoUe_7dm8GeEPMH4Yfbo-a1KADCUU0deuoq5kBBZcQj31CvMhpefZ2aRgXtal75PD9kdRs-iYPnSLuwoWWU-UTvGSfu3d-CxkoZDoAAe2KoK3U3BWQeRlH15CR4A3ND09_Uzo-HIVSsqV5-rr1UNgGM1VsT-Xebbd3z5JKbX4amEqJRqDDLdAWyxpmdBwmqTzY4sPGZCZbcz6n2pm2eth2_gggqtxw" style="width: 40%;">
                        <p class="caption">2D transposed CNN 연산 방법</p>
                    </div>
                    <p>
                        그리고 CNN처럼 transposed CNN도 input에 대한 output의 예측 결과를 계산하는 공식이 있습니다.
                        또한 CNN과 마찬가지로 크기를 계산할 때는 데이터의 채널 수는 상관 없습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">채널 수는 사용한 커널의 개수대로 정해지기 때문입니다.</span>
                        <div class="equation">
                            \[output\,size = (N - 1)S - 2P + K\]
                            \[N=input\,size\,,S=stride\,,K=kernel\,size\,,P=\,padding\,size\]
                        </div>
                        그럼 아래의 경우 output size를 어떻게 예측할까요?
                            
                        <br><br><b><i>Input: 3 * 7 * 7, Kernel: 3 * 3 크기 10개, Stride: 1, Padding: 1</i></b>
                        <br>결과: <i>(7 - 1)*1 - 2*1 + 3 = 7, ouptut size: 10 * 7 * 7</i>
                        <br>해설: <i>위의 공식을 대입한 결과이며, output 데이터의 채널 수 10은 필터 개수를 의미.</i>

                        <br><br><b><i>Input: 3 * 21 * 21, Kernel: 4 * 4 크기 5개, Stride: 2, Padding: 0</i></b>
                        <br>결과: <i>(21 - 1)*2 - 2*0 + 4 = 44, ouptut size: 5 * 44 * 44</i>
                        <br>해설: <i>데이터의 크기가 커짐.</i>

                        <br><br><b><i>Input: 3 * 10 * 21, Kernel: 2 * 3 크기 5개, Stride: (1, 2), Padding: 1</i></b>
                        <br>결과: <i>세로: (10 - 1)*1 - 2*1 + 2 = 9, 가로: (21 - 1)*2 - 2*1 + 3 = 41, ouptut size: 5 * 9 * 41</i>
                        <br>해설: <i>가로, 세로의 길이가 다른 데이터에 대해 직사각형의 kernel과 방향마다 움직이는 stride가 다를 경우 따로 계산.</i>
                        <ul>
                            <li>Input 데이터 크기: 4차원 (batch * channel * height * width)</li>
                            <li>예시: Upsampling이 필요한 이미지 데이터, <a onclick="pjaxPage('DCGAN1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">DCGAN</span></a> 등</li>
                        </ul>
                    </p>

                    









                    
                    
            


                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>CNN의 사용</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제 DCGAN이 공한한 점에 대해서 알아보기 전에, DCGAN이 GAN과 어떻게 다른지 혹은 어떤 모델인지 간단하게 살펴보겠습니다.
                        GAN이 어떤 모델인지 잘 모른다면 <a onclick="pjaxPage('GAN1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전 GAN 글</span></a>을 먼저 본 후, DCGAN을 보는 것을 추천합니다.
                        <b>그럼 DCGAN은 어떤 모델일까요?</b> 단순하게 말하자면 GAN 모델을 직접적으로 확장한 모델이라고 볼 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">하지만 GAN에서 linear layer로 구성된 discriminator와 generator를 각각 convolutional layer와 convolutional transpose layer를 사용했다는 점이 vanilla GAN과 다른점입니다.
                        그리고 모델에 batch normalization을 적용한 것도 이유가 있지만, 이는 조금 이후에 설명하도록 하겠습니다.</span>
                        좀 더 자세히 살펴보겠습니다.
                        <ul>
                            <li>Discriminator: Convolutional layer를 사용하였으며, batch norm layer, leaky relu 활성화 함수로 이루어집니다. 그리고 마지막 sigmoid 함수를 통과하여 이미지가 진짜인지 가짜인지 score를 내어줍니다.</li>
                            <li>Generator: Convolutional transpose layer, batch norm layer, relu 활성화 함수로 이루어집니다. 그리고 잠재 변수(latent vector) z의 가우시안 노이즈가 인풋으로 들어갑니다.</li>
                        </ul>
                    </p>
                    <p>
                        <br>아래 그림은 DCGAN의 전체적인 구조를 나타낸 그림입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweUfG2nq5WT7vEZFqxQUymbJDZ-XQLnVgXEx9j2TckBh66Fu4dEgK4HRBdaaPkGM6XOVa1CHEFFBdpwu1_7h2t9mEm4HAQVk0scaphLBfsTpH9O9THB8Ux97NaXu3BgSfJDUzE7hMmEaleyRrlhWZ1XOVBN_vh0qzrjpPS9EqMs8ERFiXFwNp8kzHNOJ3irC2Uh3PiQDbd-fmEHOogCxjimTZOt7ygbACbnsTt_kEBaK7p1IlLNIBs15dPKjiwqVjW-SM49EHge1HkXLGN7nMEsc5dXzNnn-beN3zmoiQ5zqChZnyB7mVeIMS2wzaUBGDWAKTtxneHfve4F-sbC6ENnh8MaZWC1cAHX9p-sbT9Lr-zgbX7Q3SsYcd-ZyB3qS2gGhx5YTDmigPcgfGBYDn45EAEySDNZZI2qINx6I7koq3Gzr9nAWvnvLdiH1MhBVjWemQB4luBdLHAKRGfrmTZDYRMuBaitafRmtFI4AjfZCnPbInotApolj58E8NZRl6FArr3J9XWWv9UNjj0be0p4xmm4Vamm_r780tF5qLU5puGnwPgyFzx7dIoJSt5l_6JfLsJkF-2X93V2hMpqpfI4WEnhPuDAjUjvZl2I79zuGpaw-dOqnjYhi6tLgCEhK68K7d1qNR0_U7vawFtyKsdH6j5KunO2bcPhF_avlQ9JmTS9W3uKFEwjmdpGthuBX-FqGuRS7AByMSK4-TnmfBDU4Ejx5ZR7t0Lm0LvD5BcqfJjj3nzGJ5TU0r8pXU4IfSDPSj8TGd2DWZyZc" style="width: 100%;">
                        <p class="caption">DCGAN의 전체적인 구조</p>
                    </div>
                    <p>
                        <br>DCGAN이 어떠한 레이어와 함수를 썼는지 간단하게 살펴보았으니, 본격적으로 DCGAN이 어떠한 의의를 가지고 기여를 했는지 DCGAN의 결과와 함께 자세하게 살펴보겠습니다. 
                    </p>

                    
                    <p>
                        <br><br><span style="font-size: 20px;"><b>GAN의 불안정성 해소</b></span>
                        <br><a onclick="pjaxPage('GAN1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>에서 vanilla GAN은 mode collapse, oscillate 등과 같은 문제점이 나타난다고 이야기를 했었습니다.
                        DCGAN은 이러한 GAN의 불안정성을 해소하기 위해 아래와 같은 방법을 써서 해결하려 하였습니다.
                        <ul>
                            <li>
                                <b>Batch normalization 및 convolutional layer 사용, pooling layer 배제</b>: Pooling layer는 CNN의 결과의 크기를 줄여주는 역할을 합니다.
                                즉 모델의 결과가 너무 크기 때문에 그 크기를 줄이기 위해서 필요없는 parameter들은 지우고 정말 필요한 특징들만 남겨두기 위한 레이어입니다.
                                하지만 사람 얼굴 이미지 생성을 예를 든다면, 데이터를 생성하는 데 있어서 사람 얼굴의 주요 부위인 눈, 코 입 등도 중요하지만 각각의 위치도 이미지를 생성하는 데 있어서 아주 중요하게 작용합니다.
                                <span class="highlight" style="color: rgb(0, 3, 206);">따라서 이러한 정보 손실을 최소화하기 위해 batch normalization을 사용했으며, linear layer 대신 convolutional layer를 사용한 것입니다.
                                그리고 같은 이유로 pooling layer를 최소화 한 것입니다.</span>
                            </li>
                            <li>
                                <b>Hyperparameter 제안</b>: 추가로 DCGAN 논문에서는 안정적인 GAN 학습을 위해 새로운 hyperparameter 조건을 제안합니다.
                            </li>
                        </ul>
                    </p>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>Black-box 모델 반박</b></span>
                        <br>CNN은 비전 분야에서 이미지를 처리하는 데 효과적인 알고리즘이지만, 어떻게 작동하는지 그 원리를 알 수 없는 black-box 모델이라고 지적을 받아왔습니다.
                        하지만 본 논문에서는 DCGAN이 학습할 때 활성화된 filter를 보여줌으로써, 특정 filter가 이미지의 특정 특징을 스스로 잘 캐치하여 학습하는 것을 증명하였습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림에서 보면 오른쪽은 학습하기 전에 랜덤으로 활성화된 필터의 모습입니다.
                        랜덤으로 활성화된 필터는 이미지의 특징을 잡는 것이 아니라 모든 이미지를 잡는 것을 확이할 수 있습니다.
                        하지만 학습하고난 후 필터는 침대의 모서리, 창문의 모서리, 침대의 대략적인 틀 등 스스로 중요한 특징들을 이해하고 역할을 잘 나누어서 학습하는 것을 볼 수 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweXaA4lhNpO_Io4cB2PUlBWotue-eI1ywhdK_rtTZZ8pyMxwODAG1iTJeyjfis_OY7VumvbC5jn-Mq7fQsuGcBb5lTInap8LjdrEqWjkdOnvQc-SBelizj3AEr_AlEPNhKjulvtdezOLr0XKq8SpL1RRdJqn_aN16jwOh70FSJNtMOVXYsV_szKTO7-Bsk0TaUy8CG8QcPVDKasPJGqw9jezM6bMTEy77ndnYfUtCO7JYj_Iu0RcpFa7hXXjv4CJGmZFvgYb5PWEW54MwECp5B28kVIh6n6J0zpTBnrwV66ASEsM6LWpmqOdmECzz_qxd_cUkDp_3Mh9NsWq8LpgqDpHkZA1Szs1dMIwzQDvAKtkH3SB_4k0Ye7QtLHN27lDxJZ-y34ac-fzitCjK3_v53hCbOxBnEg3rnfhuqZ5quATJmm5tPxgghGs_Kj0h-SDmfcCRbjwcjnibGf8CtSH4dShod_TzF-2n5-KdlhOwwPzWEysR2UfZ3x5CNTicesAlxeTafdd2hHZoqnEc7-sl6G4ZERfqLeJr6VaSWaWHFb32gqov4g1KX68D2vv58IDeqbAb028sQ4tDEsGSEBZUvmhovbFNGvC1fToq2UTpujA2AW3doeT2vP2zoHdZ_ly9GjHW7LPtD87o-k8vY8L-MGOa7qQShIHeM77b9JMaaFVnA8WekkS_hTcVXdF_iG8GFqfDdsKotnQvz9EAMfJ1U7By5J4aJGTdynkaM2OboPmmW73rIrsySCq3ayO9Rx8OuURK8rG_Ucdv_-x" style="width: 100%;">
                        <p class="caption">DCGAN의 활성화 된 필터</p>
                    </div>
                    <p>
                        <br>그리고 논문에서는 정말로 이 필터들이 캐치한 데이터의 특징을 바탕으로 generator가 새로운 데이터를 생성하는 건지 증명하기 위해 재미있는 실험을 하나 진행합니다.
                        아래 그림의 위의 줄은 LSUN 데이터를 바탕으로 학습한 generator가 생성한 방의 모습입니다.
                        그리고 아래 줄의 그림은 CNN의 필터 중, 창문의 특징을 학습한 필터를 비활성화 한 후 generator가 생성한 방의 모습입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">여기서 놀라운 사실은 아래 그림에서 원래 창문이 들어가야할 자리에 모델이 문, 벽 등 다른 특징으로 채워 넣기 위해 노력한 모습이 보입니다.
                        즉 여기서 알 수 있는 사실은 DCGAN 모델은 "learning" 뿐 아니라 "forgotten"도 가능하다는 사실입니다.</span>
                        이로써 DCGAN, 더 나아가 CNN의 모델이 black-box가 아니라는 것을 반박한 셈입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweUmZ3t79jf1SXLd7NINiGbcZs4WPMk1eKS27Qhu_9_mmexSJSAJ36HLCUvXlhcaE1rMgYgFAnOOdNBrNj0xSXl2lhWq1nIxfQQ2HKLcxR0A4iiN2otBFai5qPmCqlWQ0K1srx3khIwWXsVq2XlVRjVFjtKKZdAQMuP-QgHxqoGFDRVrCEeQHEAft7x1n4Fbd0LlDeJ0bfE6DBRvV29XOrW-47eWMVjyVFQWyPvLPu1V3OqjOmU-OVfsdNEwZpnn9EucU6vsFZyOXRnD6fBPt653nZr-XfxN_wm4nida1LEC-WuN8uD4_sEgnAtn7IFmt1JJxNo6beysYNgE8MzKw3eplS1XF1nX5kXdd20CfwNgsEF09zlyW4yjMRSGFhsjqHBPvLefltFUOooWPGhQA5o_lmSYaqJvx1v3aDy0fniW9-aEVkkBwe9XOVNtlWe2_B4QQHTTH1ocK6jwX57pHZcfG8TOcZdieXgdY9MZUnf_QTYkxzuiZ4da7727ESNjCEBrl73ONCpu7sd_tkyLNS3f4wcLEvhTq8fcXtbdb_ORfnfonldYXCFdYsZLuQFjWMvUjvDcvnHSJTFI7KKIerPzXrg21JMpouoSo5CaqnX8UQl14NraGw5ZQ62_VJmQmjHAAFFuKzXMGvM5_a8Dog7bY9N0ar3v1FwSmZVZtLSmHgj29RkuGhL7c7Ovj2rUSmH147gKNAi5PPVbgqFCJPcb4HKX4OZQ8Xhm2lJ0RZGbX1FbZ4PWi93Qx9uNP3tJ1c4DkKgIaFXp8DGG" style="width: 100%;">
                        <p class="caption">위: 창문 필터 유지한 후 생성한 데이터, 아래: 창문 필터 제외 후 생성한 데이터</p>
                    </div>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>Walking in the latent space (잠재 공간)</b></span>
                        <br>DCGAN 논문에서는 generator가 생성한 데이터가 연속적으로 생성 되어야한다고 얘기합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 이 말은 잘 학습된 모델이라면, generator 맨 처음에 넣어주는 잠재 변수가 존재하는 공간이 특정 의미를 지닌 semantic level의 연산도 잘 할 것이라는 이야기가 됩니다.</span>
                        예를 들어 우리는 "woman-queen", "man-king"의 쌍을 본다면 의미론적으로, 감각적으로 어떠한 규칙이 있는지 알 수 있습니다.
                        하지만 이런 의미론적인 것을 모델도 할 수 있다는 것인데요. 가장 유명한 예시는 자연어 처리 NLP 공부를 하신 분들이라면 word2vec을 아실 것입니다.
                        Word2vec은 아래 그림처럼 모델도 단어의 의미를 잘 파악하여 벡터와같이 연산을 할 수 있는 모델입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweW-InO4Wj-DejYjuBHkhvfFu7rQ-iMXxLJYRwOKrwCoBMotw-RWOKtqPZ4EUD-aFvAZJ1gWp1Llwfmj-aEsKh2VFxWXg8CRgxuUyk5-nSE__GRH5MANJrBJVW09HhpwNyTPxlVcLQFLex21aUWyCcVlwR_ch3s_KC0HarqAY9L1wqB6vRivBTonyh9K8rghIHpv2QhfsEt20hsTTbbAvcV8_CIsVbRaV1kKgWLixEd2wRQbS_Qmo3Whpm3bM5ZLzxxn9KBGWcJFPNRTJ-ognORQVnk1g7GwrkpMrt247cIfM4GrF3xv-OWn8vv69Xi2c_XXHIUGVh93wJ-UMmpcvZUZwYsFcwrypfVcW4J-N3097dHjV5iIcEcym1ZvINBeRhN3tDV6bhNdyzF1o1XT-hSXKdeRvYYowbsibqieApfEBx5n6zRPs0SS66GpgNbCnhzLm0a7U2Qvo-ZJO2cZBqFZziIUFg3oGQCZdLkbnHXLbd8wCuMiyYoEiODNFzQ1yLWOaYeuETrRu26wLUn76xrvLKfyr4q9f73hcehq3bzy4l4u4lNfDHX1Xcor9ZBEkGdbQZaGKGtFsy5rkmfC-zM2UFHaTO-S3QKrNBNVGWdLhmbCnIB2wkGadMUOKT2HWk7WwUvjrUwWzCpXL-RMwWduOrzZhdHvupGOm8nYnYxq3tajw7SXquj86nIBxTYfVZKZJwjy6J3vVOi3VtJPCqxiuzk-qdI4y5IOkjIxzcT9M8vRy6dA-WmBH8RoUDmhCsctesowjTRNfrwn" style="width: 100%;">
                        <p class="caption">Word2vec 예시, 출처: Mikolov et al., NAACL HLT, 2013</p>
                    </div>
                    <p>
                        <br>즉 DCGAN으로도 저런 의미론적인 연산이 가능하다는 것인데, 아래 그림을 보면 그 예시를 확인할 수 있습니다.
                        "<b>안경 쓴 남자</b>"를 생성하는 잠재 변수에 "<b>안경을 쓰지 않는 남자</b>"를 생성하는 잠재 변수를 빼줘서 나온 최종 잠재 변수를 generator에 넣으면, generator는 "<b>안경을 쓴 여자</b>"의 모습을 생성하는 것입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">여기서 계속 잠재 변수라고 하였지만, 사실은 잠재 변수는 Gaussian noise에 불과합니다.
                        대신 generator 모델이 잠재 변수의 어떤 특징을 잘 파악하여 특정 이미지로 mapping을 하는데, 이러한 함수가 의미론 적인 것을 잘 파악하고 mapping 하고 있다는 사실이 중요한 것입니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweW8SLHHGmvu3lgz9mWg10Lco8UTQ0oomVI9gygfJomcLKAswOZOXgQ0ixnWadwJ_8rl89fq2MpPKKYL3oV0r3JKMP1YLKCX8Pra9v7TyzvRI2KX3gJMbrYnt4rtjUKRLhTeaT70P54GG5YDEg0s16Yndb6AU2UbSXPNwhuV-TEHkxxE4PZUPVHJ8DzSi0PWIjauM2TSQjfJEa7s42ziSj8AH3i-5MSDqR9IvDipTik8-GGMWrDNe58iJ_a2so7HxgIh7eWrBHAzFH1tV8YYnJoTIyUzDqekMIg2s33ji2k1LAOEgSMqHjvF460YCzZhURupyhSpYnwE8f4piN9Fkaov_Dyzf2b5oCCoxu451QsElTU5lQt8B7xYLPCIJnWEC-X7J97CGTN9JHQ0XsE4aUbWW3E8OdxNKEXEtf7qCMX1_EPSlxG7aJqSIDm89TR-vCwXVKJu12aaE9kcCWF7wAlX_64Ms_zyJF6k3sz2Z1rJPzdTTLKu5bEvWNaUqE3NShqiShwqOTJ8hE-23-OCRAx5y8x6xHGNAnLTFZR9ten0QIAtRfQIuD80-HMlQi6uK66P64V9U4w4FTTQBm0hCJC7sElYINUdrlcsVwYJtvnU94SnyJSXaWAIYkVIKBLKdjroW5rPJ9EnClfqUf83yo-uV8y0LQzyqbTnp_CxvqLH44ByKotbQJbAvtxb_rdrg3KiBn2Vsn8kgKdGHQeEi9co-ugJYaOvRXj46awNtkw2-OWbUpN9e0eLrUbpv3xqbuiN8gz4O1MA_Fzr" style="width: 100%;">
                        <p class="caption">DCGAN의 semantic 연산</p>
                    </div>
                    <p>
                        <br>추가로 DCGAN은 저러한 연산 뿐만이 아니라 구체적인 의미도 이해를 한 것을 보여줍니다.
                        아래 그림은 DCGAN은 "<b>turing</b>"의 의미도 안다는 것을 보여줍니다.
                        "<b>오른쪽을 보는 사람 얼굴</b>"을 생성하는 잠재 변수와 "<b>왼쪽을 보는 사람 얼굴</b>"을 생성하는 잠재 변수를 interpolate 하여 나온 잠재 변수 결과를 바탕으로 generator에 넣었을 때, 회전하는 모습의 데이터를 순차적으로 생성하는 것을 보여줍니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 DCGAN 모델이 의미론 적인 부분을 잘 이해한다는 것을 뒷받침 해주고 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweUmK5NBw50GyQgj4ytO4n_aDjc1jQl1YKqOygFQ1vWf3wDPk22XEz9z8J7TRa9rz7zJ1BJOTYb-JhbDUWddS0GDL4lIPax3cnOYluqvOzGsYY8ejhVgnAvW1JB1tWiQj82VEQ1kf8SLdSUJKI7PDB9mhClYrUzhU46l1xDRxcY1uP3WSN62peK9lXG6BlA_ipLA2QaeVRjwEsih7NCJXpZoJkCzDRix4BmVrPSsnSLXuJDPgokAxWcXSwH19sp0LTpg3XXX0bp_qSh6J1Fr1QSvIrJfo2W83U8niRiI0bf00xZWjDurQqP23yFHY3thzHDRuvU2F84IULjjEfAhpXhxWyvHrzhIYzwpUj7YnSNqTSnI9rTapEYujFECWpOrJFv0O2AdamZQTmthZgZnew5FaGffdZKrrZHOGH31vJZVtNu23fz40ud249soPn0aLKbZYK089PPiuEj1x1sHI1KpxB3Zcnj8CG2xJcHbU9rkFuphVoL5_Ma5xUun3b1-q0dMhS2qvuvOKJA-CqTliiVCrxp7Yi0R3zQmGGjdpXD1grqEWsH6ZwBEGQOnIRcL19LfKDAMhqNZW2dYJnwTi4qnmm-LeBABNaPFq-xuhHW5ism58zRMFavbgJd5r5LytLODrxsHkCZiDCr-cFO5qntxS2xeid54YLMBcjV2fQNEH-3qSSz_YgyJn1peAzabwkigKVBVLTAdmkloseVcSxzmAQg-PWqvHWUYX-dqtzyiPYwFI2-Ua3B2P_c4IvS8JHj6cZCPg2frgVlh" style="width: 100%;">
                        <p class="caption">"<b>Turning</b>"을 이해한 DCGAN</p>
                    </div>
                    <p>
                        <br>그리고 위의 그림의 예시와 비슷하지만, 특정 잠재 변수 두 개를 선택하여 interpolate 하여 생성한 그 사이의 잠재 변수를 generator에 넣어준다면, 생성되는 이미지는 순차적으로 변해야 한다고 주장합니다.
                        위의 그림도 오른쪽에서 왼쪽으로 서서히 변하는 모습을 보여주고, 아래 그림에서도 특정 방을 내어주는 두 개의 잠재 변수를 그 사이에서 서서히 변화시켰을 때, 서로 다른 두 방의 모습이 서서히 변하는 것을 볼 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">논문에서는 이러한 현상을 walking in the latent space라고 표하고, 이는 모델이 데이터를 기억하고 있지 않다는 것을 방증합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweV507wttXdJqqjbWB4d-CZsAIlLIgzFAmNtIU97CP0OLBgTICYXPcX2sf7VmYgk_-qRs2KSk8tqDv6qXHmFMKOjXg4egPvf-yhnnar1eCZSvQQsNZHuQBtnMqV_164wbGrP6vye934DUKoelbOdJbTu3C062Fn2YJUpXjhK-tpRDH8ZYxVL_5IhwGlE7RB4ZFdZrLXVhvQpdTgJnWTbIIiFU2WjsCiUm2Bd2hxe0PqP-fNhzVKedO8NruKZo3eCNK8WZ_5bkBW2ku3lyO4bzA0NqGSieDqIIawTX2oE18nhMOwB3tcNsFIeAfWtCr-YYXfXQ_Q2u49E-UCiLtjfT-VsF2V5ncBa9PBaHKu5gdUG4WVLhA9DOEnxpdDysJ1kqI3-IClqa_o5btzmSul6RCf7gNJwvTsOvzvRSYZAfNnVhVruc8dGMRZNXJs5s-K1kD_rd7maEtaHbN-muMKhk5GdiCDdtEIPOnBto558b23fjTZAhx9se8wYf1nQjitPGVagsRgaaYgCauFa7x4dfc0d9ga-eLK-3YeI3GpmmhWgtxx3CYdS8xMEQrJsHsd9BmclPUkXciqksXOu51NvPHHy6b8LsQfc7lAfNPxvaRLAM771IPIMlpyLrOQGhvr__j8_c60lX-R1R7nkjWYqIazr9pC4IB4l76YOS1EuROByKCbMeCyVcBYWqWUkbhliGCPblbXPf5AmGAoU6gBMrScrnIwhEqkpBaU6J29H9gHKGhtWVVKPM9M6GyFWGgj-k_KZmcyr26BUU_Oa" style="width: 100%;">
                        <p class="caption">Walking in the latent space</p>
                    </div>

                    <p>
                        <br><br><span style="font-size: 20px;"><b>Unsupervised Learning 적용 및 성능 비교</b></span>
                        <br>그리고 DCGAN은 unsupervised learning에 있어서도 상당한 성능을 보여주었습니다.
                        아래 그림은 10가지의 카테고리의 데이터를 가지는 CIFAR10 dataset에 대한 결과입니다.
                        DCGAN을 CIFAR10의 특징 추출기로 사용하여 그 결과를 pretrained 분류기에 적용을 하였는데, 83 %에 가까운 성능이라는 놀라운 결과를 보여줍니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">여기서 더 중요한 점은, DCGAN은 CIFAR10을 훈련하지 않았으며, ImageNet dataset으로만 학습하였다는 점입니다.
                        즉 보지도 않은 데이터의 특징을 추출하여 사전 훈련된 분류 모델에 넣었을 때 상당한 결과가 나온 것에 의의를 두면서, CNN이 unsupervised learning에도 상당한 경쟁력이 있다는 것을 증명합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweUrkO9x6gUzDb3sP8Do9QeTvJ31kqm_yrXTr67SbmRYE_Z7xhYRUZGfw4pyWhKIyybYVId4w5YlPfW-Tk2_an0_yYomLfYnDGa5ZVjtlgDzABhcirOu7a5zzMoO7FghPqAViSr8vGdSCRGM4uZ94yK9kfMjXlhx1Le915nBxBGHxrNVccXWUCo2t_pt2t1_FuMUDydTYc9vSCg7R0YI1-g1snx6tST-EvRRv7pfhcZi2F3CpTWU-rmfChVO825sO5uOjC4wMPEFrsD4yFC-lDpa-M6aGpwp6RSoneS9jK2UZmysrAOV3PuWWqMpAvvZ7iT2kkeHzK_8QZewvKJDbqRNthAbpYjjtvvAS7TfghmgOTtcFIB_u3jAcd1ZiCYZ5_8m3izaYohJY2VdJkQgmyFtG4zWJGS_M67Bdq5hTXzwNhDw1ivIY8ZgyATux5fFe-vU7iqObjaq3S43xc-qfjGbkpvtCsWUt96tBA0mmiLUciHtGHWcOhYKOJNvUZN5Ome9h2-6yqMAns_sczx9TkyxMHXPswzST_euXAAGvn924rmmvjmiCr8Z9gf5lNZ2mt33lHbC0zL2OeH5R81bWnBjgWKVC5Lvl4fRdcmA7sKycshCQrtzIzqIAqbxcc7Wnw-01oTs0Z1RGc2XdMiRi5vO1V_rbX9sLt9-QAdVWCbMyNTxo-yrwEmLbmKaJPTJvdn-8ibNfeBlF5sijtMa716OgLkDvXmtGME5Uf3Q2MMiXTrtruqd0uqXwCuHxPnfqsJBdsAaCne8lVp6" style="width: 100%;">
                        <p class="caption">Unsupervised 분류 결과</p>
                    </div>
                    <p>
                        <br><br><span style="font-size: 20px;"><b>생성 모델이 데이터를 기억하지 않는다는 것을 증명</b></span>
                        <br>아래에서 DCGAN의 1 epoch 때 생성한 결과를 보면 방의 모습과 흡사한 것을 볼 수 있습니다.
                        1 epoch 임에도 불구하고 방과 흡사한 모습을 생성한 것을 증거로 모델이 overfitting (과적합) 되지 않았으며, 데이터를 기억하지 않는다는 것을 증명합니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AAWUweW-lPxdnnJhWfJAz524g-5ExKFfcqHK6mafqE0mMFDJs86dh05-1l6oRRuV-EahDPazAJ_AZ26l1rWGUoGwkyKmyV4GvHCL-d6KMyi18v2qTc_kHmYjqKmTDPFuRTLBTJMMjMszNxSYA28-6IKI99U2CzoOn2rZ7rgcDqd2cweC_91M4JvKN85HDjkmDiT7_-f3dHfsiY2wcP9s-w-bFE6tafuhnITPBESinhNvtA4exmcLbN12tmusUuToRRUCCgeLaWr9c_4rMlsRk48oFyctPmmgikAn8RzpmrI9sT623s3WSHq0KvCmVsT3I4pBA1YOdhOLSllAoMR4UvB0WiDZnvS06tMwtcFUFFqh0Pao4t9zFxJ-A7KhvDc1JaZegWLpfa6XeTM7MNAAr8RtKl5icmWA0bdUqIFrDECLYtuu-BCtq2Wv-7T17MM39MDxzt6Y0iAAzFSKK0s-vsmOuOpeCnSo2DX90y_zL3iT53VpAbH3A--03GDmYHhjOJZmVCkxdjpFYmd5FU0GTQQJqwuB0LLpNMwrB--KhrdUnwFc6G4JfgIXTaCJsgNEwI4RiBZtNqL_kEOQXE13ojcJDA2cKeRFgePYyPDawAux767L28gszQNvKyduDcS-HrP9oWQXlE6oSaBWIPWOARfL4Li1r7z-LhQc8ivtNX8tqJO4mG6BP3VGJ4WEergIyzyx2d_01dnBY80U3ln_aSnhufhNLVN_yZxZ37tEoE83-eoP9bWdH0CpC5GjKhHv6D2Ka1GQaVEugQWo_jvO" style="width: 100%;">
                        <p class="caption">DCGAN의 LSUN 데이터 생성 결과</p>
                    </div>



                    <p>
                        <br><br><br>아래는 DCGAN 논문입니다.
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/1511.06434.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">DCGAN 논문</a>
                    </div>
                    <p>
                        <br><br>DCGAN은 GAN의 불안정성을 어느정도 해소하였고, 모델이 semantic한 부분을 이해하고 생성한다는 것을 증명한 데 있어 매우 큰 의의를 가지는 연구입니다.
                        다음에는 이러한 DCGAN 구현을 해보겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#DCGAN&emsp;#Semantic&emsp;#생성모델
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('CNN 첫 게시물 입니다.\n\nThis is the first post of CNN.')" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('CNN2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>CNN을 이용한 MNIST 분류</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>