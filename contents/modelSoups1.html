<!DOCTYPE html>
<html>
    <head>
        <title>Model Soups</title>
        <meta name="description" content="모델의 성능을 높이기 위한 방법 중 하나인 model merging 기법의 model soup를 소개합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/modelSoup1.html" />
        <meta property="og:title" content="Model Soups" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="모델의 성능을 높이기 위한 방법 중 하나인 model merging 기법의 model soup를 소개합니다." />
        <meta property="og:image" content="" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Deep Model Fusion / 1. Model Soups</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url();">
                    <div>
                        <span class="mainTitle">Model Soups</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2024.01.17</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번에 소개할 논문은 모델 merging 기법으로 소개된 Model Soup의 논문입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">현재 LLM이 각광받고, LLM open leader board에서 SOTA를 차지하기 위해 최근 model merging 기법을 많이 사용하는데, 그중  model soup는 시초가 되는 방법입니다.</span>

                        <br><br>실제로 model merging을 하는 기법은 간단하기 때문에 쉽게 따라올 수 있을 듯 합니다.

                        <br><br>아래는 Model Soups 논문 링크입니다.
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/2203.05482.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">Model Soups 논문</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>Model Soups의 동기</li>
                            <li>Model Soups의 방법</li>
                            <li>Model Soups의 결과</li>
                        </ol>
                    </p>



                    <h1 class="subHead">Model Soups</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>Model Soups의 동기</span><br>
                        <span>Motivation of Model Soups</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        Model Soups의 동기는 fine-tuning process로부터 비롯되었습니다.
                        먼저 우리가 어떤 특정 domain의 모델을 학습하기 위해서 보통 pre-trained model을 fine-tuning하여 사용하는 것이 일반적이며, 아래와 같은 과정을 거칩니다.
                        <ol>
                            <li>Pre-trained model을 다양한 hyperparameter로 여러 fine-tuned 모델을 확보.</li>
                            <li><span class="highlight" style="color: rgb(0, 3, 206);">그후 validation set에서 가장 높은 accuracy를 보인 모델을 선택 후, 나머지 모델들은 모두 버림.</span></li>
                        </ol>

                        <br><span class="highlight" style="color: rgb(0, 3, 206);">저자들은 이렇게 fine-tuning하는 과정 중, 위의 2번의 방법에 문제를 제기합니다.
                        2번 방법의 문제점과 이 문제점을 해결하기 위한 ensemble (앙상블) 과정을 아래와 같이 주장합니다.</span>
                        <ul>
                            <li>Out of distribution dataset에 대한 성능이 보장 안 됨.</li>
                            <li>여러 model을 ensemble하여 성능을 높일 수 있지만, inference cost가 \(n\) 모델일 경우 \(\mathcal{O}(n)\)으로 증가.</li>
                        </ul>
                        
                        <br>따라서 저자들은 위에서 설명한 fine-tuning의 두 단계 중, 2단계를 개선합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">저자들은 여러 hyperparameter로 fine-tuned 모델들을 merging하는 방법으로 개선할 수 있다고 생각하는데, 이에 대한 근거는 아래와 같습니다.</span>
                        <ul>
                            <li>같은 pre-trained model에서 파생된 fine-tuned model들은 비슷한 loss basin (landscape)를 가짐.</li>
                            <li>Ensemble, weight averaging은 좋은 성능을 보여준 사례가 많음.</li>
                        </ul>

                        <br>그리고 저자들은 model soups를 사용하면 추가적인 학습이 필요없고, inference cost도 \(\mathcal{O}(1)\)이라서 추가적인 inference cost가 필요없다고 주장합니다.


                        <br><br><br><span style="font-size: 20px;"><b>Intuition</b></span>
                        <br>먼저 저자들은 다양한 hyperparameter로 모델들을 학습한 후 아래의 loss landscape를 그렸습니다.
                    </p>
                    <div class="contentImg">
                        <img src="" style="width: 100%;">
                        <p class="caption">Loss Landscape, 출처: Model Soups 논문</p>
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">위 그림에서는 3가지의 다른 방법으로 학습하는 방식을 가지고 학습한 모델의 loss landsacpe로 그린 것인데, 빨간색의 부분이 오차율이 가장 낮고 loss가 가장 적은 좋은 지점으로 볼 수 있습니다.</span>
                        따라서 저자들은 저렇게 optimal하지 않은 두 모델 사이의 가중치를 interpolation하면 정확도가 향상될 수 있음을 시사합니다.
                        
                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">그리고 모델의 loss landscape에서 보여주는 각도 \(\phi\)가 90도에 가까울수록 uncorrelate하다고 볼 수 있습니다
                        따라서 저자들은 각도에 따라서 실제로 가중치를 merging 했을 때 정확도가 얼마나 향상될 수 있는지 경향성의 그래프를 보여줍니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="" style="width: 80%;">
                        <p class="caption">Loss Landscape 모델 각도에 따른 가중치 merging 시 개선률, 출처: Model Soups 논문</p>
                    </div>
                    <p>
                        <br>위 그래프의 y축을 보면 두개의 모델의 가중치를 평균내어 merging 했을 때의 정확도가 각각의 모델의 정확도를 평균냈을 때 정확도 차이를 의미합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 y축 값이 커질수록 모델의 가중치가 합쳐졌을 때 정확도 개선이 큰 것이죠.</span>
                        결론적으로 그래프를 보면 두 모델이 이루는 각도 \(\phi\)가 커질수록 모델 가중치를 merging 했을 때 개선이 많이 일어난다는 것을 시사합니다.

                        <br><br><br>위와 같은 이유 때문에 저자들은 model의 가중치를 merging하는 기법들을 제시합니다.
                    </p>
         


                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>Model Soups의 방법</span><br>
                        <span>Methodology of Model Soups</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        먼저 논문에서는 아래와같이 표현을 정의합니다.
                    </p>
                    <div class="equation">
                        \[f(x, \theta) = Neural\,Network\]
                        \[\theta_0: pre\mbox{-}trained\,model\,parameters\]
                        \[h_i: i\mbox{-}th\,hyperparameter\,configuration\]
                        \[\theta_{i}=FineTune(\theta_0, h_i)\,model\,parameters\]
                    </div>
                    <p>
                        <br>그리고 아래 표는 우리가 일반적으로 fine-tuning하는 과정, ensemble, 논문에서 제안하는 방법 3가지를 제시합니다.
                    </p>
                    <div class="contentImg">
                        <img src="" style="width: 80%;">
                        <p class="caption">Fine-tuning 후 모델을 선택하는 다양한 방법론, 출처: Model Soups 논문</p>
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">위 표에서 가장 첫 번째 방법은 우리가 흔히 사용하는 validation accuracy가 가장 높은 모델 1개를 선택하는 방법입니다.</span>
                        그리고 표의 두 번째 방법론은 \(f(x, \theta)\)로 표현되는 tuning 된 모델들을 voting하는 방식등의 앙상블 방식이며, 이는 여러개의 모델이 필요하다는 단점이 있습니다.
                        마지막으로 나머지 3개의 방법론은 이 논문에서 실험을 수행한 방식들입니다.

                        <br><br><span style="font-size: 20px;"><b>1. Uniform Soup</b></span>
                        <br>이 방법은 아주 간단합니다. 같은 pre-trained 모델에서 fine-tuning된 모델들은 그 구조가 같을 것입니다. 따라서 각 fine-tuning 된 모델의 parameter들을 모두 평균을 내는 방법입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">하지만 이 방법론은 n개의 모델을 평균을 내다보니까, 오히려 성능 하락을 내는 모델이 많이 있어서 greedy soup의 방법을 제안합니다.</span>

                        <br><br><span style="font-size: 20px;"><b>2. Greedy Soup</b></span>
                        <br>Greedy soup의 방법도 아주 간단합니다. 이 방법은 아래와같이 수행합니다.
                        <ol>
                            <li>Validataion set에 대한 accuracy에 대해 model들을 내림차순으로 정렬.</li>
                            <li>연속된 모델을 하나씩 parameter의 평균을 내면서 성능이 하락하는 모델을 버림.</li>
                        </ol>
                        이 과정에 대한 pseudocode는 아래와 같습니다.
                    </p>
                    <div class="contentImg">
                        <img src="" style="width: 80%;">
                        <p class="caption">Greedy Soup Pseudocode, 출처: Model Soups 논문</p>
                    </div>
                    
                    <p>

                        <br><br><span style="font-size: 20px;"><b>3. Learned Soup</b></span>
                        <br>Learned soup는 greedy soup에서 순차적인 실행을 제거한 방법론입니다.
                        이 방법은 gradient-based mini-batch를 이용하여 각 모델의 parameter가 merging 될 때 coefficient를 학습을 통해 정하는 방식입니다.
                    </p>
                    <div class="equation">
                        \[ argmin_{\alpha \in \mathbb{R}^k, \beta \in \mathbb{R}} \sum^{n}_{j=1} l \Big(\beta \cdot f(x_j, \sum^k_{i=1} \alpha_i \theta_i), y_j \Big) \]
                    </div>
                    <p>
                        <br><span class="highlight" style="color: rgb(0, 3, 206);">하지만 이 방법론은 n개의 모델을 모두 메모리에 올려야하기 때문에 현재 나오는 거대 모델들에게 적용하기에는 한계가 있어서 저자들은 논문에서 greedy soup를 중점적으로 다룹니다.</span>
                    </p>
                    




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>Model Soups의 결과</span><br>
                        <span>Results of Model Soups</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        이제 실제로 위에서 소개한 merging 기법들을 적용했을 때의 결과를 보여드리겠습니다.

                        <br>아래는 ViT-B/32 모델을 fine-tuning한 결과와 merging의 결과입니다.
                        저자들은 greedy soup을 적용했을 때 72개의 fine-tuned 모델 중 5개가 merging 되었다고 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림을 봤을 때 x축의 ImageNet 결과와 y축의 ImageNet의 distribution shifted 된 5개의 데이터의 평균 결과가 두루두루 좋은 것이 greedy soup인 것을 볼 수 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="" style="width: 80%;">
                        <p class="caption">ViT-B/32 merging 결과, 출처: Model Soups 논문</p>
                    </div>
                    <p>
                        <br>아래는 ALIGN 모델을 fine-tuning한 결과와 merging의 결과입니다.
                        저자들은 greedy soup을 적용했을 때 12개의 fine-tuned 모델 중 5개가 merging 되었다고 합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 그림을 봤을 때 x축의 ImageNet 결과와 y축의 ImageNet의 distribution shifted 된 5개의 데이터의 평균 결과 모두 greedy soup이 높은 것을 볼 수 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="" style="width: 80%;">
                        <p class="caption">ALIGN merging 결과, 출처: Model Soups 논문</p>
                    </div>
                    <p>
                        <br>아래는 ViT/G-14 모델에 대한 결과이고 greedy soup을 사용했을 때 58개의 모델 중 14개가 merging 되었다고 합니다.
                        실제로 greedy soup 결과가 ImageNet과 distribution shifted ImageNet 데이터에서 가장 좋은 결과를 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="" style="width: 100%;">
                        <p class="caption">ViT/G-14 merging 결과, 출처: Model Soups 논문</p>
                    </div>
                    <p>
                        <br>마지막으로 BERT와 T5의 text classification 결과인데, GLUE benchmark의 4개의 데이터셋을 fine-tuning한 후 greedy soup으로 merging 했을 때 두 모델 모두 기존 fine-tuning 한 모델보다 좋은 결과를 보여줍니다.
                    </p>
                    <div class="contentImg">
                        <img src="" style="width: 100%;">
                        <p class="caption">BERT, T5 merging 결과, 출처: Model Soups 논문</p>
                    </div>





                    
                    <p>
                        <br><br><br>이번에는 model merging 기법 중 model soups를 소개했습니다.
                        다음에는 실제로 LLM leader board에서 1등을 차지한 기법 중 하나인 LLM TIES merging 기법에 대해 소개하겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#ModelSoups
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('Deep Model Fusion 시리즈 첫 게시물 입니다.\n\nThis is the first post of Deep Model Fusion series.');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('ties1.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>TIES-merging (TrIm, Elect Sign &amp; Merge)</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>