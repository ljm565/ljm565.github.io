<!DOCTYPE html>
<html>
    <head>
        <title>Bidirectional Encoder Representations from Transformers (BERT)</title>
        <meta name="description" content="강력한 성능의 언어 모델 중 하나인 BERT에 대해 설명합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/bert1.html" />
        <meta property="og:title" content="Bidirectional Encoder Representations from Transformers (BERT)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="강력한 성능의 언어 모델 중 하나인 BERT에 대해 설명합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/ALs6j_Fj2hVRO_BImgetjC6L3e1T8s3U9wrmSqFxOmiob5oN6KLv3cSHIX6ViXSwpGegyaZWP3BfhZkG-qmPD--8a_0troGJaXx4M6RY6YKv8icoPRxAsB_VwHtF5mJoUdPj-RjMcvqtDh2KCytWWMmH5YB2GZF0ovHF4EU81E1tnA1yPluzQmWrk-hFy540fAdYxmmmCDLF9ub_WSyqpchLuI3cq3CuAxOI48iCa2xrY-ni4hAgICWD2KxsI-qlnYazKcrGFmryqU7axwMXKmoZlv6euXXvW1Nvvjr_1pjnOI-ExeRFk5tzh2k9vYxgNm0JfRIc8DAe-5f4Byhy15eSsxxfUEQRCOcR4JSzo8DjEbnq9ppnfeSbOVOZrtzjJk34s5RM7axvARV0nI_uN1_SGo5SAKakF5gttZOt1kNT951fQotEKIWaqDE6k6ey7dyjHTMAtpvb7Dj8DPnCXxdi5QlQiKJSpGpvO1RpTro9cmaLXEoR8CPb1HPCGUTQYZrc3Y9ZcGuYmKFNeXm5r9eCnijwjBOQGwZeF13d2t0y68t-Im4iarEY-ZGO57Ye52pIu8AXiHpqSLw7ow9rg-dwLpyufSMcalXiWR-oSf3CHTiFhxc2r0o8f-TAw6o7zDV8yuj_mcwbFXUVHNxti1fL_1S_4gZjH-ICwaxJ7cVgerT5jFSrdEfuZgXHu9SvNNi4QPMVjMuXNLruSKGivt3wTUkP29suZDphOUswM9HukhmBpsh3Z-2hbfTXxTDuk1CgQ0NJIjtcVTEl_axOl7J8f9jPjLEXbHBDYod8bL6-p6EgxmXcVuRTNw-6WFGTdVf6gMGBV3rwXx3uCyTSSddWv5thPAvq68AvQ6LzjCsssaLz_TmaNGQveA6L3HmhV9Z_pgMLIX8SHa7ic7myA3EI77f-CW60fPMsfaaGGkHP45p-xXHpdnECg_yH2EcUKqsoRXORmX2iDnKnWLLvbfFzKAMZeaGdyVEnyjFAlJJM9YzSIIkmyuY-ieiiIRA6bBCn9_L24-MhpVxre4jkvNrA5ME86mSkplmbxxcBwtDtow1L8ipgJMZknK-3aEusj8YST0tdbdC3MO37JW81o-BU91hp1RdgizAYziyCbONoWGt1ckdvo51xcm799liMujunppuM3DU7tDY8WsPxngE6NElPcuHj5uZA2R484-5HCK8Z7FrsQVDTGTQx_OSkX__xrGTp-Gut8_-Uku4fINNcUt322WStl5XQYzJ8Zu_J5TCepd90yC_3Rkx3nZ29SXdJ-edPVk9H2FlARk8QMtc4qv8EmoQ9A6q_2jh36vp3JpWQI085Jl9t1fDAgVSEAzDA37jOy9mQqx6DeT0vhMrhIu3LBMpwWKESH0obNiLqrIaUYB1jqnogULpoYcL-yPHHIB5aptahmPKzoTJaV46Cx7AMWya9DvIQEPXBvGXki6RRkrMM-c7g_J-dn2S-dii_EbsITXYmCFeDcBaO2grbLkHRThhyhB9MWiDakDeu4TMJsqk_5wtfYPTdMmBtZUiKoBjJ-J67hJkG" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Bidirectional Encoder Representations from Transformers (BERT) / 1. Bidirectional Encoder Representations from Transformers (BERT)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/ALs6j_Fj2hVRO_BImgetjC6L3e1T8s3U9wrmSqFxOmiob5oN6KLv3cSHIX6ViXSwpGegyaZWP3BfhZkG-qmPD--8a_0troGJaXx4M6RY6YKv8icoPRxAsB_VwHtF5mJoUdPj-RjMcvqtDh2KCytWWMmH5YB2GZF0ovHF4EU81E1tnA1yPluzQmWrk-hFy540fAdYxmmmCDLF9ub_WSyqpchLuI3cq3CuAxOI48iCa2xrY-ni4hAgICWD2KxsI-qlnYazKcrGFmryqU7axwMXKmoZlv6euXXvW1Nvvjr_1pjnOI-ExeRFk5tzh2k9vYxgNm0JfRIc8DAe-5f4Byhy15eSsxxfUEQRCOcR4JSzo8DjEbnq9ppnfeSbOVOZrtzjJk34s5RM7axvARV0nI_uN1_SGo5SAKakF5gttZOt1kNT951fQotEKIWaqDE6k6ey7dyjHTMAtpvb7Dj8DPnCXxdi5QlQiKJSpGpvO1RpTro9cmaLXEoR8CPb1HPCGUTQYZrc3Y9ZcGuYmKFNeXm5r9eCnijwjBOQGwZeF13d2t0y68t-Im4iarEY-ZGO57Ye52pIu8AXiHpqSLw7ow9rg-dwLpyufSMcalXiWR-oSf3CHTiFhxc2r0o8f-TAw6o7zDV8yuj_mcwbFXUVHNxti1fL_1S_4gZjH-ICwaxJ7cVgerT5jFSrdEfuZgXHu9SvNNi4QPMVjMuXNLruSKGivt3wTUkP29suZDphOUswM9HukhmBpsh3Z-2hbfTXxTDuk1CgQ0NJIjtcVTEl_axOl7J8f9jPjLEXbHBDYod8bL6-p6EgxmXcVuRTNw-6WFGTdVf6gMGBV3rwXx3uCyTSSddWv5thPAvq68AvQ6LzjCsssaLz_TmaNGQveA6L3HmhV9Z_pgMLIX8SHa7ic7myA3EI77f-CW60fPMsfaaGGkHP45p-xXHpdnECg_yH2EcUKqsoRXORmX2iDnKnWLLvbfFzKAMZeaGdyVEnyjFAlJJM9YzSIIkmyuY-ieiiIRA6bBCn9_L24-MhpVxre4jkvNrA5ME86mSkplmbxxcBwtDtow1L8ipgJMZknK-3aEusj8YST0tdbdC3MO37JW81o-BU91hp1RdgizAYziyCbONoWGt1ckdvo51xcm799liMujunppuM3DU7tDY8WsPxngE6NElPcuHj5uZA2R484-5HCK8Z7FrsQVDTGTQx_OSkX__xrGTp-Gut8_-Uku4fINNcUt322WStl5XQYzJ8Zu_J5TCepd90yC_3Rkx3nZ29SXdJ-edPVk9H2FlARk8QMtc4qv8EmoQ9A6q_2jh36vp3JpWQI085Jl9t1fDAgVSEAzDA37jOy9mQqx6DeT0vhMrhIu3LBMpwWKESH0obNiLqrIaUYB1jqnogULpoYcL-yPHHIB5aptahmPKzoTJaV46Cx7AMWya9DvIQEPXBvGXki6RRkrMM-c7g_J-dn2S-dii_EbsITXYmCFeDcBaO2grbLkHRThhyhB9MWiDakDeu4TMJsqk_5wtfYPTdMmBtZUiKoBjJ-J67hJkG);">
                    <div>
                        <span class="mainTitle">Bidirectional Encoder Representations from Transformers (BERT)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2022.12.20</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번에 소개할 논문은 바로 NAACL 소개되었던 Bidirectional Encoder Representations from Transformers (BERT) 입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">현시점으로 무려 6만회 가까이 인용되었습니다.
                        과언이 아니라 BERT같은 경우 현재 연구하는 데 있어서 없으면 안 될 모델이며, BERT는 summaraiztion, 기계 번역, 감성 분류 등 현재까지도 다양한 연구를 위해 transfer learning하여 많이 사용되고 있습니다.</span>

                        <br><br>BERT는 pre-training 후, fine-tuning을 통해 NLP task의 성능 향상을 위해 대량의 언어 데이터를 통해 인간의 언어가 가지고 있는 특성을 학습한 모델입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이렇게 text가 가지고 있는 정보와 특성을 잘 파악하는 방향으로 학습하기 위해 저자는 Next Sentence Prediction (NSP), Maksed Language Modeling (MLM) 기법을 소개하였습니다.</span>

                        <br><br>아래는 BERT 논문 링크입니다.
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">BERT 논문</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>BERT가 나오게 된 계기</li>
                            <ul>
                                <li>ELMo</li>
                                <li>GPT-1</li>
                            </ul>
                            <li>BERT 구조</li>
                            <ul>
                                <li>Encoder 구조</li>
                                <li>Embeddings</li>
                            </ul>
                            <li>BERT 학습</li>
                            <ul>
                                <li>Tokenizer</li>
                                <li>Input 데이터 형식</li>
                                <li>Next Sentence Prediction (NSP)</li>
                                <li>Masked Language Modeling (MLM)</li>
                            </ul>
                            <li>BERT 성능 및 Fine-tuning</li>
                            <ul>
                                <li>General Language Understanding Evaluation (GLUE)</li>
                                <li>The Stanford Question Answering Dataset (SQuAD)</li>
                                <li>The Situations With Adversarial Generations (SWAG)</li>
                                <li>Named Entity Recognition (NER)</li>
                            </ul>
                            <li>BERT Ablation Study</li>
                            <ul>
                                <li>NSP와 MLM의 성능</li>
                                <li>모델 크기</li>
                            </ul>
                            
                        </ol>
                    </p>



                    <h1 class="subHead">BERT</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>BERT가 나오게 된 계기</span><br>
                        <span>Motivation of BERT</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        BERT가 나올 당시 유명한 언어 모델이 있었습니다.
                        <ul>
                            <li><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">Embeddings from Language Models (ELMo)</span></a></li>
                            <li><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">Generative Pre-trained Transformer 1 (GPT-1)</span></a></li>
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FROreMPpwAAWByhLiYHlH2WkOEzJZVkCf-j2hjg5EQ-xHThQWJZ_8QEnYVMryaY4oc5afx2U1S7l8JCJ1fqV_gD-ob_c30pQLV-8EbG4lSoERErM-eKNtmem4nPaiCAI3RLT4Z0t-bwp88hvIEcrzTGKTVFm1Eqkg92YZxIX_7nnicdBJB1bb7uYg6NRq_uCmHo7M7schx8T2bNZBOKhCfYDM-oaUpRLdsKyQe_GFkeTQPvmQdrX9osRCSBOc-UbOQztB6kzj2mnChRsE9z_7_iVWY0TlppE2MR-P8NiqbYcsFgj0ZkttaL50Av7yhDGR295xfIqknQyeKsnUEvc96TsMNJCCNPNdaELkwhvGCdqxcB3iDvmZc-1dnTSiE9051Yr_imjkpfFm3HzfxqDaYfDMeeX8LxnkgNsL6Ma4N_E2UST5dGuhD6sk465d5gVfQ677npGALzmdBSiZbqzM1UHYoQTrhnZJHh5WrzGQ8eWUlcexXX1XRtnG7Qml3Z7GrRAnNjGc4sjaA4e_i9MbM3hCfHWG5OxUpQsvj2rM-0l13SUUu-Teyim99G5-Q96a4k6_qQMnzdR8HkawvDGy-P0czKNmd6o5FL_Ahh0pvhTiTI4H0NG0sjdq-PHw46Z4BLEJ1CEyPK8uZ8fO35jOcSqrLsgxgFw70b18N38RR2jTOGJvNzzlGc5RRiW7Hc6NhHNeDrB8rryVyXcHBriXaLtJyxCm5K0lDFt59GvGo7sx5vqA4Ma2QS-15EWHbzWqISl79u_q_ZThFEzakbwfTDvIJ5dP3olcN0Q94MnHPamOnYts6Laqxu8uWKAGBldWTbAjSSwuhVuU8V0kartAVeFPhib6fzI6fWRI6DkVtuFm9l-rZfgKOI5iseYir1M4hRIer07lHqFFX2jyF2-Il0oLzJ9fRH_hCmcf67HDO4CAXVLvvuekXNwVEpv1fU-aBuL5bTLo9VmBrsMgPnRWrGle7VJo-uuzh8xKSenXi23OM2kFnHlxghG5xFbBhOvogdOU-Q6LJ8aqB4q2fwNjpe3iY0s3iG2946T4-B8HkqTMbMuB1A9PeU2FYXuB9cyw5wH1ZS-H_t_G3sm6VeWuCx9q7CyvEhsCwiFgFw2SBHrIvh3c48BW7CGMXJUv68Is-rcFHhcOBMF6GuQ1CpHbx9LY_qkhbPe_nYnYgir0jK4MPMPO3oAjTDZgcJD_KeAygGwK6cCsv179dNehwvtxSiAmv-HGnpHQIgvqrYqco-iS9Ht750wrjDFsalXEKwyCLiUHo1ZSZw8KJ5MjLSwJIqUq6mgdu88LgUoDkt92K6oD8_sS_Z6fi8zo7ffwfBZd7C88NhjcJ0GMEWvYzE9_PwbKNprz3401opu4leMljJ7_PnSF34g-zHlTf_V_jsAlfrf3LkYVecUl-7NoecAtccUVNL2lH6P7liYHgSjARwByEDwYzKYYCD4CncYQui1zt0pnpAM_7VEmVWIOqmwO_W472fqPHZPZm7nEXL5PN0TTdOfHkQHu9YcwmBwMJE6XTQDJcLEou" style="width: 100%;">
                        <p class="caption">ELMo와 GPT-1 구조, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>1. ELMo</b></span>
                        <br>ELMo는 bi-LSTM을 바탕으로 텍스트를 학습하고, 학습이 끝난 후 파라미터를 고정하여 input 문장에 대해 representation하는 모델입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">쉽게 말해서 input 문장의 토큰을 임베딩해주는 레이어라고 생각하면 됩니다.</span>
                        따라서 NLP task를 수행하기 위해 pre-trained ELMo의 파라미터를 고정하여 문장을 임베딩하는 데 사용하거나 fine-tuning을 진행하여 사용합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이러한 ELMo는 단점이 있는데, downstream task를 진행하기 위해 무조건 ELMo 모델 위에 task를 위한 모델을 하나 더 붙어야한다는 점입니다.</span>
                        즉 NLP task를 위해 ELMo를 포함하여 총 2개의 모델이 필요한 것이죠.
                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">따라서 BERT 논문에서는 ELMo를 feature-based approach라고 설명합니다.</span>
                        더 자세한 내용은 <a onclick="pjaxPage('elmo1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.



                        <br><br><br><span style="font-size: 20px;"><b>2. GPT-1</b></span>
                        <br>GPT-1은 앞의 토큰이 미래 토큰, 즉 이후 토큰을 참조할 수 없도록 학습한 언어 모델입니다.
                        즉 일반적인 decoder와 left-to-right로 학습이 진행되는 unidirectional 구조를 가진 모델이죠.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이러한 방식을 BERT 논문에서는 fine-tuning approach라고 설명합니다.</span>
                        더 자세한 내용은 <a onclick="pjaxPage('gpt1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.
                        
                        
                        
                        <br><br><br>특히 BERT는 위의 언어 모델의 단점을 언급합니다.
                        <ol>
                            <li>ELMo는 downstream task 시, 추가 모델을 붙여야함.</li>
                            <li>위 두 모델 모두 pre-training을 진행할 때 일반적인 언어 모델 학습 방법인 단 방향의 unidirectional한 방법으로 수행한다(<span class="highlight" style="color: rgb(0, 3, 206);">ELMo는 bidirectional LSTM이긴 하지만 학습 loss function은 결국 left-to-right, rigth-to-left의 단일 방향의 loss를 합하여 수행</span>).</li>
                            <li>이러한 unidirectional한 구조 및 학습 방법은 언어 모델의 성능을 제한한다.</li>
                        </ol>
                        
                        <br>따라서 BERT는 <span class="highlight" style="color: rgb(0, 3, 206);">downstream task를 용이하게 하기 위한점도 고려하면서</span>, 위의 문제점을 해결하기 위해 <span class="highlight" style="color: rgb(0, 3, 206);">ELMo의 장점인 양방향성, GPT-1의 장점인 transformer</span>를 가져와서 새로운 방식으로 언어 모델을 학습하게 됩니다.

                    </p>
                    
         


                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>BERT 구조</span><br>
                        <span>BERT Architecture</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>1. Encoder 구조</b></span>
                        <br>BERT 구조는 transformer의 encoder를 사용합니다.
                        Transformer의 encoder는 decoder와 다르게 input token을 attention을 통해 모두 참조할 수 있습니다.
                        따라서 양방향(bidirectional)으로 input text를 볼 수 있다는 것이지요(Transformer 설명은 <a onclick="pjaxPage('transformer1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a> 참고).
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 encoder를 사용해서 text의 bidirectional을 다 살펴보고, 각 토큰이 다른 모든 토큰을 참조하면서 모델이 text representation을 더 잘 할 수 있다고 생각하고 저자는 encoder 구조를 선택한 것입니다.</span>
                        그리고 encoder를 사용함으로써 bidirectional LSTM보다 연산 속도가 빠르고 깊게 쌓아도 성능이 좋아진다는 장점도 있습니다.
                        아래는 BERT의 구조입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Fj2hVRO_BImgetjC6L3e1T8s3U9wrmSqFxOmiob5oN6KLv3cSHIX6ViXSwpGegyaZWP3BfhZkG-qmPD--8a_0troGJaXx4M6RY6YKv8icoPRxAsB_VwHtF5mJoUdPj-RjMcvqtDh2KCytWWMmH5YB2GZF0ovHF4EU81E1tnA1yPluzQmWrk-hFy540fAdYxmmmCDLF9ub_WSyqpchLuI3cq3CuAxOI48iCa2xrY-ni4hAgICWD2KxsI-qlnYazKcrGFmryqU7axwMXKmoZlv6euXXvW1Nvvjr_1pjnOI-ExeRFk5tzh2k9vYxgNm0JfRIc8DAe-5f4Byhy15eSsxxfUEQRCOcR4JSzo8DjEbnq9ppnfeSbOVOZrtzjJk34s5RM7axvARV0nI_uN1_SGo5SAKakF5gttZOt1kNT951fQotEKIWaqDE6k6ey7dyjHTMAtpvb7Dj8DPnCXxdi5QlQiKJSpGpvO1RpTro9cmaLXEoR8CPb1HPCGUTQYZrc3Y9ZcGuYmKFNeXm5r9eCnijwjBOQGwZeF13d2t0y68t-Im4iarEY-ZGO57Ye52pIu8AXiHpqSLw7ow9rg-dwLpyufSMcalXiWR-oSf3CHTiFhxc2r0o8f-TAw6o7zDV8yuj_mcwbFXUVHNxti1fL_1S_4gZjH-ICwaxJ7cVgerT5jFSrdEfuZgXHu9SvNNi4QPMVjMuXNLruSKGivt3wTUkP29suZDphOUswM9HukhmBpsh3Z-2hbfTXxTDuk1CgQ0NJIjtcVTEl_axOl7J8f9jPjLEXbHBDYod8bL6-p6EgxmXcVuRTNw-6WFGTdVf6gMGBV3rwXx3uCyTSSddWv5thPAvq68AvQ6LzjCsssaLz_TmaNGQveA6L3HmhV9Z_pgMLIX8SHa7ic7myA3EI77f-CW60fPMsfaaGGkHP45p-xXHpdnECg_yH2EcUKqsoRXORmX2iDnKnWLLvbfFzKAMZeaGdyVEnyjFAlJJM9YzSIIkmyuY-ieiiIRA6bBCn9_L24-MhpVxre4jkvNrA5ME86mSkplmbxxcBwtDtow1L8ipgJMZknK-3aEusj8YST0tdbdC3MO37JW81o-BU91hp1RdgizAYziyCbONoWGt1ckdvo51xcm799liMujunppuM3DU7tDY8WsPxngE6NElPcuHj5uZA2R484-5HCK8Z7FrsQVDTGTQx_OSkX__xrGTp-Gut8_-Uku4fINNcUt322WStl5XQYzJ8Zu_J5TCepd90yC_3Rkx3nZ29SXdJ-edPVk9H2FlARk8QMtc4qv8EmoQ9A6q_2jh36vp3JpWQI085Jl9t1fDAgVSEAzDA37jOy9mQqx6DeT0vhMrhIu3LBMpwWKESH0obNiLqrIaUYB1jqnogULpoYcL-yPHHIB5aptahmPKzoTJaV46Cx7AMWya9DvIQEPXBvGXki6RRkrMM-c7g_J-dn2S-dii_EbsITXYmCFeDcBaO2grbLkHRThhyhB9MWiDakDeu4TMJsqk_5wtfYPTdMmBtZUiKoBjJ-J67hJkG" style="width: 100%;">
                        <p class="caption">BERT의 구조</p>
                    </div>
                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>2. Embeddings</b></span>
                        <br>이제 BERT가 왜 encoder 구조를 사용하였는지 알아봤으니, 위 그림에서 embedding 부분을 살펴보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">먼저 embedding을 살펴보기 전, BERT는 대량의 text를 pre-training하기 위해 두 문장을 한 쌍으로하는 데이터가 들어가는 것을 염두하여야 합니다.
                        그 이유는 아래 BERT의 학습 부분에서 설명하고 있으며, 여기서는 두 문장이 들어가는 것만 염두하시기 바랍니다.</span>

                        <br><br>먼저 BERT의 embedding은 3가지 종류가 있습니다.
                        <ul>
                            <li>Token Embedding</li>
                            <li>Segment Embedding</li>
                            <li>Positional Embedding</li>
                        </ul>
                        먼저 아래 그림에 보면 두 가지 문장이 들어갑니다(현재는 [CLS], [SEP] 토큰은 무시하고 보면 됩니다).
                        그리고 세 가지의 임베딩을 거쳐서 최종 임베딩 된 값을 사용합니다.

                        <br><br><b>Token embedding</b>은 말 그대로 각 토큰에 대해 임베딩을 하는 것입니다. 즉 임베딩 lookup table이 vocab size 만큼 있는 것이지요.

                        <br><br><b>Positional embedding</b>은 말 그대로 위치에 대해 임베딩을 하는 것입니다. 즉 임베딩 lookup table이 max length 만큼 있는 것이지요.
                        예를 들어 input max length가 16이라고 하면 0, 1, ..., 15의 임베딩 값이 존재하는 것이지요.

                        <br><br><b>Segment embedding</b>은 이전에 BERT의 input은 두 문장이 들어간다고 했는데, 문장을 식별하기 위해 각 문장에 할당해주는 임베딩 값입니다.
                        아래 그림을 보면 \(E_{A}\)와 \(E_{B}\) 두 종류가 있는 것을 볼 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 임베딩 lookup table에 0, 1의 임베딩 값이 존재하며 첫 번째 문장에는 0 임베딩 값을, 두 번째는 1의 임베딩 값을 거쳐서 더해주는 것입니다.</span>
                        또한 위처럼 2가지 임베딩이 아닌 3가지의 임베딩을 사용하기도 하는데, 0은 padding, 1은 첫 번째 문장, 2는 두 번째 문장의 임베딩을 의미하도록 하여 사용할 수도 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_GsPLq2THa9aveju4ZdNpG0Egoe9ge8Kd9HoSYwlXOu64VVIl5Q7flm9NSlwZsB7mT0QJ4RsYQbBdnTCrWAaXr5_fFB-wf_cwWU1aJXHmp9aOoFZv3LorjfIHdGthZGyfunIZYgC-ubK6L0iKUzH2kYUmwuizsgT9vUtV1jiutO2lFqQWs_oiZw1zeiK93xhpH9A4hv9yLzPmuIE5JRxGL8nO1kNUAU_wnEaAlpG-8Rs6wH0aR3qPhD3aAUOx_2WItK_UH9B8C_oa-Vt-k_j92I3n8FZZP0m1kTlCVx9IhX4IFuIbmrLenxUc9izrgJOISv8po0lG-o6s_C8Ba5rl15mRxUVE_smIg-gtn5hjdZ_Q0mHdTSWOgmWXLGjO2czk5fHFlORecVVIQg5lsjfMS1Co_Q207NMF-s4KxRSma7hnA6VlrPTXGRMfUtBre6aeUQ8V_qj_CQDRXcBvuAY-CWL0443qeXqKadNiKy6-onPieEtF02APs5ou0s9m6nG07V7wiDwc-w7BV4V-_5KsK7ynKaMmj2ityCrR_bA-GBjZlNYIfNOBjcuIgPWCJcprTpiR6mM9EBk5fqxtaazpZfFLoZ__vIWqTSNUVQWQxkoyYoGl3zIMODg-CiEeVUpaXFBYkS8NEiNDfrSxKrE8Z8LoEqAu4pbP2Zhi4HDP8YSCnOoEg4EfnsFrmrHygE1FCRwAPm7iFaQ6Iri0erDm14P0fkUs5F9Ly-cWOiAJbhIY5tSEpGTcmwgXUD3TfcfOq1FJFaJnDBhYkb76RD2UbJW29viIZIzYizQ8tA4SaMsdiBkSUAt1LrXiNQHmyDhOpPTY70zUz64DJeQpWvOtU72r_ZLX-JviReQwOJwVsobj8P3KTEkCHaKWYyKQ9MxqQZMEIx5awAVK9JhBSmRoGzrHyaF6kWK1phCbBKN-VG6xxEHwlEmkBaXB5DiLvibjIt4UTItG98QtGzVFOQDJPi4z3zO4skGmY8NqXkPAahUcwC-3fOvpam4vxsIcTdperYJhkxXY6hl6cq2gi69tRqHOS_XVy_A399pmhtmKfIyNFyX-aLbWl4ORuSPeIPw27Ya4FmeYMyuNDdJW2EVYKPDAtGHPOjLJQT2RbY2x7BEKfM38XhVO08wldOS80PRap_3ikP3JBA3Z1O4QAtzf5TodElfO4-GvoCYRf7p11YZpUmqcuy6Epw6sM-jRRNf9mg481CUjSoiPJ3UCqRq--Z2cUN0Nq_3QMgybSx6IECNAT8ZF3rt_nXMDJhWlqRGxGOHkM6PNIaO2S26V0Pvn7dATURGKL2KS-XtADxf74tdHgol8GKiacB6ETrameetTDYA6jUnSH7Lo-EdvrR5Lv1EFaV2Y3KbDSJ9KDh-cNRksvc0l-LZn3T_I-oNhI10kuHbeTPwHtqGsybTFi1Fw4D6vKuehSvZ4fEvn0uvmrhmlbRmz1b1jz7AK6CwR_HJXoLkfmL4kysb73N6WRUrzv5H1pEXs6xgqmMScsk-Rq-nUqy1VYAq4KP6B6i6RyUhG6NqJgyeyeM" style="width: 100%;">
                        <p class="caption">BERT Embeddings, 출처: BERT 논문</p>
                    </div>
                    



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>BERT 학습</span><br>
                        <span>BERT Training</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>         
                        <span style="font-size: 20px;"><b>1. Tokenizer</b></span>
                        <br>먼저 BERT는 likelihood 기반으로 BPE를 진행한 subword tokenizer인 <span class="highlight" style="color: rgb(0, 3, 206);">Wordpiece tokenizer</span>를 사용합니다.
                        BPE 기법과 Wordpiece tokenizer의 설명은 <a onclick="pjaxPage('word2vec1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.

                        <br><br><br><span style="font-size: 20px;"><b>2. Input 데이터 형식</b></span>
                        <br>위의 embedding을 설명할 때 잠깐 input으로 두 문장이 한 쌍으로 들어간다고 언급했습니다.
                        이는 아래 설명할 BERT의 학습 방법인 NSP와 MLM을 학습하기 위함이고, <span class="highlight" style="color: rgb(0, 3, 206);">단순히 토큰화 된 두 문장이 들어가는 것이 아니라 거기에 [CLS], [SEP] special token이 추가</span>됩니다.
                        
                        <br><br>다시 한 번 아래 그림의 문장을 자세히 보겠습니다.
                        아래 input 데이터는 "my dog is cute", "he likes playing"이라는 두 문장이 들어갑니다. 
                        <span class="highlight" style="color: rgb(0, 3, 206);">이때, 첫 번째 문장의 맨 앞에는 [CLS], 각 문장의 마지막에는 [SEP] 토큰이 들어가는 것을 확인할 수 있습니다(그리고 보통 input 길이를 max lenght로 맞춰주기 위해 [SEP] 토큰 뒤에 [PAD] 토큰을 넣어줍니다).</span>
                        [CLS]는 아래에서 설명할 NSP를 수행할 때 사용하는 classification을 위한 토큰이며, [SEP] 토큰은 문장의 끝에 들어가 문장을 분리하는 separation 토큰입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EaxGxE4znx8Gg32noBweGgliyxPSECwjV_U3oIBHBwzfoYA4KWE0I0rCGgjTIQ8rhTsgosdrqrN2kSW-b8lJ3L8w2Kgiki9F5BfBYNK8fAq1IzAuAXPVDAUfTTeLcKs3lnLB9V9hV8007sf4oDCzQZirkZPSNJL1UOiK54ju53fbpFOnsbgWNKvpUXBpkkSdjTEKcZrqG_XUwsFOJdK2R4K9S3DI21TdpNpbWIeAFyjGyYUveJItXBCO2fxm1ksn1FixmHcMNg65ppIDpfkzXfZLdNkKw_8S-A_c9P8TvopMffa-jQxN9_MqRTWAKOt7kfofeGEKTMqcnXT-49pWdNKyRVS9NTwQiI1K_a77aVN8ogovu7SwsWEOG3NfZoAKszAk-3b_jh49Ry5tmUU_EZ2cvoMxfR2ViyLvnujiBAWQ_71tHIKtOitSYNQFgmfoUwVU2Qk8_vzfJX2wVhAGGaYuy3kfF1XTSKTQo6r4ZwDVXs8UdZ0p7rb1ifTFxwmW_psWYy1GivGUOUrIjKa9yKqykwHTilLHmfu9hIfhG0d5KtX7WSoarcBXL4_-ZVH29VV4f7M2kzaJ-M3MVQYpdEUHebNEDbh_F7QmIfdeLQmD94wILoFSr9LWTgl1nltI1UcgSGthiAtv7cwsPT994VTKD0tcT_L4hWozLbr0syyodyFfFrm2PUudT6COXweFvR8eCBonwR0h7EeL2FpfaRCsdAzSOASknp07jAiEI23zPQ8kLHOqsOkda929gl_k6Iobdhvt0GcrgspAfaC2Q-mZ8PDzlF6cY_3RP3hFBflBsDJVHL3pJfZAAFYWuDQORFZ45JRZMcb1EhxUYX0majCdQ6JlZ8dFZ38DwK5Dj53b5M6szbobZ-Gr0iKyF57Mkf6B7ohtRDUmkC3Vsm_sw-IE3jvTPdlMm-dqstQPKtWXevSmDnkdOSrd6Ga4DVLo0xmG2FGP1oOMRUNMXLwk1iug2I3TvVXD8-aYETIzkzDDBfff_c2tnZ5oDRMOHjysqY1SG8Mu_0zoS4c6uAbjLTpc1oax3LakL4SCuvJMpt8G6luXnLc0UUvC18ndY_J0n31rHU2EtSAO9nRSxHFDkIapERDsvMww-M__-_DiFaExaWVCTus0LCAOAss5bCu-ACOilzbbOz2GII3wWVh0Qz5l6v8bY1-Krxr_8ZWv8H1VzXkyucjXJbv5aBeenUp9oUpsxUMbiub8oqS75vV_VkacwP3k6iuJJ2G4_8xQDl5NQonyL4eC_vFd7Qd2wHgeodW8e72pNohsr-KqdGW2iLyNDWEUiG6K_4iCEQZuPPDM4WokA8mVCT0aYdNwMzoARn5h-YG6nwhwM_CNOZ1MUNkX9f6yFT3hcy_GGwiQtPC8Ujc2Laxg_a_be-P6fy2U-N7UJ5tN6-S4oznlr_07XScxRpsw66PPckD58AmHE2pmEYQYskd7gPkVSeohCFjX1LitpuS2Ek4i15cjH64oDCC8TKyMfu4NDc9VQnqnXIchCMWiorEUuQx2RH0o8hP8dri0orkA" style="width: 100%;">
                        <p class="caption">BERT Input 데이터 형식</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>3. Next Sentence Prediction (NSP)</b></span>
                        <br>이제 BERT의 구조와 input data 형식을 알았으니 모델을 학습해야 합니다.
                        저자들은 모델이 text의 문맥과 함축하고 있는 의미를 잘 파악할 수 있도록 두 가지 학습 방법을 제안하는데 그중 하나가 바로 NSP입니다.

                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">아래 그림을 보면 두 문장이 들어간 후, 맨 앞의 [CLS] 토큰의 output을 바탕으로 두 문장이 이어지는 문장인지 아닌지 예측을 하도록 훈련합니다.</span>
                        예를 들어 아래 예시와 같이 "I love girl friend", "And she is pretty"인 경우 연속된 두 문장이므로 1로 예측,  "I love girl friend", "My car is broken"처럼 연속적인 문장이 아닌 경우 0으로 예측하게끔 하는 것입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 sigmoid를 사용하여 0 ~ 1 사이의 score로 예측하기보다 0, 1의 2개의 label 예측하기 위한 cross entropy loss를 많이 사용합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FSp06igmKhfjGJHM915B9umni4PXEN8k7RGmZ3EfZj2wWovSjWEpR6uifZJmQCWQSbwY49nbMrgM9DbeUvKVY1aGoN8U-N68p1eV0rovxYGxYAO-maKfL5fUl-BVj2FcGYpJKKKHrN103R8wv4GpI3tx_B_iDCPbfT-VBbuQVoFoF9ZRpN_fv0QaLyKBpcKXm03EmNmoZT2DIRqsZ25Vd4s9LaOb2NRzDIeBf1EuneiSGEH45RHYEqd7vGpalfvUuhF27yA_ulT4-tHvETXS3RplOJM4Qx8iq6V4Zp1sLVoAaXkQtb2IgnlCG0f3jLrp9xppHGTcGNHRdptHJV9Q4XMK1nX2w_SuJFSKmqmXwehK7mfc1HatKmW_VB-XEY0pt09zziqwUmlliQJBcqBgMZzsivZy-8L9DskKliACrESIccFeZ5klqFgg02Vbt6LcV6vxBlQxMoH6eCbH8i4wiOY7-DiGwNKJyvHczEJH1vJSPQveBRvqwbosqCUYSLxFbGuv2pO3aA7_CTYUxgF5YWCB1qnIwnBilw9Rqq05NeEHNLop8javqXQzMR_cOYAYY7t0P3InISjUv8ERHCtJ846iM3cHqtB3QfrPAIEBi4LbdNI1tA0Dlz7XDnN8qVmHEvqry6e5Vrw7-e7dPcigKekysYjGlow50Xavjp6blj6MMMqxiGXCbNw-LLlgfsrNXB4VhxKdEwMKucCkCIO1zpRW6hzaAjKYVqfhmTwSWjysWQ1Wkt2a2xwwFVZWiqpuQNGAOfxa9HYymhuEjaLnzVsHDmVniq-cV44XoLipQgZtow7ou6YgqIk7TllMQOhdyPOnbwfEZa3gxqA3pEjW0zIk8QRwiFAveweWoQs_6CwOs6M8alZKL83YFT6xWfWDj0Lp3DqrQZPLLG-ZzgbXJlddkXXZ3RZAsgIxV_90ArUWReS1zS6IypjI5AjV6j2aLilZRkpWItqBWeNdtlY8k0u7xI8-Gg0cqd2Yn0dDllqM5aBxbho2WFn-6f5N8UNqWlY7yZ6ItdqLNHmZyyCrE_Xgx7CZ7NALB5IGGbuf3-Pps5XosJM8kctR4iJjYibyNgniGsqam5Ak1VD8tQCkZh7MsWbok_E8NSVxXnaLAbvSIwgPY4Zw51TC1LuusyCCqhXm-NlS8dH-oL9BBJuB8BvlFpBxb_zgorqzuXIw2sTr7g8KBe1YntnczM5pFPhESRmbC-zlyeI6E5ajgCrq1WWSFD7I0aKIN6sYSu9oZvo_fba-pWTyd_owgIEUZLpSDTUSmNOvAq1KdHxdX089pWpecKFv3WFgYOp7nn-c2RhiNvd-3ZyRWjqVhCJcg80QsfP-uBp8EB2tX69Ausz78EEm693UrYonsY66CQl_ucquFPJgtFnFfQO87O3qNT1AA3MZNkS22bHZQa6NgwixJgznoReA517p6AgsbmQGoab65T3FiJNUgzlurHa5XUfwM0GSKj3Izp902VaEbJwNmdI7xN0wAMi7dbs6kQ97llKLwLJeugBFtijmt2bcw1iC6k3G-oB64u" style="width: 80%;">
                        <p class="caption">BERT의 NSP</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>4. Masked Language Modeling (MLM)</b></span>
                        <br>BERT 학습 방법 중 마지막 방법이 바로 MLM입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">MLM은 input 토큰 몇 개를 random하게 마스킹 후, 나온 결과를 통해 올바른 토큰으로 예측하도록 학습하는 방법입니다.
                        Input 길이의 15%에 해당하는 토큰을 무작위로 바꾸게 됩니다. 예를 들어 [CLS], [SEP], [PAD]의 special 토큰을 제외한 첫 번째, 두 번째 문장의 길이가 100이라고 가정하면 총 15개의 토큰을 무작위로 마스킹 합니다.</span>
                        이때 마스킹 하는 방법은 총 3가지가 있습니다.
                        <ol>
                            <li>15%에 해당하는 토큰 중 80%를 [MASK] 토큰으로 변경.</li>
                            <li>15%에 해당하는 토큰 중 10%를 랜덤 토큰으로 변경.</li>
                            <li>15%에 해당하는 토큰 중 10%를 변경하지 않음.</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">위와 같은 방법으로 마스킹을 진행하며, 1, 2번 케이스를 포함하여 3번의 변경하지 않는 케이스의 경우도 모델이 자기 자신 토큰을 예측 해야 합니다.</span>
                        아래는 위 3가지 경우를 모두 적용한 예시의 그림입니다.
                        <br><br>첫 번째는 1번의 케이스로 바뀐 모습이고, 두 번째도 friend라는 토큰에서 juice라는 랜덤 토큰으로 바뀐 모습입니다.
                        마지막으로 is 토큰은 그대로 유지 되는 것을 볼 수 있지만 MLM layer의 마지막 결과를 통해 자기 자신을 그대로 예측 해야하는 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Fy7XBT1BLuYa4zOH9UeO0cXcAZNRdU0tHBtdP9rY6EEywTufeOnR4OMoeGe5F60kO4AFho_n_GVMhB6cMS6xcnKjdyPTOpNEk8c4_Gt_b9qyREH9mT63B3d0BdkKLElwjW65ewhUpj0XmT4mfC1QRNvZ5YSrd5r-_0zF_uiVXKLacQ8tF2EzFkgOLgvcI10-Fwz5FQXFh3eLQNJMfAaRaGe3D6VRrXTuxLKQ6iwuFQ78MAQbQuQCdkBvBvkEKSZPv7_51ohcAWlbIBTppFkyAE6IWWWQ_iric50AMRa3q1rsPu-8jQW5M6f2Poc2wJhS-jNmamkfDTQMdLkbGBCUUXwIAmvCJml48grglVuJuX0bitrC44u7gdyg2WyhxiqdJHqw9R2zUA-1vW4Ks9txf4NpLo3mr62sSszAOdVpDQqHKqecfmrXoamWhuw-HiDmh9C_7eC43Cwk8DoF4jKgRRuIO9R-v-DMYTMeYiv4q2nMx0O5uRY-sT2jlOtquJc0hBdpPyO8U-HMNbVMCEcyxlPikQtPyVF_5ZK96SvFt_g3xwqAIUpfCH98lS1zvHhXMAz7Jp79evMOCLX-8Mbib8m7_CgRdQasZddPrd5U3W84OPFlbh5nPzVfMFgVjUieZaw3b5oXRXAQDMAguV-zugi_PDMDQ5pqHDLdoH9t7l7aTclGciPN89wjOuS5xCF_YbIcxY8IFStp35o-PMI8zB_Mlmn_Q4po5BZTNmnxYUuEHhaFRXZ1-s8ybmVUCKnW4HUWdarqPhZmcIYkNPG7qM63GDCuLFYw82wqDFvCakNJsGlH23YS_qQUlWHWILVqLKqEPK3xixjQZBoFzOMCOEUBtxV5MPZ9Ri_nS8hRN1eDf8XAXpBRV1Fiy3BOtnZrTXSwoqVL2rUFVoBeoPYeCD3MgGS7UYUK03YoKKnm2USVjGWXjuXjfuw276jcilJnPo7ZCgzBoIOHqdR45JToodRvesM06C2qTRgxKwIKoWfFaZTliJGV1EHxtdy7ooqwg7tAhDUCytXyfytzWeiWgw8t9dExKpQ6sCYIru0TvUS_WCP_JyPKSOvUCGHzqYPucJrX0wo36Y8c0_9jPMC0X2ziSsGXGMFKfT7Vw7lYm8F8Nm_3uBSieSXWcDb_PPjdAWXB4JsnaB8n4gjymUFZC9M7-4euh9Kv8Uks6x4Bg-uVmTZx3dH6lI_bey_INDsLRxtI3R1aYPbNu-pRZMAKxahDT6aIps7eDfejxTsl5oZyhfR1x5uvcnF_QCSmsswXLD9V374JfDAom_9yoDV7HUcyKYzUjSFwCnKJhJwFAulqYh1dU0K10koNXzW_X94JuNUS266XGLUzGgeOO8XsCbEtaUViUBdViwt4KhiJjfncophDkvwfldcxJasp5dJVb4tdiXXNyAFZtes2Cj46sJXs1dSxt6-D0ySCZ2EsDDEjXxqT9JJhNb_01G7nLZ2jaWHWCJru88oXsRaArwKwaKZnPagFB5e4ssQZH1Tonu8yiOQj1hG3faENyEK8rStUWgBdUONw" style="width: 80%;">
                        <p class="caption">BERT의 MLM</p>
                    </div>
                    <p>
                        <br>이제 NSP와 MLM을 종합해보자면 MLM은 토큰 길이의 15% 중 위의 3가지 방법으로 마스킹을 진행하고, <span class="highlight" style="color: rgb(0, 3, 206);">이는 두 문장이 연속이냐 아니냐와 상관 없이 진행됩니다</span>.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 두 문장이 이어지는 문장이 아니더라도 마스킹을 진행하는 것이지요.</span> 그리고 동시에 [CLS] 토큰을 바탕으로 두 문장이 연속인지 아닌지 학습하게 됩니다.

                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">그리고 추가적으로 2번째 MLM 케이스처럼 잘못 된 토큰으로 마스킹을 하고 모델에 넣게 되면 모델의 학습이 제대로 이루어지지 않을 것이라고 의문을 품게 됩니다.
                        하지만 저자들은 1.5%라는 적은 토큰만 랜덤으로 다른 토큰으로 마스킹하기 때문에 BERT 학습에 피해가 없다고 주장합니다.</span>
                        저는 오히려 모델 input을 perturb 함으로써 모델이 robust하게 학습이 가능하다고 생각합니다.

                        <br><br>마지막으로 위에서 소개한 방식으로 BERT는 25억 단어의 Wikipedia, 8억 단어의 BookCorpus 데이터로 pre-training을 진행하였습니다.
                    </p>
                    




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>BERT 성능 및 Fine-tuning</span><br>
                        <span>Performance and Fine-tuning of BERT</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>1. General Language Understanding Evaluation (GLUE)</b></span>
                        <br>모델이 자연어를 잘 이해하는지 평가하기 위한 benchmark 중 가장 유명한 것이 바로 GLUE입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">GLUE에는 감성 분석 (SST-2), 언어적 수용성 (CoLA), 질의 응답 (QNLI) 등 9가지의 task가 있는데 BERT는 이 모든 분야에서 state-of-the-art (SOTA)를 달성하게 됩니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HHzb9n4zJFb4IPo6WEDJL4AYPtV9AiWf1fQ2sIK9z9Rk_mViOe44o5G6FpprJ3tUONhxE7NG3hPpVfbgMx5_dHEEAe3whp-2BgfP5bi11OEouIIE6oIP_X8u_96r1CyMx0eyOnIQeHidQEwawah6GOsvcnHs6mfcR3mRVy95oZ0DGt_E_FztPSzgXEbzuIxzxkksEugpN-YAhteLqkkKIckc-oFZDhwIrQkSEktaU2EaRF3O8y01xmof31utA4BE9D9-l2WVnOy1125tV75YYvkBx_AbdQ1DsB3u8lO4-mcmUmvFzSWJVO294chn_599Ys68Bo313zE3N3s4lZWf_HAsVRZz_xASPmOySPl35SG4ldhtAn6TTwU8QPiufJt1NmkARPBcFGhq1PI6kY9JLh-GldGj1ZQhVriFZPfIhwaITXpDkgPKxD5tU0s6sFRTPi-Q0T4vjXrCwtpPKCdzlKWW4e-RtrduM0mrKYRdi-fKfiw64lAk2wFeF4AmZ7pAszf4-HM8sTKA9o36ZbrfQJQ15NJzNk9anTHATMVINfCCQon982qfPKm9YNFlHcv5qiLbtIl0x0mzQHYMtHUvAit7ubw3N_NcPr9VbENPeGfqllAeDzr9p1ivBCN7kvMIv9UGwxASMdGI2Xqminlsd3QVPc8R7i7xkLA1psQaK3csRPnbF_KqYl1jpFQ6o2XDwR5RGzLS7cGKCDlJ-dC9ctNZc7OhEqpgRu3LU8ro_LikZeKkYTA7J0vievUne80Dxg8x-1wGDwuwM6mZmeZn-RGQWiD0P8xo4Qi_bVXgb-_l1Wnpd7VVjfoOoZcgSBRlbw5dNNUbOyXdVM89p_sxgh0vxcaG9pnOJZGIHK4KP7Iij2rBaK-TPqX6Kddf8Afmj_KUH1vEmbGpy3mIOIUd0sBlZDi9ItQ2r9mLNxM91bQQGM7vX5OI07CzArqgtm00KfRAj_psk_uZ_IsRt9xymxMNjBeC8UZl8ZmRg_91iKuF1TcWEkIkBRy7NEx8En2_tNxn2jcGgGX-UGIv_408TeQcm82HaPapC1_j3QqlnwUqB5A2AUzx-pOyEJSk1UwqFDpETIsyZNnMYZzNbcA2Sl-Jfjupc5Sy7l9UgZkL1Chq-kFC1qEMNHJu551giiLAxwCz0y6X-RFWAhod6pF7cKDfEZTIznUqZE2BDdY6cYaDhSgbWwSAz6dLoeHWYRsoZiqtufQCML4GbvG8jekexwDmizzShw0F-HVyePbaw1Jrqvrr1-fg4ZHWaX8hOpdgJc9jAlAPS5_sB34kH8zhbKOU0PWcY8LReTXdI4tkUDcIV0c-u3dDYrBmMZyfmS95gBDuzKBrCim2rbxzXrdVZQGVpQlizxPRCWt-rpkhMPChghphblknj6pb_CNaH82WNn2n1b7fyz3tYDy6N9-1Wuk6Ircix6QXWKESYTyWAqQKPCHBZVA5dTLiOuRX1aeHOihQmoT3lkZKg8bCmez4dEJ3E04NLGMffiiULL9X6v2nR_Oe5JN2ne9s_idO_UWkS9GBwbEC7D" style="width: 100%;">
                        <p class="caption">BERT의 GLUE 결과, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>2. The Standford Question Answering Dataset (SQuAD)</b></span>
                        <br>GLUE뿐만 아니라 저자들은 BERT를 Question Answering task를 위해 SQuAD 데이터에 대해 fine-tuning을 수행했습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">먼저 질문을 첫 번째 문장, 그리고 정답이 중간에 섞여있는 지문을 두 번째 문장으로 하여 BERT에 넣습니다.
                        이때 두 번째 문장에서 첫 번째 문장 질문의 답에 해당되는 첫 번째와 끝 위치를 찾는 task로 변환하여 fine-tuning을 실시합니다.</span> 
                        즉 질문에 대한 답의 시작과 끝 위치를 찾는 것이지요.

                        <br><br>BERT는 이러한 SQuAD 데이터에 대해서도 SOTA를 달성하게 됩니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_H7NZ3_XuIkf9OXucGMvQkibDaXEKFUU5OOwX-HecyEqc-_oz_c6CY0YodefLxPAA-MBteHiv_LVavKg_QjdLrtq3kjYSIF2n7dvDkido0tYyDNmZsr6F3YtCGwYW0qxFHsv7VpWpexjwdwJXsQ1B0GajHSUZ9AuOsV99IcsEjmTP4pIA_8wMRBSs7XamDr7Jj7zRULMbg1ykcOMIeu7j8jvHJeo0_WWYmzZrs2-8kP_4Yd-YWYKR5IOAQfZabDQD9y8NGOXxspvoyzyQlUFggrqsibF95iroKXG_JXeSHTTALy4pFAInIhQxsBkoEZ4q93QSU4069a2lCfe68TTnaEJb9x_ezt5-UtrVOm5YCDWVhVIGlK1CY4sXvkDMG317ooUWF-qijkXe7pdpEzwjunhR9ZTAgsUMQI-T3FlHvjJfpoPq269VU18FdWvVEN6da_kdkCx0823-3Di4a6Jtm4bL6eUoi3DwtubC4OSA8D4TcfOR-yO-F6hej3QpwropBLgSW0qbpllWiMGetJthK4i4S1ZCd67U0oOzRCopxr5UInpL-GgR08wlVTbrltCjEsRcUFDy_Hk8FQfrN7IyZo16S2aDRDkK4eMCzmbomncZo7ofSR9IMLO76zMKOrBYedBktN8TsbvZauO1PR_hSk3BUuK7afiBOoG6irQZubXBGhixZioXpziuz19sJ1jCPBrzwRErdVYqVJ46QLmd5jb545FqsifNEUyV-R9uQiwSyj4g6GGBptJ-AoKj5c60IeLnGufD1ylWrsP2bnudmG-5XlniL4MOM_m0gScp2-rs-25ac0PEnBxSfc_cfVvV5l0OyquuzkweZN_RD1pJ4ReJ0uYvJn26NteOzUerD9Vc5OUECr5gTKPgCV8bCk_3smbowjdL8vtBLvIbpVdAMqbCxvfq19ONXlXg-hz5bM4XPMq4AiOlDI5Wil6TuBfKa7He0xUtS83_TgFjtFLhpRXrZeKrRpDb8pvNQ2Ngh4xOSlASbh3ItqmZN-nLmZeeUZuhO4vhyk5yETycfnpai6qf2Nth1kwkRKpnBZ_MpP0-r_i1i72AndZaupnHWEjivQrnrHKjGhkEDmCRWt_XgbukoIWzDkqDKguAHWqa9J94hNFvIDmIDLKnk0bAZiNWXholEM7lqj_NBdtRJP3Y1dOD6THSUshI_OGYloizRKlVBxm1E82mxw8DSy_exIMWmbsxIDVmHvYOr1KtSSC9zN4ZP1hdm5dxQi0uouWHuGQ8BG1ZEhN1jUwelLbgLUFk8K0whUHU9kE1HhGD5cK4GMA82dkugHWCKxAGzTfQIW1eJqQL08aSubhv4AOV3X6Sb6EI4hJMLqzXhqbzms-VrDADAux4BYHzdGQGc2GTBL5CuVYDc9hjMuM2YlZMBgSjdyoxUxqvhKVduCDkEYCYPjgdpgzCSYnd91mU-ADR-77x9yxIYrCwJMwRYSCdaOke1Esh1kfYX1y82CYXNin1TNHJJkFvWf9K9zkvzQJM5kwleiYeZUNln1hmbuORFZd27Tk9sfIQ" style="width: 80%;">
                        <p class="caption">BERT의 Question Answering 방법, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>3. The Situations With Adversarial Generations (SWAG)</b></span>
                        <br>SWAG 데이터는 주어진 문장에 대해 보기 문장 중 가장 적절한 문장을 추론하는 task입니다.
                        보기는 총 4종류의 문장이 있으며, 주어진 문장에 대해 4종류 보기 중 가장 적절한 문장을 고르는 것이죠.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이때 주어진 문장에 대해 4종류의 보기를 모두 concatenate 한 후 BERT에 넣고 fine-tuning을 실시합니다. 그리고 [CLS] 토큰으로 나온 결과에 softmax를 적용하여 점수를 계산하고 정답을 유추합니다.</span>
                        이때 BERT는 SOTA를 달성하였으며, 사람보다 더 좋은 성능을 내었습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Engyxnji5ydn4jyt5bWfSyROX5awdyKISZXYMN3C_D13-yUDYXTtBkWZDFsR4oSiyHqTRrVEUrSibBH25UHwVFtAe7DoTKrH-DuD8W2-LW4Bdva_k_tOpcjITp-oeEU6LhAhdRcYMwH3LeNdA8mFAPvLv7JOrpcEpPmOHlAs658lbObrMxMPyUuiVkl_H_Wksxcsjc7kIUXoGOxzLgey3FDzPeFiD1cvAd3g3NBNzHoH_mdI5YuS3n7Vv155ziw0XC5SwjgBYgXYyBFS9cMbExCCSfSkBHkiZqf6LGtx28cJ0jkETn-8PELW71CKNS7Ir1-dJ7HLMuYxNgtp1LDOJhjjcsorN-vQlMobIaNE6XkIak7AlJy0lGkGw9THl5kPgx3iOtlf5dwzUz6-Q1VJM0F86yp-12OY8kAsftpjKoGbB4s_dUBQhWBJHDpnrpAzFyFgr8_qPkscfGELXRuJdKkNVNnkH4VeTiF0YmmjcXrGFfFK91lSkyTZWpkweKeVgnCInKGh-brG4Gv59XFZPxqH55jLqXo7HSY_oDihMbNqoIXdEPJs6He0dpvSTHSR392TdDOQOdJ63xAZPmN3iIjivY1sYb1-UwOtLTX2_jaCmVGivqMMaSpBd0sBj_xbOoQUcsZu7smIqlMqW-oY3_n--v8XyOAUBvvLZHmCCbJcMO6rxKiDsI8fyScnxe-f5-_m6GUYsPTQgTXbZ4z8S_pdAknBmTtsNC8ilOGKMR9xDh3qCytmwaWj8YbjsKSXuuGaMsGmSmgvvrTW_TzI2bmFWcNyM1X3HQrEHVQHravyZTtmW-73i4zaumWdtMinH4UfJt7D3hr62NHBnkkTHnqWkbr6SpeI9kveCJjOjN-qJ5d03rzgKwmojCzlts3OH7W4Yi1F4uWX7bK5ZcKmEKiEw-WpXQfVHOs1yt4JqaF6JqAFq4fYvsZjz6lIi2cIAkANtJmDF_nYI-FiYrabQEVXg8BTjZZ5Sfl38oadFLw9_-2ORtc6KRU29bu40uULZwQh1_knjuSRYRNl2guQ3AMbc42mpBrJovFKmooU29cktRHli_JgfNChAdrwZlDRED7LGX8velbrBn7s5uMnyO9hw-QKZQLo0aPbJJmaLXvtyUmq09iml7jJ81svRh27dZWHNy2uy2klNpcvEJaCgpiSbX_VTuv1yHWi2AYK-l5W3QKd6rO23mlOdrRwv0cOZLbDmI5Zui2yq7A3hoipv3o_eRbIC2zS1B7gK2FDMN1Si6gxukIsZR5yiep7VrADxGZuk6fx3rCoGhUKTtEUBhH8fFIK9x3_VcoUDOD2kj0hhi3FR7aU5TkXEfpRLFQ3X-n55mNHjlMVSD-FFvRh9QQqmX_rBOVJhMc1C7fZHRlPrzKUZmu1lRVSFzatqKRgG2yXN2-nV5k0rStSsAYaCduxV0rnzQreiiN_v7a16FnqiX_DwI1e6Xon79aOScxajId2IMwe5Fra_TpfeT9ApHyfKtuJ-cfGLOsDQRWnvEhl4_00rFW5Sa0PLZCcp4YcanzzVk-hAR" style="width: 80%;">
                        <p class="caption">BERT의 SWAG Fine-tuning 방법, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>4. Named Entity Recognition (NER)</b></span>
                        <br>원래 개체명 인식 부분은 논문의 ablation study에 등장합니다. 하지만 제가 생각하기에 이 챕터에서 다루는게 적절하다고 생각하여 소개합니다.
                        먼저 개체명 인식을 위해 저자들은 CoNLL-2003 데이터를 사용하며 각 토큰에 classification layer를 부착하여 fine-tuning 합니다.
                        이때 BERT-large가 96.6의 F1-score를 달성합니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_FWsF0Mipm1Tkxh2qBDwOSNjLDPCTCnqYE6dQYWN3zSJ4wFHfEHzD-Xw1E4JITTVRnWuH-aU8kKKeADilCMvImy7Mp3v2xEcv66g0v5rFrk2V5uNH2bEsy7ik1jkSbJSbhaC2m-q3Z0YkIhO1CEjy7oCpOexq6PtSe06PQXq49OefzinlB1Pl6D0vmq_RPt7H9TlPOm036KSoV3gKeLcxnrNhVvC0hkWLNJoPDhCRz_jOKQyjLTusQzu51JZf6mtGYAKSgjycsr2zMpJENx2KEFbw31GKyDBU-6m3GHrDUnYsB9vg53uNW8EnuRZNEfkzfH4YJsNh0okEUXA6VzxMKbUXpz3lwnDerD4w0UZIAf8ASwZLfd7_wGwo-vgKTY7FnotqIXl3Z48_-xoF8vnjVih-UI4QVuUHu2b3s1dI6OGomCNGWVZkGD-F8mjRsGq3YIa5lsgyNCGPWZET7kcQw9C8NrzzJB_as0u-Sd_sjtCp6jsgOavqtf_FjdneJstDTd94ZGWQtI8O4wEs1NWPc2icJcUvwkh4n3-LZPa_658wY3RJQhZMwDz9hEbKoAgrNjmOis_5q3Qg_AJOs_ZQgQ5Mf1hRV-FgfTbwnYEYTHodhpqoDVqaT0gLZuTJOe_yLjloipPQT0IoeubOGN94eo9IioqucbhuZA40fjeDP_rOfU1zDoTZJOfaip81GAaLUA9iq1eO2kxs3Rok9TVMlyGAB80OV9iCCN3vh7yrmoLhdU0Ykqy7J7oYm2j13ZwJ8BOOejGkTjkkPFu6B-D_aVcBBDhZqMlA_rYgUz3TyoaLL60-fs5200VSyXPn6-D1IcWNy59VThBCh2oFNEb0FZcx931cL7rR8YitFZQPBHyItEe1aMuf-_loOryR5J9YZAadCsXwKxOZp0mYUnVWEeQXAPg_WpBIiiiuDRIvEUaFk5FILPmYKk9gBQIWz1Iv6jAQqdGalwhMiT6El8stxs-mcvJj8lyBOLRq5fx1sTdJIRqMGtcLu5IVoTuK7sbGc13VAgDEGrYpYR8gCLBzTOEYlr46PzGSfBlRlPuPrB0Gik0z6VFY757yWItKewX-PPezyaVQ_MVAFP0Aw4Jxo17qLoKjNXkK8Y70tHB_2dvi4v4_DfjYY42yfUT4l_e9VQl7KPaOQdZDW-XM9f2zinHGmZsAbrqYuktMzTytMNtJ0PeNG8cXwagz1-0iPrqSRxmipLvQ5L3XBCsG5nXSywBiK2qJQKeNECAbXI5hv23tuyzH2vNzB5ZXZERYYuxHSu9ckya0q722MrXaySk_zcvQCh6ZO57NqPrUlJhX_3B4FwTsCvPRwvqwLXVGHjc1OefDskh8PDhH7BG_MCpkdKUj4ku3VpGo-XxRwbOAqFzSNes1aZe2Q0MjNmcWzAjIm8zl1MDTCn-7J0_rBaAXz2iRj64ONISngkiZa6nL99v8OEVwdtLL7Frz9ZG30tZarzq-fcLnLkgEaOSYbGyyBaWKW6Hh-8SCXaWa_rrOSWsCt6hWQLy3sItFox2ol1Hfa-6CMY5h64" style="width: 80%;">
                        <p class="caption">BERT의 NER Fine-tuning 방법, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br>원래 NER 부분의 내용은 ELMo처럼 파라미터를 고정하고 feature-based approach와 fine-tuning의 성능을 비교하는 ablation study에 등장합니다.
                        Fine-tuning에 비해 feature-based approach의 성능이 조금 더 떨어지긴 하지만 <span class="highlight" style="color: rgb(0, 3, 206);">computational cost의 이점, BERT로 불가능한 NLP task를 위한 embedding으로써의 활용할 수 있다는 장점을 고려했을 때 feature-based approach도 절대 나쁜 성능을 보이는 것이 아닙니다.</span>
                        즉 저자들은 fine-tuning 뿐 아니라 feature-based 방법에도 BERT가 좋은 성능을 보인다고 주장하는 것이지요.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_Exjv09QLxx_4J7PiAAomt1NBMT27n7D0LU2Lk0yf58zgNZggLqWmEFTXHr6y-9EnrfxndhZRLfCe4D6zgEFpO9ge7EmHFX7dzgtxikFd_Cz3oy0VLv9fZMMU-p7tNFD51a9PxElUA4QhOxWcolUPlBzfQst7pFY4ZeCPKwT55Xn4AcETcoRQG0w3dtA_QI_cDVXqDj4BQpKWMKFSKme6sTnC2IqBzSYDlRTqAldze5UBedpCvmfxJTww3-KMNOGPKlkaI1Mez6qOXxQvschJhQeJgPcKzeFFS-dg-QzSLqOWOtMelDpQAEKkEdxhtmmx6FV68qtLqCl78mcsJ7VNB6U6LnPEozWr4aq2XPAb9AbpAwRgJEF-DY92od_9iHe94VN4FH77q4oUqvCjcP5r6nTEJLc1uh4Kjr8_jJYZadv7p-vlP99eY5un0JkWMDHL1WT68LnqQiS5kd_Umgt4mYjSCVuXd5b3nW26XILWVhNWc8CXX4ltFSrvK3ZsXw7p4P-USF36GSdDLBRu6kWzTzH6HoO4fv4QIQQCPTjaTy4Dfvy5XZkmbQ4CupjjIP9IqZ1QHSkgsPfqKzjq4edhlLlMK-qNTQ0fuverjmmO_GyV-VAoj-lL1dMxWno79TEDBUvV6GIZp5pzvAvYWO2PBPjZjniX2WabZOIF3o-Z9Ae1op2iD3xj3aDcwcDpI903VfTyYlypbnz3EYvXuTYeRxzEzobTDSbcKcAfEIFW9si7TYIScm_wob6Mf4yg-JxBY-eJVe31B6b7q8f1jtbmn44F0hzjp33yFHKWhCtV5QjRctI-bpKw3zY4D42cEm3Y3rg1GWRRRR_9URreNUvJze2WwsLD41f9VOjyy3yiNj3XBdKaYrGKcutP_Ugr5tJpjtjP-oQl84IrbU-SjKMSIEK-FNLv1bhb2r_8dkr5lQXJYVwfzCzWsqA4YbV1pO7mxKfUftfVsweiaz-fjIHvg2JtYsNhSRyrKVeUBlV-YzpXWprpuioZlb7CjgiTZk86LakCH5bfErkIi7NsZKjhRqgirWGPR2wUksBnFyO2qvdDpZt83uLi7yXNNXU10zAWmjH-11H3sOfCZJmEydlIsbjWcvDF23-_hZ5VnMutwwcqP_p7LHpEhxagURNpaHUQHxJ3b5yK5f3jJQNbuQzrNSyCR4ezf_umFVbr2CES2bm4ZOl4nKO_-1KyiKKA8WR4KBkAdWcV_qYjmfpW2lAAxmjsu9sorFiQhJT1I9TeFpgMpOKiyen1jIaUMh0z8T_IhV0uXuo7aXVENC5xZD4t9GcH_GHZ362A-4QtxyTFFqSzBWNo22KdujLzQ3OL_IXQJgeV7Z1jJ0u2APJ1zydBZujD3s2AMEJhSYvib6m0d4W1Ll7Djz8uD-JQx_yXxkcgTlr_qpIOISmhfeDbBhacSGQPWsT_sFLZ1cFDkuhQqs_gUfkKlzCySrs2qrAnupJbVAeHD2dZndQcVWN2XB1bLmSPFX6fjmfMuxkvtMd5mXAAxBcZ-iyhlmKFrvnDUvKpzS8d0bRy5n" style="width: 80%;">
                        <p class="caption">BERT의 NER fine-tuning과 feature-based 방법의 성능, 출처: BERT 논문</p>
                    </div>



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>BERT Ablation Study</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>1. NSP와 MLM의 성능</b></span>
                        <br>저자들은 NSP와 MLM의 성능을 측정하기 위해 아래 두 가지 케이스에 대해 실험을 진행하였습니다.
                        <ol>
                            <li>No NSP</li>
                            <li>No NSP &amp; left-to-right language modeling</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 결과를 보면 NSP가 없을 때 Natural Language Inference (NLI) 관련된 성능이 많이 하락하는 것을 볼 수 있습니다. 이를 통해 NSP가 문장을 이해하는 데 많은 역할을 하는 것을 알 수 있죠.</span>
                        
                        <br><br>뿐만아니라 MLM도 NSP 못지 않게 중요한 이유가, 기존 일반적인 unidirectional language modeling에 비해 MLM 방법을 채택한 것이 더 좋은 결과를 보입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이는 MLM이 bidirectional한 특징을 잘 유지하는 것이라고 볼 수 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_HaSYvarct_vCvWyULdjPN_jIaydYkNL4NWM2-x2YSAV70eTmtTKF4jZ25bhnIgwZZnvDCEaJBNb4x9OoAbAut8FbkMQjRGQuql5yGqkTfStZ_z1acmQJ0FODOQud1-Bq-VTdiQHGn59u64yjO9Jk3jpRWi8QAOiIBeVouJP8hFVQF54Gr_V5FrnVvOLf8HOwvOJo4K_kz1e2Ibw6VHc8umg0d-KuM2FOK7ualS2dlDfq4OJtB1KgjvBe3cr8MS7VVEssTewgRllo-BC0kUD6knpsVCo58N6e6dqvhf5Reg-KQfg95tFNBmk6g9jHVoMDorUnRObYjZOO1Uwe9R4JNxCL3RIB4SPcirJ7prmCaXdAgh1G15k9m8FECnTGj2FwDW4ctdvdg2-rgHSWzM5qbobEeszpJU-32MPAaHBvtj7vLUq165RT6bmFMcC-889CemXNzf30Sab5lEQLOCSquLPGemukWJ3vivpuaydywbxL67gLjehpMdN3BwQ0c6KLUojxCcIoF7_aUvgVuZ_gA-heTYN8v2VMoVs1TAmVpTsz6-XjiAC5Ki42imCiKcfFZZm9siLS-TyX-dgilEvItolCRBs2kQWs8PLuQBJgG_5qtR5A930ypXoieCOyVfBm4Fd1waYGOTGEzU3Sl_8q4Trow7NGzkz9XUEsa_Q21Xz6TV9ohAftQqsLOHOBDEyRLpfT-OmlGREYdXK0Va1vQc4tAAwdGUjqbti6DWM3UkFd1h6aYKrt5UWMlsTLLWX2IAmOHIT8li_QvlZmTUqpM5BOrykHxvShsaUswdixB000c25j3NgD32JVFlweRlOKZBZAvFoM5hNzPHCk2gKZgh3kDdgXewkoWoOmdfN1FvU5UhJeMQMt-8EHXdRvSazwDz3TayGQ3-179XK0-fYF8kRFZcrgrd1NXdFSv9KPhASHX-p7JONcyjAjBotDSIolACASPtECh43I_pv1P3RrCH_p-OTm7r-J1TmDSgLUcQIJxVURN9e5Va84ux31UkrWktkE5mQlhThWyOReeoLJ-R9NgY3_1MOciCgKBaCkmfggQ2NEcPHHogb6aq0V0w0LTDz_rwoVXUt_DOthKXd3svfSBZTrAiNi8DdezeHcsRjBV7nC22Ai1jSSFSbkQI8-FFxRE2ytJmtgp5bgZr1z27QBLCJEhu33NCWqJZto9Nl-n33NKE12PVvIPhsQN2WxuosT8U1imRxaSdAheopzjuesZgqsYC8XcVeSMWQdiSoNNYAAsY7IL8p81XrdCEDAWQPzLYPzZAdnqcnhmCX-P6cRDhLd-n_MzFDKx4trLJF4Klv7xZ4mCGEnd_UvAMe4HAx_QGFPhZInhLoU-6_o_cPUBrE1PSqKo81U22jPLNRbtMQaatpUC-k_pc5-4CLCO4QNzojB_9KrYacqUWB8y_-tuVlHaRFQMNrwBN2uGejoKTdU5UBZoH-hG7wbouT3TgSkVIle3SbAtnXkda-FgfUWbRfxKKQaSw3ICyyNx0XkmppFXMOMeiB4prjYopIuKznHL_LYEj" style="width: 80%;">
                        <p class="caption">NSP, MLM의 Ablation Study, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>2. 모델 크기</b></span>
                        <br>위에서 소개한 SQuAD, SWAG의 결과가 BERT-base 보다 BERT-large의 성능이 더 좋은 것을 봤었습니다.
                        뿐만아니라 아래의 결과에서 봤을 때도 모델이 커질수록 benchmark에 대해 결과가 더 좋아지는 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/ALs6j_EHabXKgm3FxVOU_-upmlpT8_R4kwuZD_ntjXbaUG2f80Skj9xrMqpPCteV6hMNj27vzXt0wzHKoc1LO3vzqfGZvN1iswnJkUN_gS2DvLqoiRNcPtuZ0FhFOJx1iHMemfaHhdHUlJGHgfz_uSS7HvYZZF-HZ2zDLtuk3sTN_3MyZVxfUeVFgjWnG_Rz-n6NNe3jxooNg1AL53wnldq_TKcrGAfniSdf4GqCJEMa8azGLIjrcqXrjQV0x-Ua_B5flAAt66TCZCqmtrRO24AHEJrMDIQU1yBTYgxsFPPN1Z11yZWpovnWS_U2HW0rGTaXYSX_H4WGUvq1qfKtDSRjZZSIoAnBGNIhhE9VlAgJEjmEodSmR13tvtqcPvtwvfe0wd0U_HmkrpoSHBiGtFb_lwBjByMsUk3FR-ZRbxKGK5xtsMtTLXWT7aJdRIXeVOh-m1O7aS_OCBNt7BrUsrjG_2UboP7czZ3BBRnsb7dOSF8UPUlhwidk3BibthvRpgPO-pSw4tmSnPzp-xXlh14Ve61hyOlpKdUH8c9MFbz3_4N2Rx00DBzFwPR5jgkLqD2C0WHVpYtmLv8B9zJZ3Ef6Ui7EzccuX8GxAKqvH-1LRfhWeb0lYaXJtHL1jA_bthtLjKe8xNzLSVQWPdGRdafrX9_uobXUfrOdZRNrbt3qJOk1DAYYnyvs7m_FDbcxWLVGZ5x0weV7dlamiKJup85hOGPVlf0SUgozVyNc4gtuHJxNy8HfBu2wScJr__ivfwkbx3xHe4UpTiXI07LH8GgFojX8XzJ-0XdkX3FexYjcfV5RvSiIL7FifkyoOZHhB7UcokQ5fre6yfadFhKwqg9A7FhtOM2tp-UbTdkd19wVT8rHM_3tacT8c5ED0IT5iMG3qGzsKTjaD3mpbaxKCP7X0Cn4Mv9KXptzAJqW5iSX6dqdHoIDZrQQIhk_GW5QVz0eE4jypXOQhxa12RhUYg_1xp7Cv-q9K97Cub19KyKyi2hO-qoxFwdVQF61TgR0PmhrMJhYBBKvU8__HbtPaZzKifmHj99AGYj4LywOhWHd2pjUotaoE45wiw-SQqSwRShnVIdxNKznro5HjoJMMkoc_WIFs_Jh7_VSw-IuKGcSFYGj-ko1yNc-Nei3lK0n5b7Ai-8uPzSYZjmx1APZBiaKI3WCZpVJxkw51dfRwsvVXulcIYe720YpXEiTgVqjlGxPVJIDmH1HVP5CTTg2zczQBVNC6S8PJU32WgVOnPeASeNmApwLmnyY6RuYVAoF1rWYp77FTYFa3eB0Fn6VuwSSfQICEjgqjNfr7wlcBofnA8UCwo_HNRNdR8KbGT4-BWPqx7dzj1GM0-sbfQsxFuxnNwrEJ3ImEo_4oWptPd3ghWuRHk-XhX-dtbVzZ7KuVjPM184g_t-1UUTTv0zYdDE97voy4UaGyQ7P-_JikWBwjR5nvkkZkpcoYp7O7BNgK1gLgVsGTabb9SdF7Yqp1BXnGO0AJXRfvzjyw0oYBpk8rvhQJp_CfiYSuB0uL7eCCZxMizcy3Z7BqaIF" style="width: 80%;">
                        <p class="caption">BERT 모델 크기에 따른 성능, 출처: BERT 논문</p>
                    </div>


                    
                    <p>
                        <br><br><br>BERT는 등장당시 엄청난 성능을 보여주며 모든 NLP task의 SOTA를 갈아엎은 모델이었습니다.
                        많은 데이터로 학습한 영향도 있겠지만, NSP와 MLM을 제안하면서 보여준 ablation study에서 이 2가지의 학습 방법의 효과를 입증하기도 했습니다.
                        다음에는 실제로 BERT 모델을 pre-training 해보는 코드를 소개하겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#BERT&emsp;#NSP&emsp;#MLM
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('BERT 첫 게시물 입니다.\n\nThis is the first post of BERT.');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('bert2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>WikiSplit을 이용한 BERT Pre-training</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>