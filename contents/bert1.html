<!DOCTYPE html>
<html>
    <head>
        <title>Bidirectional Encoder Representations from Transformers (BERT)</title>
        <meta name="description" content="강력한 성능의 언어 모델 중 하나인 BERT에 대해 설명합니다.">
        <meta charset="utf-8">
        <link rel="stylesheet" href="init/index.css">
        <link rel="stylesheet" href="init/contents.css">
        <link rel="stylesheet" href="init/index_img/icons/css/fontello.css">

        <link rel="preconnect" href="https://fonts.googleapis.com"> 
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> 
        <link href="https://fonts.googleapis.com/css2?family=Dongle:wght@300&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Gowun+Batang&display=swap" rel="stylesheet">

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic&display=swap" rel="stylesheet">

        <link rel="stylesheet"
            href="init/highlight/styles/github-dark.min.css">
        <script src="init/highlight/highlight.min.js"></script>
        <script>hljs.highlightAll();</script>
        <script src="init/highlight/highlights_line.js"></script>
        <script>initNumber(window, document);</script>
        <script>hljs.initLineNumbersOnLoad();</script>

        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script src="init/index.js"></script>
        <script src="init/jquery.pjax.js"></script>
        
        <meta name="viewport" content="width=device-width, initial-scale=0.8, max-width=1">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-219110982-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-219110982-1');
        </script>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-7745178886614102"
        crossorigin="anonymous"></script>

        <meta property="og:url" content="https://ljm565.github.io/contents/bert1.html" />
        <meta property="og:title" content="Bidirectional Encoder Representations from Transformers (BERT)" />
        <meta property="og:type" content="website">
        <meta property="og:description" content="강력한 성능의 언어 모델 중 하나인 BERT에 대해 설명합니다." />
        <meta property="og:image" content="https://lh3.googleusercontent.com/fife/AMPSemf--n1ZUKJnTmBJBa6AJdoKKxx1yznmcT4RM9WzMCE3xpJLTw4uqLjPvuM3zQGauHLSVUCRsc2eNPn5P9D0B3ifXsBHk9TJVblt8tJyh_L4AeAhJPRRmUVsOJIn7f9hx11k8RzrYOX8rSF2qya-jVbQErfl3IERKUxRluPUVFkSH2l_8ao-kT2tsEYn48ZYAIjvkBt4hJKx78V2PGSAMMyER3nK_0jJo6pD7N0aXcHAel6QGV7VTZKrCQa1CV_vQ73lJYNWUtGRjuO4Y5UGJIzOULmnzqkpikhk4rZwHwNkT7pIyj8TzH-Opx6YPXsJgJYfYOaVF8MNTXlcoQvUwKMChyWsBlaQAsBbRxlvStL9eolw5_Gi52hiOuyRao1_e4SG-p9BhAXLe_WjWXRbSqcZCjAJ6sGutq8p5W9ZrRVsbLLkX9s01FQgfSE2_A5n5MzlRcRwL-Gn5foObQMy_AKs8HhXXWDTjLGi3QH5V-KnS3cT71MKQ0wtAAGWEAfwUUXaQiWnP2QOWgY2dv3hejMyHO_VZ5f0Iyat2ipIPZ-q2gxbuQ-6e5PU_zZw7l3YfH95MQ8WXCScY51L9T_UMsSZBCnyqGsjy-JqEv7FLjMmfAB3jHJBY9yEsQZE4tJ1nXS4dlGvO9n41nXmm_zL7304z39gtuv84ALIvM90tXQ3gBBR9Ru78y84J5LF1LHp0wq-sL7HVImZidz1m8aruEhgLWGPunm6XvKuN3nEupuMdhfqHcCA6HDQSEhbPAS88uBnKe_8rNC-6RePkHjxe530Lg5xdRoDBBWwE3iP2JTHGIOVQ629K1pLIoWPzKsgzFhzlg7Oa5-BIDB2E39TyFP1J9jilgbD3zcWDRJcfvlA2Y43-6kWM_FFX8ttlbFSDQVstUwieindtACLAvt-WwBpCr3ldx0_4cMDWv2xi7rj7W5PS_rDHvfabD0p1xkg11DpiQy4iwiWsi8FTOsKR-U9qf7xZx8Gp1xtU07GJCvwKthXMWmesPIXyFClLMgRWQYnLj0LfBQKvV5Ya6iAv6j0QeMPhqIeQq6QctIV1ZX1hyKCDcWLuWwNU3VsazeEDtjDk8XN_r2hIyjk_H7Zi-EuUKxuNNffORHJrazozXustNRRe5rLe_BcsLkM9aQrXVQbLm0s4eXKiqRlXpFgZT55FxjsboLR1nvIxjB8Z-qIT51a7Rir5zbFJx8a_VKPelD222yfi3d2MizpAkEkpZ83TGuk3BAhzfgSLRDVuOAVLjeBxsWcAigHfdbm6pGVqDz9m4SDErEYLUcYWwjoFiQKhvpL2TH5BiaoLmWA0yBheUZqKEVKlJby7zhXwaruNP5ix5dQ4mHkBIqGT9kdeA_pgSCTAre7xljHMJcdgtJv97OYWy2SBEJCxgo2OKpZJ-ZlxhYlS61Pw_m0Ggyo98OZUIMSSCwtK0fvN7jcvowzbGaMmtgLeZPh-pVEfX7yf2gIoX9Wb-nMF20B5IEvzy0" />
    </head>   
    <body>
        <div id="modeButton">
            <button type="button" value="dark" onclick="darkMode(this)" onmouseover="hoveringOn(this)" onmouseout="hoveringOff(this)">
                <div class="modeImg"><img id="modeImg" src="init/index_img/moon_off.png"></div>
                <div id="modeState">다크 모드로 보기</div>
            </button>
        </div>

        <div id="container" onclick="reload();">
            <article>
                <script src="init/highlight/highlight.min.js"></script>
                <script>hljs.highlightAll();</script>
                <script src="init/highlight/highlights_line.js"></script>
                <script>initNumber(window, document);</script>
                <script>hljs.initLineNumbersOnLoad();</script>
                <script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
                <script>
                    initMathJax();
                </script>
                <div id="mainHeadWrapper">
                    <div id="mainHead">
                        <h1 class="contentHead">딥러닝 이야기 / Bidirectional Encoder Representations from Transformers (BERT) / 1. Bidirectional Encoder Representations from Transformers (BERT)</h1>
                    </div>
                </div>
                
                <div class="title" style="background-image:url(https://lh3.googleusercontent.com/fife/AMPSemf--n1ZUKJnTmBJBa6AJdoKKxx1yznmcT4RM9WzMCE3xpJLTw4uqLjPvuM3zQGauHLSVUCRsc2eNPn5P9D0B3ifXsBHk9TJVblt8tJyh_L4AeAhJPRRmUVsOJIn7f9hx11k8RzrYOX8rSF2qya-jVbQErfl3IERKUxRluPUVFkSH2l_8ao-kT2tsEYn48ZYAIjvkBt4hJKx78V2PGSAMMyER3nK_0jJo6pD7N0aXcHAel6QGV7VTZKrCQa1CV_vQ73lJYNWUtGRjuO4Y5UGJIzOULmnzqkpikhk4rZwHwNkT7pIyj8TzH-Opx6YPXsJgJYfYOaVF8MNTXlcoQvUwKMChyWsBlaQAsBbRxlvStL9eolw5_Gi52hiOuyRao1_e4SG-p9BhAXLe_WjWXRbSqcZCjAJ6sGutq8p5W9ZrRVsbLLkX9s01FQgfSE2_A5n5MzlRcRwL-Gn5foObQMy_AKs8HhXXWDTjLGi3QH5V-KnS3cT71MKQ0wtAAGWEAfwUUXaQiWnP2QOWgY2dv3hejMyHO_VZ5f0Iyat2ipIPZ-q2gxbuQ-6e5PU_zZw7l3YfH95MQ8WXCScY51L9T_UMsSZBCnyqGsjy-JqEv7FLjMmfAB3jHJBY9yEsQZE4tJ1nXS4dlGvO9n41nXmm_zL7304z39gtuv84ALIvM90tXQ3gBBR9Ru78y84J5LF1LHp0wq-sL7HVImZidz1m8aruEhgLWGPunm6XvKuN3nEupuMdhfqHcCA6HDQSEhbPAS88uBnKe_8rNC-6RePkHjxe530Lg5xdRoDBBWwE3iP2JTHGIOVQ629K1pLIoWPzKsgzFhzlg7Oa5-BIDB2E39TyFP1J9jilgbD3zcWDRJcfvlA2Y43-6kWM_FFX8ttlbFSDQVstUwieindtACLAvt-WwBpCr3ldx0_4cMDWv2xi7rj7W5PS_rDHvfabD0p1xkg11DpiQy4iwiWsi8FTOsKR-U9qf7xZx8Gp1xtU07GJCvwKthXMWmesPIXyFClLMgRWQYnLj0LfBQKvV5Ya6iAv6j0QeMPhqIeQq6QctIV1ZX1hyKCDcWLuWwNU3VsazeEDtjDk8XN_r2hIyjk_H7Zi-EuUKxuNNffORHJrazozXustNRRe5rLe_BcsLkM9aQrXVQbLm0s4eXKiqRlXpFgZT55FxjsboLR1nvIxjB8Z-qIT51a7Rir5zbFJx8a_VKPelD222yfi3d2MizpAkEkpZ83TGuk3BAhzfgSLRDVuOAVLjeBxsWcAigHfdbm6pGVqDz9m4SDErEYLUcYWwjoFiQKhvpL2TH5BiaoLmWA0yBheUZqKEVKlJby7zhXwaruNP5ix5dQ4mHkBIqGT9kdeA_pgSCTAre7xljHMJcdgtJv97OYWy2SBEJCxgo2OKpZJ-ZlxhYlS61Pw_m0Ggyo98OZUIMSSCwtK0fvN7jcvowzbGaMmtgLeZPh-pVEfX7yf2gIoX9Wb-nMF20B5IEvzy0);">
                    <div>
                        <span class="mainTitle">Bidirectional Encoder Representations from Transformers (BERT)</span>
                        <br><br>
                        <div style="display: table-cell; margin: 0;">
                            <img src="init/index_img/profile.png" style="width: 30px; cursor: pointer;" onclick="pjaxPage('/');">
                        </div>
                        <span class="subTitle" style="display: table-cell; text-align: left; vertical-align: middle; padding-left: 20px; line-height: 125%;">작성자: 여행 초짜<br>작성일: 2022.12.20</span>
                    </div>
                </div>

                <div id="content">
                    <p>
                        시작하기 앞서 틀린 부분이 있을 수 있으니, 틀린 부분이 있다면 지적해주시면 감사하겠습니다.
                        
                        <br><br>이번에 소개할 논문은 바로 NAACL 소개되었던 Bidirectional Encoder Representations from Transformers (BERT) 입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">현시점으로 무려 6만회 가까이 인용되었습니다.
                        과언이 아니라 BERT같은 경우 현재 연구하는 데 있어서 없으면 안 될 모델이며, BERT는 summaraiztion, 기계 번역, 감성 분류 등 현재까지도 다양한 연구를 위해 transfer learning하여 많이 사용되고 있습니다.</span>

                        <br><br>BERT는 pre-training 후, fine-tuning을 통해 NLP task의 성능 향상을 위해 대량의 언어 데이터를 통해 인간의 언어가 가지고 있는 특성을 학습한 모델입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이렇게 text가 가지고 있는 정보와 특성을 잘 파악하는 방향으로 학습하기 위해 저자는 Next Sentence Prediction (NSP), Maksed Language Modeling (MLM) 기법을 소개하였습니다.</span>

                        <br><br>아래는 BERT 논문 링크입니다.
                    </p>
                    <div class="link">
                        <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" onmouseover="colorOn(this);" onmouseout="colorOff(this);">BERT 논문</a>
                    </div>
                    <p>
                        <br><br>오늘의 컨텐츠입니다.
                        <ol>
                            <li>BERT가 나오게 된 계기</li>
                            <ul>
                                <li>ELMo</li>
                                <li>GPT-1</li>
                            </ul>
                            <li>BERT 구조</li>
                            <ul>
                                <li>Encoder 구조</li>
                                <li>Embeddings</li>
                            </ul>
                            <li>BERT 학습</li>
                            <ul>
                                <li>Tokenizer</li>
                                <li>Input 데이터 형식</li>
                                <li>Next Sentence Prediction (NSP)</li>
                                <li>Masked Language Modeling (MLM)</li>
                            </ul>
                            <li>BERT 성능 및 Fine-tuning</li>
                            <ul>
                                <li>General Language Understanding Evaluation (GLUE)</li>
                                <li>The Stanford Question Answering Dataset (SQuAD)</li>
                                <li>The Situations With Adversarial Generations (SWAG)</li>
                                <li>Named Entity Recognition (NER)</li>
                            </ul>
                            <li>BERT Ablation Study</li>
                            <ul>
                                <li>NSP와 MLM의 성능</li>
                                <li>모델 크기</li>
                            </ul>
                            
                        </ol>
                    </p>



                    <h1 class="subHead">BERT</h1>
                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center;">&ldquo;</span>
                        <span>BERT가 나오게 된 계기</span><br>
                        <span>Motivation of BERT</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        BERT가 나올 당시 유명한 언어 모델이 있었습니다.
                        <ul>
                            <li><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">Embeddings from Language Models (ELMo)</span></a></li>
                            <li><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank"><span class="highlight" style="color: rgb(0, 3, 206);">Generative Pre-trained Transformer 1 (GPT-1)</span></a></li>
                        </ul>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemfj0a7cv4NA28KWwY6equvHW2YhUD-K0dJBIfDr5x7u8XQ0v-kr1LX_8Ezp0ZTqUIDq_uspTowPrAsMXdWtAMjQOOC12JCXTL4SP3uSwp17w8okPRLXT485qgWxaHXmsPSiSfQp7wD49g2JYoMQuFUxVV9hhPcDTVyZcdnqua77pAQX_pJLq8qWk5GUu94pLLI22UPTfQYRqU9PEg-jQynsgjeJpohq5KgCQj20AgbLc2MHhv7pnPY6Zot_QO_CASWBDmNeCePCpAY9Exehlk5Yjp2FD98wx4c2sAn0XMooKuD5nQ0aQgbAlzzzoH6cgt-0W-GBIq6_IgLc5-tBJMxKfPqQT2XN4Qb4rN9XV8WAA4eByKLACviN7N8HRxGmRRDqSbpqMLb6IOdzk6KLjEkUqeui58s6UlKy9XPMv9ba9foJd0f47j6Y14c3CptK73zI2r2IKNKI7yt1tkhpC6z8Ctjv3cMQd3vInDruWBUP8ACF_-vmkLln4miXCjM6BkAVxfRJO-QumLZtBjckmWeF8GCanifh2q_7kizZkN6hyTIR6tO7k0qfs3BXDzSBRSF5uNyOqJmzTYVuxoAJxgf9qcYxpFXYSDjq_Q2zuHUEOp4Nb_z5TpU7QFvX0p1RB7aVNdZIiJvg1ErXVg3Gqe_FcUBUzobOJKyCWO0thOa4bSrPmEGk7yYRHqDlL1gps3YX8pm_Bf1byfX64c_Ye_3jANMt5zI2-GLD69PJ7NX3yfGt-BfZLsr_Rp5N4gGNV6kdhF57RxhSLPIdT3PGI0WZ4i4VyGTC_zCNuqNa8XwnMcOSUYIpo4Flgl8MZE3zgVMxEdmW7YoYE6ILC1ts7KrqDhpe8nCQMEVL-9GxxFdAtbvpXeHyABOAUkY7E58m2S_JYNoqO9alvF-QayXFDl3PbKGPCbeojymoShEAf8P4V6sl5NJGrl4wWdrHYdMiZ4TFgAUMNp7RnNlP_1nlP9XwUU9V4FJAC4TbJxSgVszKPxFSb2TmuYhDOr68UMMlO0xYbhIEVwjAMn4Tg4h4IH5w8ANjPv3JxzxoUEPJugACPlSNWkH4HRnqUvBncEMdz4DCUysMPFdqqK6_FFQzx4uPzxLBK66ZO1VnAVbdC59UKCMqJNTe2Og3WVKl2dXTUhuPcOnfBrlsC_WK79SucLtb0A0mvbMvT0crFPkRNmRSMi8vA91dCfZsE26u-ZU0XPG0YjL5i5kZgkVY0XcvG8GSY4IBRh1jTieortkrDL1pI11k2h3429zJ7jQ2nuov_uJZ_mhPze5RLspCfo1iFdKATexVLr2vO1n5O3IWFsIzTsospS_DP2di55qcvS-x2gZqCboap96Rj9SXSprH28lNh8cHwOJRStNE4pAYW4OMAO-uneuWPpacqzkGnZsEN2pkvMg-zmW49v5TIifdGFcJf5ff4-d67erhylvwb16FZU_-nJXpxGt296m_kTWIPdjYMp_6qpBRGW-nJFIzv6M" style="width: 100%;">
                        <p class="caption">ELMo와 GPT-1 구조, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>1. ELMo</b></span>
                        <br>ELMo는 bi-LSTM을 바탕으로 텍스트를 학습하고, 학습이 끝난 후 파라미터를 고정하여 input 문장에 대해 representation하는 모델입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">쉽게 말해서 input 문장의 토큰을 임베딩해주는 레이어라고 생각하면 됩니다.</span>
                        따라서 NLP task를 수행하기 위해 pre-trained ELMo의 파라미터를 고정하여 문장을 임베딩하는 데 사용하거나 fine-tuning을 진행하여 사용합니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이러한 ELMo는 단점이 있는데, downstream task를 진행하기 위해 무조건 ELMo 모델 위에 task를 위한 모델을 하나 더 붙어야한다는 점입니다.</span>
                        즉 NLP task를 위해 ELMo를 포함하여 총 2개의 모델이 필요한 것이죠.
                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">따라서 BERT 논문에서는 ELMo를 feature-based approach라고 설명합니다.</span>
                        더 자세한 내용은 <a onclick="pjaxPage('elmo1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.



                        <br><br><br><span style="font-size: 20px;"><b>2. GPT-1</b></span>
                        <br>GPT-1은 앞의 토큰이 미래 토큰, 즉 이후 토큰을 참조할 수 없도록 학습한 언어 모델입니다.
                        즉 일반적인 decoder와 left-to-right로 학습이 진행되는 unidirectional 구조를 가진 모델이죠.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이러한 방식을 BERT 논문에서는 fine-tuning approach라고 설명합니다.</span>
                        더 자세한 내용은 <a onclick="pjaxPage('gpt1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.
                        
                        
                        
                        <br><br><br>특히 BERT는 위의 언어 모델의 단점을 언급합니다.
                        <ol>
                            <li>ELMo는 downstream task 시, 추가 모델을 붙여야함.</li>
                            <li>위 두 모델 모두 pre-training을 진행할 때 일반적인 언어 모델 학습 방법인 단 방향의 unidirectional한 방법으로 수행한다(<span class="highlight" style="color: rgb(0, 3, 206);">ELMo는 bidirectional LSTM이긴 하지만 학습 loss function은 결국 left-to-right, rigth-to-left의 단일 방향의 loss를 합하여 수행</span>).</li>
                            <li>이러한 unidirectional한 구조 및 학습 방법은 언어 모델의 성능을 제한한다.</li>
                        </ol>
                        
                        <br>따라서 BERT는 <span class="highlight" style="color: rgb(0, 3, 206);">downstream task를 용이하게 하기 위한점도 고려하면서</span>, 위의 문제점을 해결하기 위해 <span class="highlight" style="color: rgb(0, 3, 206);">ELMo의 장점인 양방향성, GPT-1의 장점인 transformer</span>를 가져와서 새로운 방식으로 언어 모델을 학습하게 됩니다.

                    </p>
                    
         


                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>BERT 구조</span><br>
                        <span>BERT Architecture</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>1. Encoder 구조</b></span>
                        <br>BERT 구조는 transformer의 encoder를 사용합니다.
                        Transformer의 encoder는 decoder와 다르게 input token을 attention을 통해 모두 참조할 수 있습니다.
                        따라서 양방향(bidirectional)으로 input text를 볼 수 있다는 것이지요(Transformer 설명은 <a onclick="pjaxPage('transformer1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a> 참고).
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 encoder를 사용해서 text의 bidirectional을 다 살펴보고, 각 토큰이 다른 모든 토큰을 참조하면서 모델이 text representation을 더 잘 할 수 있다고 생각하고 저자는 encoder 구조를 선택한 것입니다.</span>
                        그리고 encoder를 사용함으로써 bidirectional LSTM보다 연산 속도가 빠르고 깊게 쌓아도 성능이 좋아진다는 장점도 있습니다.
                        아래는 BERT의 구조입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemf--n1ZUKJnTmBJBa6AJdoKKxx1yznmcT4RM9WzMCE3xpJLTw4uqLjPvuM3zQGauHLSVUCRsc2eNPn5P9D0B3ifXsBHk9TJVblt8tJyh_L4AeAhJPRRmUVsOJIn7f9hx11k8RzrYOX8rSF2qya-jVbQErfl3IERKUxRluPUVFkSH2l_8ao-kT2tsEYn48ZYAIjvkBt4hJKx78V2PGSAMMyER3nK_0jJo6pD7N0aXcHAel6QGV7VTZKrCQa1CV_vQ73lJYNWUtGRjuO4Y5UGJIzOULmnzqkpikhk4rZwHwNkT7pIyj8TzH-Opx6YPXsJgJYfYOaVF8MNTXlcoQvUwKMChyWsBlaQAsBbRxlvStL9eolw5_Gi52hiOuyRao1_e4SG-p9BhAXLe_WjWXRbSqcZCjAJ6sGutq8p5W9ZrRVsbLLkX9s01FQgfSE2_A5n5MzlRcRwL-Gn5foObQMy_AKs8HhXXWDTjLGi3QH5V-KnS3cT71MKQ0wtAAGWEAfwUUXaQiWnP2QOWgY2dv3hejMyHO_VZ5f0Iyat2ipIPZ-q2gxbuQ-6e5PU_zZw7l3YfH95MQ8WXCScY51L9T_UMsSZBCnyqGsjy-JqEv7FLjMmfAB3jHJBY9yEsQZE4tJ1nXS4dlGvO9n41nXmm_zL7304z39gtuv84ALIvM90tXQ3gBBR9Ru78y84J5LF1LHp0wq-sL7HVImZidz1m8aruEhgLWGPunm6XvKuN3nEupuMdhfqHcCA6HDQSEhbPAS88uBnKe_8rNC-6RePkHjxe530Lg5xdRoDBBWwE3iP2JTHGIOVQ629K1pLIoWPzKsgzFhzlg7Oa5-BIDB2E39TyFP1J9jilgbD3zcWDRJcfvlA2Y43-6kWM_FFX8ttlbFSDQVstUwieindtACLAvt-WwBpCr3ldx0_4cMDWv2xi7rj7W5PS_rDHvfabD0p1xkg11DpiQy4iwiWsi8FTOsKR-U9qf7xZx8Gp1xtU07GJCvwKthXMWmesPIXyFClLMgRWQYnLj0LfBQKvV5Ya6iAv6j0QeMPhqIeQq6QctIV1ZX1hyKCDcWLuWwNU3VsazeEDtjDk8XN_r2hIyjk_H7Zi-EuUKxuNNffORHJrazozXustNRRe5rLe_BcsLkM9aQrXVQbLm0s4eXKiqRlXpFgZT55FxjsboLR1nvIxjB8Z-qIT51a7Rir5zbFJx8a_VKPelD222yfi3d2MizpAkEkpZ83TGuk3BAhzfgSLRDVuOAVLjeBxsWcAigHfdbm6pGVqDz9m4SDErEYLUcYWwjoFiQKhvpL2TH5BiaoLmWA0yBheUZqKEVKlJby7zhXwaruNP5ix5dQ4mHkBIqGT9kdeA_pgSCTAre7xljHMJcdgtJv97OYWy2SBEJCxgo2OKpZJ-ZlxhYlS61Pw_m0Ggyo98OZUIMSSCwtK0fvN7jcvowzbGaMmtgLeZPh-pVEfX7yf2gIoX9Wb-nMF20B5IEvzy0" style="width: 100%;">
                        <p class="caption">BERT의 구조</p>
                    </div>
                    <p>
                        <br><br><br><span style="font-size: 20px;"><b>2. Embeddings</b></span>
                        <br>이제 BERT가 왜 encoder 구조를 사용하였는지 알아봤으니, 위 그림에서 embedding 부분을 살펴보겠습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">먼저 embedding을 살펴보기 전, BERT는 대량의 text를 pre-training하기 위해 두 문장을 한 쌍으로하는 데이터가 들어가는 것을 염두하여야합니다.
                        그 이유는 아래 BERT의 학습 부분에서 설명하고 있으며, 여기서는 두 문장이 들어가는 것만 염두하시기 바랍니다.</span>

                        <br><br>먼저 BERT의 embedding은 3가지 종류가 있습니다.
                        <ul>
                            <li>Token Embedding</li>
                            <li>Segment Embedding</li>
                            <li>Positional Embedding</li>
                        </ul>
                        먼저 아래 그림에 보면 두 가지 문장이 들어갑니다(현재는 [CLS], [SEP] 토큰은 무시하고 보면 됩니다).
                        그리고 세 가지의 임베딩을 거쳐서 최종 임베딩 된 값을 사용합니다.

                        <br><br><b>Token embedding</b>은 말 그대로 각 토큰에 대해 임베딩을 하는 것입니다. 즉 임베딩 lookup table이 vocab size 만큼 있는 것이지요.

                        <br><br><b>Positional embedding</b>은 말 그대로 위치에 대해 임베딩을 하는 것입니다. 즉 임베딩 lookup table이 max length 만큼 있는 것이지요.
                        예를 들어 input max length가 16이라고 하면 0, 1, ..., 15의 임베딩 값이 존재하는 것이지요.

                        <br><br><b>Segment embedding</b>은 이전에 BERT의 input은 두 문장이 들어간다고 했는데, 문장을 식별하기 위해 각 문장에 할당해주는 임베딩 값입니다.
                        아래 그림을 보면 \(E_{A}\)와 \(E_{B}\) 두 종류가 있는 것을 볼 수 있습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 임베딩 lookup table에 0, 1의 임베딩 값이 존재하며 첫 번째 문장에는 0 임베딩 값을, 두 번째는 1의 임베딩 값을 거쳐서 더해주는 것입니다.</span>
                        또한 위처럼 2가지 임베딩이 아닌 3가지의 임베딩을 사용하기도 하는데, 0은 padding, 1은 첫 번째 문장, 2는 두 번째 문장의 임베딩을 의미하도록 하여 사용할 수도 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemdGgM6Z4NhnWBoEqTkfnsktPpmPKkVMX_WCRY5FszaRwvlD_uRULFWb6uakk0hvjrYoho_oNl0XILOyQXClsbUBzS6cRhuduNalMj7LTuV037XT1EFSty4FwyVTf6GGafo4BYSah317zbdArxSVn7saQnARISH8m_593KuOQsyphKdpqQwY3r6Vi1xsaR1SwKPvIolQOlrZ3c5bajIfXanevt8mQBe4dOaIXFPGEpo3ph54ytzBXwCb8flcyjB04yud3TacoilT5p1EzTD24_sC0b3s1ks5lYHJPGDkEgV6aswWnMtPmEVFxkiVLXSXcpCex0S0U8I649ZCCuTf6EETRwAbFNKP37J3TCg8O_fYKL0URhP_Hpw2CopKD1yPT_hrC7Ft6mzjlxH-8DJglHrIpzow7mCfIlW1EZZPYrYqBHPHy7WMnnDnv68PAEvbGqXJ3J7ePkV3vNCVXs7UshK66-JvtLNFmqz1cETVH__c6P1cXGZgBqycDQEI_ORBQUSj6RjUJhFtoddVKRADrm9o075mI3UgCgpxyigOxrIsY6TaF7Wx_kC2Xi6FBgox57-uR32vKfE_iMYKjDwGAiDPMjUnE1Gev4Mba49kiXbMPB27WDhPoV94RZ3DT_lNyTYZpuSTKl6lPZRhLa2zkJMh6uCrdJSwx3v2CajUlySqr8ummW7e3tbIA_8dlFShLhQdV4kx9W6fmM19WVttq7DVb3sT-0cf02CDIflYuORUsxqFXn4xmPKSnkhKRWcFa5iSI3GoqG4ApwyCsLXq-YuRgrPb16RDV3VKCkhfD4i4o9iRR7z_pUFX4jI1LqWQ9jS4BLG5OxybIveWNadYBF2hHtxNGj5NH--B-nqtYaeGbpR9_uKOUkhRB0kjbNPxiJElEGkcs8PIp1s2vbUPRu6oXWu7LCN4HvUC8T-HHf36BAqg3cGvDkawtFOpEdodn69Jbx9N5E9QEuRdJBdB0Jjp3JIgdVK8pqEy_jk0oGRlf4DAPs5fBq6Rrpg3lJ5BNWQwn8hW6DLGSgVyNbRrS7S1-EFqnZnBgj3iHkveZEEbmwibLu6gv3lAiC57pcgX0Cz612KfIBl-mvK3-G2EJTw7wlC9ffkZPPKSVy4_FlJ6fMVFLuM2zZlFaIQuuoKWsSAUfRBpbLeteEHtT0So-l1Nqy-Nz5SA_YQSfB1FCFER4Hqezz6AgKQ7iFToF9KvlrWxBZ610CwrNF96iITcQIRUVXcc9dfrwllKouCyvAdzJn6vA2Y09--A4L5xtRMQc3-yjTnsup-sG1Lzf-JVH5uLA-lFbEscvUXvyX4fCQQBTmu3rCRquQAAD0YjxvWh3GyTR4BFixA2RDNkzDxOyZ_wocC9qKjqPvoUJiDillYwFCOq4_6MEEBEzbQKB1tK_7NpDRysR7BKodcZJyvVDso_nx4DiCuWen54p69Ui64WJE3FKzEiUuU4G4ovVBwFzF3a7sN5wU7KO8lzYD6MtTY" style="width: 100%;">
                        <p class="caption">BERT Embeddings, 출처: BERT 논문</p>
                    </div>
                    



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>BERT 학습</span><br>
                        <span>BERT Training</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>         
                        <span style="font-size: 20px;"><b>1. Tokenizer</b></span>
                        <br>먼저 BERT는 likelihood 기반으로 BPE를 진행한 subword tokenizer인 <span class="highlight" style="color: rgb(0, 3, 206);">Wordpiece tokenizer</span>를 사용합니다.
                        BPE 기법과 Wordpiece tokenizer의 설명은 <a onclick="pjaxPage('word2vec1.html');"><span class="highlight" style="color: rgb(0, 3, 206);">이전글</span></a>을 참고하시기 바랍니다.

                        <br><br><br><span style="font-size: 20px;"><b>2. Input 데이터 형식</b></span>
                        <br>위의 embedding을 설명할 때 잠깐 input으로 두 문장이 한 쌍으로 들어간다고 언급했습니다.
                        이는 아래 설명할 BERT의 학습 방법인 NSP와 MLM을 학습하기 위함이고, <span class="highlight" style="color: rgb(0, 3, 206);">단순히 토큰화 된 두 문장이 들어가는 것이 아니라 거기에 [CLS], [SEP] special token이 추가</span>됩니다.
                        
                        <br><br>다시 한 번 아래 그림의 문장을 자세히 보겠습니다.
                        아래 input 데이터는 "my dog is cute", "he likes playing"이라는 두 문장이 들어갑니다. 
                        <span class="highlight" style="color: rgb(0, 3, 206);">이때, 첫 번째 문장의 맨 앞에는 [CLS], 각 문장의 마지막에는 [SEP] 토큰이 들어가는 것을 확인할 수 있습니다(그리고 보통 input 길이를 max lenght로 맞춰주기 위해 [SEP] 토큰 뒤에 [PAD] 토큰을 넣어줍니다).</span>
                        [CLS]는 아래에서 설명할 NSP를 수행할 때 사용하는 classification을 위한 토큰이며, [SEP] 토큰은 문장의 끝에 들어가 문장을 분리하는 separation 토큰입니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSeme3m93rqsnQGvdyCtdy4M25Fa9NFN5qIFD6ENlcgM8j_6hJrhPi9GE4SaDLjFJ4_Hk_Jh7znaqXvTWl4CJp_4vXV4tJAsxgo2zebUsM1lAUo9effv-avsYtI0tCLMidVHzNU4NsBy1qffiDBrNMyfnWFNxPjguO06IzNC_3KOyayNjdlspNI9ER5RrCr3L-XKL_EecP6GujQ4tNjz7yebEpO1VXitc_IN-99K7HdeoaR0hqlL_rav9PT772gGOkikJTQub09onqXpso2RMWPdKwsWWzkZLWEg_t1wnfdZd0ufhSgH5kufmSDlHAiKmHTnHcyHVyB5lJcBnCXMi4SA12mR2INyguJ4TG1N6SdLRS8E1RBlUqUcuZ8wk5Qh6QAKV1abuhqyZmPTeX-MEJyFEGKmowNd3_fjmZm9ZE2V7DpuXbluAMC2JSMbo_G-EBuaJjQiNyTxiFbpOptcbLHALGG1CEhEfDmg4i3ZHXkFlhl3GJj6cilhjPXCWVfqMhwnyxONf3q08xc5eu9Pl8x9PX2quv7GYofTiiAqLxiVstoTFgBywSHt05ssLAZjFA-n_y9ynmqKQno0__AdGkPngWpGXjOVO0F-A5k3--94UAzof-r4BEvDLJZNnhsIYAKyjxjKAAUkDv3V3ohunlKBWhePM2zrInkFlROmGkQmjeW9cg_d2oFwEiSDhMOBXCII4o1q2zBXZHojoqb6-HcCv7_6qY82o6j2MAahs6dIrdkuuVbp3NGmaIM4jCZ7fRoq5sZlR6-P4XC_JVSLCpaPzSFfXxQOdzZpet1zXq1t1ZZhu4sQfOJVbd0PSlU1ICpRWw_DRazyszpDKAalmJtfi-usQ3wMKr6wLuLSZLc0TTugLnI1L52G_JN2Tf2SDPo_FWPwq2KYym9CL9y_vyVzqgtxbjLHvu60rNsJ0G9JqisAUwWkd_jlFgziXctJwQGK2I0uKx0NrZiMfpOyy3tBAF2kd9YxJSYlTw5Wl6HR0rWQREBN7rFSCWKREn_T48VqsOF6DtG0iaa_lHTWYuTF0XJPphuj2rewUb8c5y_roYllaqS5LfBFKrT68UgU6k6JkYDeI-o2oXv7yTv3NMo71MDWDGarHZiD-PKoT6cn2ABkK2h6LZZQ29u6E0BisebwFcGTXnfYwjAIQUR2zVlM5wiB0acIlV7ZRESp2WhlVesGiDSQyJZ3iGmfOUKx31PTZNNCiMXtKe8juvqzG_jz2CLrGvVzN0pwjtgs8G3R7vO-ee4MDZWUz7SjlB2cBXLZZ7kMdRFuFLpjkju5UMk77wPqYzsrok5o8i3mJOn48k1_AemxwT9iJ2xOJDdLrWT8jw9Lo06-yD3jBJKQ_fyN7mIEfQzyPEO4oKEh-KaZwx298zeB22kUUhCFogddGQTmhqwKg-gpefgkds3_IM-dscvdXOC6C7Phos6eSm6T1IA9LRgbqZEGDgOOz4coAAKG7MYkEEVv9fR5wBDJQXK1s" style="width: 100%;">
                        <p class="caption">BERT Input 데이터 형식</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>3. Next Sentence Prediction (NSP)</b></span>
                        <br>이제 BERT의 구조와 input data 형식을 알았으니 모델을 학습해야합니다.
                        저자들은 모델이 text의 문맥과 함축하고 있는 의미를 잘 파악할 수 있도록 두 가지 학습 방법을 제안하는데 그중 하나가 바로 NSP입니다.

                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">아래 그림을 보면 두 문장이 들어간 후, 맨 앞의 [CLS] 토큰의 output을 바탕으로 두 문장이 이어지는 문장인지 아닌지 예측을 하도록 훈련합니다.</span>
                        예를 들어 아래 예시와 같이 "I love girl friend", "And she is pretty"인 경우 연속된 두 문장이므로 1로 예측,  "I love girl friend", "My car is broken"처럼 연속적인 문장이 아닌 경우 0으로 예측하게끔 하는 것입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">그리고 sigmoid를 사용하여 0 ~ 1 사이의 score로 예측하기보다 0, 1의 2개의 label 예측하기 위한 cross entropy loss를 많이 사용합니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemf1JLqQzi9Vi5SfbM-Q1zzSIGcQ3Bygow1DWeAarHsDoOb_N6QUUQqprl4kNhzamYTOMSCTbSHT0BLPFg14YjEUZEtcpSO-wVTPpXUKe7sXTnrSxETEjw1cQthj1z1TJ4QsIgzuOBpGkx_ZPL_MjnwNqxB5LoNVyJSGWwJCIV5ZP2fiwUbvqM9W1v5RKiI6rSUIDVIdAnAf5vVbfhDINPsjFZlQ-38KLDghEnM0iPMdK4_J951KE7GND5qFTrUf627giwADX5vWhDgFSlmrFg_idZ8lQbVS58AmHYf5XTajeRjJFuqY6mO-JpcL5veBbPoJsdihggs9UTC9Si0hFcJDCu1bOtPcHcz6lH2CIyINI4hocs7LnMuQhnbWPkwEiLzo7V_IZ3l9CxkgVh7ByGDa-5PYYQbTtfZ0cOaSnAG-yPYhHA70I_XC8fYvNsZ-tF4LFcl2GYMcAKf4NFmvS_9JsItf0gQ5vqWbUAXLlvbmlJ3uZoo2NQWPj_2meVWz5r0spp7DHXZPOaI-Nc2pAkAcm9jN5cZrbpr7joVco-wHZG76xsUEQrtwVputUsIK9i_maVXlaZr83bEuGWZAY-GedX9mnUXBrVq4Cj_gVL-T7jNjlIbfZ41U3QG-fpP5Y0xh10IxdYFRgyBHzGOD5tlRHzneNsOReAZXOXVzFRT2BvyawWUabJU9MC7OfWNPuZRRVqb2dJnXC4Qhndb7pq6LqsLcIvDS33B3-He8uuAVFf7-qy-69bHSffMviY-7Hwh90j2H1y6OCsxVhBrW_iDZOMRctMHR0JijG5Bw2iWd33WQ6aELgQ_Y5wBxVLevhYAWgkpsqBYiZi2zlhk9jZ1BjfTOv4bTvU8OK28u5BOxMyWe4nHb9l7823Y19KBEwn7VDupMaUFkY8caXoOa-fbNCT8WobY-Pef8ankOlBVqRYAUnvJaTa46Vv1Caf79lgYTl3H7MvDIeEWZCI0lXIhuC9NcFVxvY8rZ4wxB3tJrCDTd28qWMPp3Ri_ULTGY-Sy5BCezWa4AiEkTOAlUmrT4s_qBaQdp19qhQjK9q1P4VeqzsWNUnDCt1YGT32vb27QYtF4l1OzfxMBwSDytbIpkpp9NYb2pzgwKcyR3zGA5Zh7yASjS83mhr2hAiKdg3x2KsAsNO8I2_zJAB7Qf-NXsDWhTYoNumQuksT4uW6MG7DXPxcyPi38xOSkq4ylLxESirp52cLhrwE1MDKOqHCU2HwezyGiEcZc4uHk29OAWkwtveiKhuDz-N7xTBuRlA4B9cqJFPvzH5iI8iDhu74vAJZajNLhrVpPqMcPyNlx4TBkccROyl-zvRF6eJeMZGmfrF5TYqioPSKIkChAYHMFJ5esM_lW8HRu-6QFG8uI5Z_KrtxjRUhKRr8YgIcolOCyOrL_GlJclKqxkUi8MIevtkhRUfmrb-gRYIP3-kMbTdaeLv48AanbeJsS84DvsGUaf2T5tOb6puzxBIOLq084" style="width: 80%;">
                        <p class="caption">BERT의 NSP</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>4. Masked Language Modeling (MLM)</b></span>
                        <br>BERT 학습 방법 중 마지막 방법이 바로 MLM입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">MLM은 input 토큰 몇 개를 random하게 마스킹 후, 나온 결과를 통해 올바른 토큰으로 예측하도록 학습하는 방법입니다.
                        Input 길이의 15%에 해당하는 토큰을 무작위로 바꾸게 됩니다. 예를 들어 [CLS], [SEP], [PAD]의 special 토큰을 제외한 첫 번째, 두 번째 문장의 길이가 100이라고 가정하면 총 15개의 토큰을 무작위로 마스킹 합니다.</span>
                        이때 마스킹 하는 방법은 총 3가지가 있습니다.
                        <ol>
                            <li>15%에 해당하는 토큰 중 80%를 [MASK] 토큰으로 변경.</li>
                            <li>15%에 해당하는 토큰 중 10%를 랜덤 토큰으로 변경.</li>
                            <li>15%에 해당하는 토큰 중 10%를 변경하지 않음.</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">위와 같은 방법으로 마스킹을 진행하며, 1, 2번 케이스를 포함하여 3번의 변경하지 않는 케이스의 경우도 모델이 자기 자신 토큰을 예측 해야합니다.</span>
                        아래는 위 3가지 경우를 모두 적용한 예시의 그림입니다.
                        <br><br>첫 번째는 1번의 케이스로 바뀐 모습이고, 두 번째도 friend라는 토큰에서 juice라는 랜덤 토큰으로 바뀐 모습입니다.
                        마지막으로 is 토큰은 그대로 유지 되는 것을 볼 수 있지만 MLM layer의 마지막 결과를 통해 자기 자신을 그대로 예측 해야하는 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemfBLGO1RKIpiniDFxYxbXCwqgePApLWDx6797jPge1W7fw7E3Kr0TgexECsKXCzmtzgxEg0A-M4cW6QfbbQE9FGl18biwP2zVhNFShF0uFwk_5-humW1ZwGiPtDYxszy_dxV2_ZtNygK_a0ZeTpeG1A1_dpVBwjVmjvCyu-xjGTTaaqE4MuChUytpV-Qnv0W0dDUmxV7mctB_ydhzG157h4TI5KSiR3M5ONmPgiiW0hYIU6MLm_VJZqmyOOp0s1ZdtVp97CtbeN_fKzc2zhkvNvaKQHYsjhhOFh4-MPBtxj6h9FsV-amaMba-RepjanS-kHUJxi_CbLegdvyVLneQnKl7bDlriA5I1_N8UDYdFNJZovm4RS4GjTyCV99rUXIisMg3XezncEOY5qsm81sHG-n6ImZ0ptgmv_9jRQFx90gOzD58VPkAj3JXfJUF9dtgohRqTaK2Y7mWU0DNHNsA6PoyfMXIjcfxBeEelIAxOg5n5wNJUN84-zq4paEIp5_yISIyoKOuzEbMdS2NyZDNe63ZYtf1GspDY7IbNX_HbHAzunPWmRGWNl9ifNyTy-esN2RwThV7hojpNzkiTXvbAIyvK_QcX6WN1S0w-0DQrCN643Qccc-reN1LHkb-2pkk3rNoyMFM-G7uiG3oo2Lfnw9o1neaJVOl2H5xw4cTrmoFd5KV8buzKVlryCOjfILeEcjTW32Lnc1o4xDxbSVzKfihG3lqg-pW99H0pwlWToGFm0WXYFXr1Gcw4KQH2mW73LeIcAw9HlfEGNvaDP8WcpEOLkYOTpClXUUYU5MyAnSWmyB4QAlcTRR_6YyJz7JBc3xVeeXvNSEd8dPh16f4wHhvlICotRo6Q-lqbE1HqTsKU-p6mYDXdzxK5x5PCY32oERAUHQv0IbAXsOPKv36t9IttaoQl0wz4AlebgvXiqv5Pb869soRTfrs-FKH8zXXyMAr73-QTpLaBNMeFRduLrMOmCaqbQTch9btwNe3KM3bjctKw47dijcFKS6IhoeD8uuYER0qa64zTG_N8_iS0Inf_nkexbMD_wM0PdXQaNP_P69J8sq9P_0LjKXEjQ3ORKf7DOcFTdLVH-fkNSkNtZg1PDf3da2N5NTtO_1vcaLQkwtRwfZcKUBM4h3_qdLmRv4M9ocFa5NlDaD_LVFF_IC3VKNnlHIqi4R3yJ8s2tXPjhTSA1IdIY77B8qjmN7r412j34-1pIYGf9hifiETvdx5I3rVuZD5RJ88RzZPa5IvbJB3NjurSQ1zoAIcSwDDBk1bWmwMu-jddQOC2I6O1WboqXcOD-QxptBRwzfgMztHFZLjVYAP6kebA4PTCm5EupR5Y5oPGl4FR58GflPNMXRnK3a9inX4AcV2yvTJUv-g3NG3m-7LBnpIXlVpYZVzC6QgXZOHaj0f_-M7-V1QzVf_at-RM5-NrYOLDH8j99kaMAUi1SFCXQSq1Z0qk8LTqhq-sb7-7hgsBOcsKlLsc" style="width: 80%;">
                        <p class="caption">BERT의 MLM</p>
                    </div>
                    <p>
                        <br>이제 NSP와 MLM을 종합해보자면 MLM은 토큰 길이의 15% 중 위의 3가지 방법으로 마스킹을 진행하고, <span class="highlight" style="color: rgb(0, 3, 206);">이는 두 문장이 연속이냐 아니냐와 상관 없이 진행됩니다</span>.
                        <span class="highlight" style="color: rgb(0, 3, 206);">즉 두 문장이 이어지는 문장이 아니더라도 마스킹을 진행하는 것이지요.</span> 그리고 동시에 [CLS] 토큰을 바탕으로 두 문장이 연속인지 아닌지 학습하게 됩니다.

                        <br><br><span class="highlight" style="color: rgb(0, 3, 206);">그리고 추가적으로 2번째 MLM 케이스처럼 잘못 된 토큰으로 마스킹을 하고 모델에 넣게 되면 모델의 학습이 제대로 이루어지지 않을 것이라고 의문을 품게 됩니다.
                        하지만 저자들은 1.5%라는 적은 토큰만 랜덤으로 다른 토큰으로 마스킹하기 때문에 BERT 학습에 피해가 없다고 주장합니다.</span>
                        저는 오히려 모델 input을 perturb 함으로써 모델이 robust하게 학습이 가능하다고 생각합니다.

                        <br><br>마지막으로 위에서 소개한 방식으로 BERT는 25억 단어의 Wikipedia, 8억 단어의 BookCorpus 데이터로 pre-training을 진행하였습니다.
                    </p>
                    




                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>BERT 성능 및 Fine-tuning</span><br>
                        <span>Performance and Fine-tuning of BERT</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>1. General Language Understanding Evaluation (GLUE)</b></span>
                        <br>모델이 자연어를 잘 이해하는지 평가하기 위한 benchmark 중 가장 유명한 것이 바로 GLUE입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">GLUE에는 감성 분석 (SST-2), 언어적 수용성 (CoLA), 질의 응답 (QNLI) 등 9가지의 task가 있는데 BERT는 이 모든 분야에서 state-of-the-art (SOTA)를 달성하게 됩니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemdRVGJ32GWLD7ZzPjA07bwhmM2ky0dyFB-g9AECn7DWCMnEraDwGNr8XNbuRXFUWhhsaSWVm0-qrGEcZi_6OLgxSlFU8mdSeE3OUn0bZKVYa0uN-xghNj1RZazrErP6P9Xtoybg2oNylLaUKsIriIEkzJsxBMojPt3KYz0p2v6Pnqn3WUiS9rd476KgdUerQAjIi10nxbUsK8dxmhVy3FeLpNYwOcxy20bphkSBA3NybRFR7DDzQmaqcal0eVwBIV5c6jm3NbjfqTH9DOEd4AbwVimGZF51CJMOqF5M5frrn3Zg2hj6qAsxG67mxOny0YqNYaEvvX8gl4NdTZBM8ASi4EZbm0M0EuYQWG9O6ZtDxiI-tvRNq0571FebHe6vi32i3HicrNYVHhPBeWxdkEtRDEzA_YjyHLY2MfUg6cl1OGKo8xZfGtdhunCwgk6Sl1njPr_y7nxnS2Ngmisi9ws0TQqOJSB6_8Mms7afJp8bFtr4HHZnodupiSL5B9WcLDA3Lia5yHquQmB1QW004yyzuSl5B_wGfm83DRF7U1_v25WzrqWjpv0HBNQ3w22twPrvRr5G90NRlMqqPS0XRioOLF00qfHnUuBmpWMQyZkYz3YY1D_0tCtlFusehNzaID0QQuH18SIbyCcu3bAHqcZ7_EdB1h1kSB6zmCq3JPS-1adUZQeeZYB2SKg4nPuoIvCJnBh58pei71pwEbHnT-lvQsAiEGeoOMuKoTc_aRe3s1SLtYNWMmZPIKAMhUry9pgqNAQLsZK1uFb4IniDSud-zEWuf-OKWrNhd6x1vwJ1EhaELIy31RRFXBEbrufyz9hGnuqfCYsHTKil9_6ZjnS2UTMAXbx0deFmUPJDu27sw0omkawveRF4_cnRtRdhpNoqQnRyRSfdLp9F7nfq4srZ647inuPxbMSloonYQeU5iPiwyN-YeF3eDLmtKeAwzkU_zSiLIkwSR_R9ZKJRlk1bCwReGYFH_scQEVFYTkn0x1CnBvQ8b8oDveiuQ1N98PkbCb7wQLg109tDnyUMjN41w8v6vw7MdFgUtysCOoLvoIMPBVneRkgyQI19JchmskNmRy7ubBaDJG4QKJfigwraUqhdRKOgbkBuQnczP4OGm6qMbeva9YoIJXHNuAOCCttFciDf-INFFpof5Bg-DTK5jWtZ-eKEk5hkWk80alB3NSAXU3ZsInoxGP1FZuRcJAF-4alQQLOL9WcMbhNZURqerdLjl7o4GJdEkembws2dp57uDWI8SWWF425a6CXYizYau-v5yWYydwKMyKFvqW0an15Eelun3TPnxJq2QSpTZ-_U15g5pynseOp-XXgVS4V3ZL8IdMgfcQvaVGNIz2OO6XsteO_KYIR7pRTdo3fiG6J0QgknxjORnaO-I6kkxKbFuEKga3DreD5sgoijZRwqasXC1ja9r5wdCpn1R95O0zQr8ROD6hEIUgtMNpgCtu9pzKMYV8pJD-aNaPzeAvI" style="width: 100%;">
                        <p class="caption">BERT의 GLUE 결과, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>2. The Standford Question Answering Dataset (SQuAD)</b></span>
                        <br>GLUE뿐만 아니라 저자들은 BERT를 Question Answering task를 위해 SQuAD 데이터에 대해 fine-tuning을 수행했습니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">먼저 질문을 첫 번째 문장, 그리고 정답이 중간에 섞여있는 지문을 두 번째 문장으로 하여 BERT에 넣습니다.
                        이때 두 번째 문장에서 첫 번째 문장 질문의 답에 해당되는 첫 번째와 끝 위치를 찾는 task로 변환하여 fine-tuning을 실시합니다.</span> 
                        즉 질문에 대한 답의 시작과 끝 위치를 찾는 것이지요.

                        <br><br>BERT는 이러한 SQuAD 데이터에 대해서도 SOTA를 달성하게 됩니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemecJvCcTszVahPAFEqACwEvfvBR8l7db9ODk4SQgIT2ekLlwWSeFkkoD1FkMCcRYRCI6t3G4RLaofbe4xIJ4JNAkINSnEADBlg02RqU9n-VDDfM4CLdVgzYSA2B_piSFs7dfPCqXAO2fBsuDfM59HJe3IDbYsM4-FsJ7RvvZP5QsgLEWUDsehms_8vmf7n87HIb_HbhD7Z3lRBPIeeMYkctjIMZtbpdgLJnL35u-LMRG-JZZoSyiM15Couyv30HJH-maitngsST3RrRk8iU0VWoviFw_EoBrOy9pwHxACz1CNa0ZQIMKinAYzq6T2J1Defa8i7qnAHlUcEykEk7VGz7RhYHeRBaAPDK2MS3kXUtkoq3CUqWjbjUjlmpJmKy9KuUY22wnZBAfOQT2uD4Yf27OTjpbWHvN1TNRYTWz7XvJvO2OT7EfGixBdx8tdTr33DZW-fYkUAa-wrdR1EK8PzO2Ho5apCpJ2CyRiqpdxYrGTFGNYJeASh15rmi1GBPPG4Un4MOGozUeSjMYeIS9B5FzB-AaHd45ocRNsChb58VjH1G_fVsZ8GB-nMBCHQHpU--dGWyHnHU_hDn_Jfi9FIDJ5sKEdp4d5Bx5pPvxEyZwaN32ZK8A7nniL-1IQBbxqwMSNaL7V2A1fq7bUTxxerCkHHW3LrWsz14CQqyasb97jHhTkbe_ZmwTtWO2PSc3i5lTXKnIxtxcrW6HkMjgQw2b61bjyPrHMyaRYQ8ERpx1NrQH4AXGkNz4w0pEGO_U3FlT3tkcI3j9C1xN7P2Y8XuL8FiNDJWQxzD0sNjDKfTth2oSQ5q0FPnlnU3NvjuQAfZjF7s4sHoLW8DviICYIyhrJN9JLlD4SrDwpT0eRYT0_tZ5PzZQJw2HKFKGycITc7yfuzjwXZCiqcy1WzDS626AKoQKadyGkundxHqeH_RkItheriWsZgyRnLwbxeP6OkcU6Gufz8gaL5a7Mkv48G2-jMZrRb6ZxU0gyUkXL7SE7U5MiNw9g08x_2xlTzGMbJY9lukXTMbnp-fv6erSx-xCpr1xZ3zOlkjmD-IH4M0CWopWQd_EItweq8OCUls4TmSxFu2sRWEDTRaU11pJusUvuTkpxJjzMSxsJ_e0BHZp3uS9a3qTFpJDiXzPt6oterlyMCS4wzqcvDtEw0znZ4m_fidlpBQyn3fZDBge_HQKYs4I1WMFMJ4pNd6HN1zQOWhQZAwsphkr0U138EBUmtmhw58smrAKWlbp34LX_Y-k2ZRQUgsKLwdyauAMaXZ2aLfXFscgkf50Jwx05V-zjdBRtbb3yPk_HopoQFQQbw1HKbQjaeOx_nuBzkuh6lZ2oSermjXMSwvbCAl8VmlzexMjjSXRKIpIFW4_GXjFCBsZCvQb1MTXWnGEZUL5alVNIOY4bNdcmKxV6z7nj5gjvkYLtqiJrruaMoogadcppYomYK4Yok7crtRgHfVvGCpTYU7XNAMOz5EC4-62NavZTQ" style="width: 80%;">
                        <p class="caption">BERT의 Question Answering 방법, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>3. The Situations With Adversarial Generations (SWAG)</b></span>
                        <br>SWAG 데이터는 주어진 문장에 대해 보기 문장 중 가장 적절한 문장을 추론하는 task입니다.
                        보기는 총 4종류의 문장이 있으며, 주어진 문장에 대해 4종류 보기 중 가장 적절한 문장을 고르는 것이죠.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이때 주어진 문장에 대해 4종류의 보기를 모두 concatenate 한 후 BERT에 넣고 fine-tuning을 실시합니다. 그리고 [CLS] 토큰으로 나온 결과에 softmax를 적용하여 점수를 계산하고 정답을 유추합니다.</span>
                        이때 BERT는 SOTA를 달성하였으며, 사람보다 더 좋은 성능을 내었습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemd_NWuZW_8ZuADJNL-stoyvRSD_tL3lkXLU4x3yxytiNEHwkDEgtGgbBJvTRU3gsXQHf_hJCfTv6fe1Aavlw4PsDLtgYtVRQ0TyuBzOoE681q5pq4V1owEcXEOAEs3OW_bAo_dpMnCpamIAuJRqFp4lEEksil8LuaInuZx9UHrrgTaw59DXnqeRleDD5bdS1qDN2DqCHAJpTTNMIlR8YUhYqq6M95t1-ACeN31kalSc06eD9XrhhUQjQ9N-kwrd6CKOb3neWLd40hv1hTuj4VOnYPZUI3oIGHrdKniseLo-z--248TR9AVcFrvbXl94Bk8aRhIuO-03bZfZtW8xoR3erQQlO9Ylq8XOcAZY9FUZ6L5UvDxBdfqxXYO70ubqzosKFrgNAg1cY7w47IhGVtSIjij6D6MlWG6zG_J4a2fGp5EzjxEuvGkPJl7fnfP0AikhEYKFMBsHpbrmfb8pcEl9w0scApJOnWhsgm958aHcFPrSVnqkWdnvrE817ve59xBDE6xEkWlGh-y5NesR0TfgZOx7I31W33Dns6-pBwxJMmUO1KrP-dqKFG6AIJkh3eJgGbDq9QuHXNWcctO1oYMzDEg1rnaqu4WGKjMHWU8lc8ZEFQjo2do6lAcmz-uVow1FNe3xwNsU6eYkmPkg3sFI1cdZSEtNWN0L79cOCA0F3gF3sIvQdhA_IdtnahITnMP9TsHrMsWSTi0q1Yz5T4Inbx1rOXiuev2x6q0QyCKACRUV7jjcJfk0ojaiRjNzUJi6yrd7NTFJJYfVPjATppGJLGg1AehoRkJ6rXZF30vX3-PddWs8bzKYz7cUs1f5_YNB-7H7pT-TPSHIII3qrp95NEfk8KRRQ8rVS31XGWnzrlEUihO99XdITkfwx8Br96O0_1-6FlNZCc9ZLQt3OrNKrF7lLuhmRAX0mPYfX-28UFBtaKo_ym9ZO0VloCdOYsL2PL5jqXhiIKF4sLibD6aXPewDr9dLXutHvVIoTf7lKyMlX0HQL9dWgb066_RdcTaPyFaah94p79Eij5mjeWbXv5EQE1bDl6q3tHIIuF3t54FdeDHnez1jNfCPt3LuuG7SOpbkwvw_2NW7_7hLBNwJhXAhQfgbgKw_e3bLn-Nv5rUK5PQkr4SOMN_5aV228bUOJe5TAc98nAO9CI6m-omPJP2qjM0oplmXSVgQAc7LXL3W0UxN8PB8XTZxzMc7WFJsjbqQra30G1fTzV1QOcAs2sDh4vpYX8WggFv2XIlX2dKG1mCqi7xagtVE7ksno_bIhCy940xJmzIoQ4yY_Z9OhC1Q-WMG047N1vF8B4soxkycbZKb2jSJ37yLr8vCz59hAmF5meiq79BqxfStsrvn-QAdFjZ1sx0LZ9uzEXBv8mu-eD3GohOfUS91KWglg2Qk76VhVdUUYAmktjcKWOklisJoVvznvSU_elvGLQVPSYZ5x2PVrueDmRiR4seCQyOROkm2ii8L0mNnjl35Zp4" style="width: 80%;">
                        <p class="caption">BERT의 SWAG Fine-tuning 방법, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>4. Named Entity Recognition (NER)</b></span>
                        <br>원래 개체명 인식 부분은 논문의 ablation study에 등장합니다. 하지만 제가 생각하기에 이 챕터에서 다루는게 적절하다고 생각하여 소개합니다.
                        먼저 개체명 인식을 위해 저자들은 CoNLL-2003 데이터를 사용하며 각 토큰에 classification layer를 부착하여 fine-tuning 합니다.
                        이때 BERT-large가 96.6의 F1-score를 달성합니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemdWgmxQR4yy5XTwFbUbfDrgt8Mq5FlZiP_6AVSxK45oa7285oK6zgkW2omkH7du5G-LhbaTdwK8U6dF6uqL_BYz-2ZmP8fryvchGqf6XnDDtOrMvG92fBw4uY8InTv3R4E65ZJhWbU1KUnW0oJQ4eyitT7NA9uDcVTmr_WhTBxk_8IuJHDjTH1nqKOrGzIRIKfW5vz1o2Bu088Gy__YrTNQBm98D0TBUDbRu9X5575aLmf6AUOYV-S_oYSGLnoz0JHH_NprbTa5mFbuXBcDKHxDU-tT_fnfHr7iBXZcYPmEEVPkDYWG9ISt0AFdaglMRYarsiHOdXnUPpswkIgMMtnO5OmUUk4BqRX0_NmJ4Ha0ervu-FjQO7KZd8lFDO6qA426F14V4w0ERJlomR1DiTqonGa-6ka6A4ygqR_7nbBZ7pQYvBymLudkvxATZxbe0BA61ucJrHyiXI_TpHeNZJ83ueW9rxX5Q6EDhgTnDlZXMh_MRlcRHUoWCjCSO9UDscTbJsJWyInM03QaOyXtayF7CIuf6QW39xUxgs5RvmTj2fvUGLcOuVdKPn8L5mPlug07UugfiUV9FJuT37eSXRBCWA2yf_vAtwA-x7AajGfb6GK1MrNATsot1FZQCEjS0joaF37Cx-arJyLQJD6r5WL73eZhX8TT6WkFCi1tYgTutaTCsYzWKwwSLkDYj2g0cyV1H6RTBNzMuLcPCGk4Bkd9GSdbP6kg5Y7CpI6wZkGcEGTXtSqffQdj2vcg8HQtFCQ5QAewExagKlNq20U-yOfsLIs3Hrm4g-FRzVJ8Amr3W5z9YZZ-pguDdDHO72nO0549qr6A5hKQNclThrkrf11jtVvnZttgG22CerJy2DcV7nt_z89iryiN_bNakM_5Nol91Z2lcybPJUWaSVtZ4sxgvMZ3f1jSEXnd1kW5RdfFDC7v93PX5TzrsHiKYSVDzwlExpLAtKGiZgGNrv-68vOHYyakd0SKhX2H146sCF88LYjSOWxO-9KpP8iI51vdBC43juE8cx7LdzZG5t1eQALS7I-qKxHikBHXVxxfRkNwNVKZMF-QE3qi7hmGEqGtTInvFrd0Q3OVxywxNWMkbav8vUHwBSUgiWnDqIejgtXgL7k3W-4rVEPPtyvI_VFfF0xNbZN-fSJTDx3kgBK7HLV5MqyzPAh1z07NmVnixur7lBxHEi1Ti6QImOytYFO1Bb10QGLA2iPVxvN0AetBknqpmVeKENR3M_0dGUiDlHHhOvz1BogDdMfdLMZ_FWqaKH9Ams41VlXncyfPGHExfy1AmOlVopxaPFNiwQOOJCPx72-Nj0_99IDceTksRrCz7jDq0H3n65iZrk2WVDJg4aB2U-ZuWQpGRcwvBoTnztM5VFaHFEIUpJnEzozv0xigSuzDbHlJfXK4wCrxN5oATYFQXejRKk1NfJMXOWIjLyghaobkU5X9gW_kJDQv6uwFkZ2dlX8capitQt_Q8DLpFhU" style="width: 80%;">
                        <p class="caption">BERT의 NER Fine-tuning 방법, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br>원래 NER 부분의 내용은 ELMo처럼 파라미터를 고정하고 feature-based approach와 fine-tuning의 성능을 비교하는 ablation study에 등장합니다.
                        Fine-tuning에 비해 feature-based approach의 성능이 조금 더 떨어지긴 하지만 <span class="highlight" style="color: rgb(0, 3, 206);">computational cost의 이점, BERT로 불가능한 NLP task를 위한 embedding으로써의 활용할 수 있다는 장점을 고려했을 때 feature-based approach도 절대 나쁜 성능을 보이는 것이 아닙니다.</span>
                        즉 저자들은 fine-tuning 뿐 아니라 feature-based 방법에도 BERT가 좋은 성능을 보인다고 주장하는 것이지요.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemeX5G_OD6iPW2wjVS3nOmmU3PRjw3q_PqD8q3uJf6tFuTKjtHgvijWjweOW5wKEmrlj6fANn7gN_YpwBD9JfDp6-X3qQW_Z1zRViZ5XeXRWUJwiss1WDXFsgUOZAT818R7RLioPDtamdv8ZRNyxKUHp7MzyomlqZg-LtcDsM4aLTWj8bLoCZzQ2Y62XFlY8AZljnwEKiraNoKBRvKtXjcptgD-XzW1HppZjALWQG7X1B_FHbAQLPmzi9L19RC95ufADo8bUhcrLRDBqDqm27hNcCUh2qeu6YrM9zgdJjpmbdyQb18zFeFeO-E4bWQrYAqQ98CrfkU7jaRwARNGCzXwpDozk6TjRku1fR9QVZF4lFQpA4jNLK2wIqC2Uy0lUV18XeqQ3pRZbPQmFQC3JrJlGKZm6I1TFx9cMYse_MCq8Q_fVe6_yopporXjZpNvORHwu1krmCWoFXIqNR1jhNYMbr8tljn0YM8MgpIT-ynSTv851GcolZOqv2ca06lwtNQ9cyB_UEOatj2sldGW0YH_9X6moNnACUXXHrqjD_7tkY9twTPG0KHh-R9lE38TuCyP4Mb4MHcGqT4GCVjnh8SK97sLfSPlHuT5b6JRnlROrUO6OTKZXQGtc4iraw3rIux9ZxbPZOB0nSl9S-0jWicgMbpB2ACAuRkkQ96TvnZHyjpcwl5Fsct2MjqcBc7w21DfzE9HozX2btZVmCkVxxUUpaposaETMl3xxB-cvlIMIKYpAZ7m0XeiB_wgp9AhelAzT2nQPdP1Rj1Pm7Im6QouuwQu3qksR_BVAxdHmfVdvinkKTkuGZn-qtIlYtsyMGciQJS0xoceIDaOPhD0YPuHbqjl6CBL8BrVdyf6LQg9KguPkVhsgJ8hGe44sH_ExPDxLHZFrdz8myQNajFsB4SxkLEtbCRi_yimuihcCU39ACiIX3ViFrsiXLortVQmwBzwFWFwWX5PysrmrQp_OzdUKhsxNbswX8rqX_zNK4JDX6gcfh51x8mG81_1ffC12dqNSMJ9-sVDfCx5YRshAQ3qNPM0W9UGo8mXZPDONAewMaEMK9KV43qElZwVN_cZnnTvAxmh6sFserPJihGTTD-pX1bkV7UhZ0qQEVvjhlehxyQ_yQg5xwHCqoXLS_ztrKETD6LH1vVrfL4kFFYLxBowXMoodqpPnFr9gyg7-xDd1L9X55aGSUOTGR7Wu9OZTXhMyftLyoET17kO2CousqsPePsnM9QYf_IpdS8OIEX2Mw8IleE2rIZ2CBRcOL3zK1OgZVR8hmQkriV6wZHWjoWw8jvaKJl3eIEKZP_6bmvcJ2Gvw0sCGBCPGo8Qj652qSonEKfDwVlaChQjuy-o90nPVqlCQO4rG8ajlsS2OZduLQOqzwhHxuRRZW3rYZfIqAE7FG4MMfXr5YeD0OU7Hq8s_PKaJjvqDMhYo9QrTSChfawPzvrZ-qudU3wVenEXSie7meHxwSY_ot9ivW3xzL28" style="width: 80%;">
                        <p class="caption">BERT의 NER fine-tuning과 feature-based 방법의 성능, 출처: BERT 논문</p>
                    </div>



                    <div class="doubleSubHead">
                        <span style="display: block; text-align: center; margin-top: 150px;">&ldquo;</span>
                        <span>BERT Ablation Study</span>
                        <span style="display: block; text-align: center; margin-top: 13px;">&rdquo;</span>
                    </div>
                    <p>
                        <span style="font-size: 20px;"><b>1. NSP와 MLM의 성능</b></span>
                        <br>저자들은 NSP와 MLM의 성능을 측정하기 위해 아래 두 가지 케이스에 대해 실험을 진행하였습니다.
                        <ol>
                            <li>No NSP</li>
                            <li>No NSP &amp; left-to-right language modeling</li>
                        </ol>
                        <span class="highlight" style="color: rgb(0, 3, 206);">아래 결과를 보면 NSP가 없을 때 Natural Language Inference (NLI) 관련된 성능이 많이 하락하는 것을 볼 수 있습니다. 이를 통해 NSP가 문장을 이해하는 데 많은 역할을 하는 것을 알 수 있죠.</span>
                        
                        <br><br>뿐만아니라 MLM도 NSP 못지 않게 중요한 이유가, 기존 일반적인 unidirectional language modeling에 비해 MLM 방법을 채택한 것이 더 좋은 결과를 보입니다.
                        <span class="highlight" style="color: rgb(0, 3, 206);">이는 MLM이 bidirectional한 특징을 잘 유지하는 것이라고 볼 수 있습니다.</span>
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemc-xNIULSi7sdj68ApjTAFksEuvkZIGGG9ERafBT1CZgiTnMlYIyKaJYpZc8ygFWbDoDKSZMWdJNJNvyIXW-pWtDOtkmry7FvttVMtMsmjFM6-sE4V2Gc_pmIpbxnYBj44Sal1RnTltun7wb40tbUCBhe-MSg0gGulI201x8Yivy0UBiBzanRl8AP5WzQfgSs2cGx84sC_f8k6n_3YPMsFb-aA8ae5vIG6CH6Zn3QaWm60iKR8iRPIyN51MKUUmEzdosNRC8FymJrXBKSRstJ64yp0_yjuyaXNYrrAtp4p1ZP_648ND2L5RhegZsRBaARTJPHoTdd-Ols4WmGw2lUxbLiOoouxVP6Kx9E0tcpvoeA5bHdmy59ffUWQkCmb_-9qSUx2ks8nwXtLMW0pLDF3tKcvmHedK2YjN8bRSL9MHWYDcHCCyGS1bHgojHqwbVgQb0kJbeuav6OJ6FcJccb31q-A1fQN9eeS2IbWBUl0-wa-gaN__rbrlcsuunP3H5Ua01Ph7K407LpAxfpJXOJuTxmTag4JA8qX7EvloL3feoq7sJCyjW8w_1Iznm1u67YMVx5pNsbzWm66aUsIjH8ERZK6hWUjMd05veD8boW940C0NJqhOclLLGoo-eJcWoZnUVOTxyHqWBDsXTDvQIAQ0-c4ZJl8VBXuibTjQbQheRmrCQLtTsLBDk2zuxz6kEG82NaRCMD4N0ln0YK5wRemeQgznW73yFcT9IKA58OadIVaJWEet7VcJ-Ge9HFwECHmvjKYEXHtFWzpi63_259LVU6cspijEE3R_7KCVzngclx4SQsDglQ1a9XA1l9Dtt7qghrWVHciv5IZQuOtzUKN5xkJzWCfvTCH3QthlaSvMl0Z5o7fUdm44r_sjEPUj_hi9YxROijt2In1Nm6DG37zC3bUF2vRBcgKC1nVpoVyjThxzj0vmMyeQDmSF4zj6Edws154TsDUAgUFC4Mq1JGT0eWrcRzX-FhuU7y6mr22z-TAAysvhelEp4bbkIsVwIMvVzA8uwCXywSjtFIxvYOj2oLSK17KpsQJFtwxOzPTeZbvgfcO1ynLQCa33oqfa1tbTUOU2vTwkXQU2IeBdLF3nvY2Fl4DPcDg5x9eAA0sYhSrRpnqbSDWaAKNAr5LOXgf0lo1ddzkLowLaferJrinvwkk9iRhIkmFhqxIxD6vF5maFd8yYAhTxQ08KpN-IOH6wqgUnUw-vcvTrgYI_bfHGu9D0epqQK1dRZU73LMb9Rib8vSfQL_sMMtUBKwVHNgGOwQXi0Ak8lyqXa1D-B7cPP1nPFDuxF5Mg4Mi7pjjjwKAlXIa8g-FCiQOSd3q3YoEcTtLSsU_58eROUi92zsieUl2Coh2cOZdUgeSo97sK1S8zSkkwzOMHYbDhyB2Ajx4Nfd_SCwp3VX3gg9Ri1IMz72CagExFdcgHzXtj_0BFugZhus4S2SKHe3VR73FooI1EoQ2dSF06iPCSPiIjytM" style="width: 80%;">
                        <p class="caption">NSP, MLM의 Ablation Study, 출처: BERT 논문</p>
                    </div>
                    <p>
                        <br><span style="font-size: 20px;"><b>2. 모델 크기</b></span>
                        <br>위에서 소개한 SQuAD, SWAG의 결과가 BERT-base 보다 BERT-large의 성능이 더 좋은 것을 봤었습니다.
                        뿐만아니라 아래의 결과에서 봤을 때도 모델이 커질수록 benchmark에 대해 결과가 더 좋아지는 것을 확인할 수 있습니다.
                    </p>
                    <div class="contentImg">
                        <img src="https://lh3.googleusercontent.com/fife/AMPSemfgvhjqTF3Pn3KIl_rNK24eQJzNXSoVKeNXVseWxnch1rw6xpSn3k5L1liUsvUs69rha7eI8xEMmLnfi6Wk1EIBENReOXMi-CsnOkpisAV85TpS2fShV5DBVU5KR21wD2tGkOp0Acl41WUr49iMrKk6XdgHP8cSK3fuSItc5VFO4u7vpN4pzwjaQsZV6srbeuhgDN_oGVSNFHGvguZH0dmSMYiTbrytCEQMsxuWpoSVya8PFR2_XJLTbicKUPmxd4OWRgks2OkOUNW7ksJ-79vQNcjV-0Oe1WpFpukT4eBCJNjnEvcDHKo370yaNbD3AX0rX6AMBjNk0u79Wl-KODY946RqRbr9oeBuR6lWgwalp3CT-1QKJc4zLiteuAL0bkcXwKkyL352RJRtPQ7EuHHg_9qDuUXtvgg7Rvpj8xpJGiVgclZt639tF_indtw_-2TJXa10tMKK_oc8AF8Ejk4tT6n_RdkLyr6PAJz0DnLVXI39n1IkMomFZYMYl325EYWZf9dltBK2xTKNBdUJPZsNj_Z7zyItguQ9Da8JPDuk5LQhOI-Dr0dpCut-aM_57pREeA_4FCx9fZ6nAMbYBIP88L8L67sGHgUldVJtdq6COZDl7iMOZ6Jn_svupnljwo3o1D7mjVjpa7OPEGd1NklPdlOsLbHRfkIOTtN_W7PmbxwIZBMEmmPiAsdEQ2Grn-JHsTsRaRTA59xzxtCtXYmv2u-8eohIHH2h_mbcHk8xJeQR7w9L20bn6e1Bqd4XFAAnj3cWCnT5hSdNR7jA8MPAi-JestZ9y7izoYyupmudvEJZkpgyB8Q-VLRK8oVTNkK_HSl2MSo7J4YM1v0YZG4fFDFdya6w2fylEnXMINfEtRsiZveHeDUlZU64GBLVAhrBPyhEGbpH3qOo7Ij-pjszSYQhCPh2qOfR8nEjOgpYGLtHkWDfq4JBt3oRZxu84xYe60ifUlYx-kXmfOo-wcynkH9J-Kg1PlcPxOpP6RYzKsl0v9ZYEMJ1c_FT1FO1-TQtMQ0fFGtxO49RQxhtPl8y50F2MEWDgwRltgurBltsJtjcSkWDTb3QqYRbkTkdn4jEIbONukAQCeKgcu5syKphgIjuu9utqOsV7AORO04LIejgMnPtRS3RVxC9zUH-lRO2RnTJsRO2sCtAFP2fKyFk4pMXmVg6cW7-5CKEwVy_QK6e0IagyuDug8l4-ZjQIYBjriqgB20V2z6UqkoXv7I62HPtpMi9cvB1UbgKoT9bmNnxCKiBO0wekRctDExBOfd0ePEj3jklHPz9AEOEFlDEOlEmyemH6PIwmtUSoVSFS64fLSYRrIGYNOKd833b9WL5HP7OrZttW__zuJNmRY1n3Qw1OaMTRwH01lAP0WEQu1ViXDJCUeyqeFMB8-JggTXaSKkmXUTCch_i1tTMDW7ROGmvWX7WSqNGjhzeXWKmUIsuOMTk1Fua9RQ4jkECx3bKn4bnjeHqs3UsJU55PT0" style="width: 80%;">
                        <p class="caption">BERT 모델 크기에 따른 성능, 출처: BERT 논문</p>
                    </div>


                    
                    <p>
                        <br><br><br>BERT는 등장당시 엄청난 성능을 보여주며 모든 NLP task의 SOTA를 갈아엎은 모델이었습니다.
                        많은 데이터로 학습한 영향도 있겠지만, NSP와 MLM을 제안하면서 보여준 ablation study에서 이 2가지의 학습 방법의 효과를 입증하기도 했습니다.
                        다음에는 실제로 BERT 모델을 pre-training 해보는 코드를 소개하겠습니다.
                    </p>


                    
                </div> 
                <div class="tag">
                    <b>태그</b>&emsp;#BERT&emsp;#NSP&emsp;#MLM
                </div>
                <div class="pageTurner">
                    <div class="pageTurnerLeft">
                        <span><a style="position: absolute; left: 0;" onclick="alert('BERT 첫 게시물 입니다.\n\nThis is the first post of BERT.');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">&lang; 이전글</a>
                        <br></span>
                    </div>
                    <div class="pageTurnerRight">
                        <span><a style="position: absolute; right: 0;" onclick="pjaxPage('bert2.html');" onmouseover="colorOn(this);" onmouseout="colorOff(this);">다음글 &rang;</a>
                        <br>WikiSplit을 이용한 BERT Pre-training</span>
                    </div>
                </div>
                <span id="readNum"></span>
                <div id="disqus_thread"></div>

                <script>
                    headHighlightColorChanger();
                    (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = 'https://novicetraveler.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })(); 
                </script>
                <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </div>

        <div id="menuRelated">
            <div class="menuButton">
                <img id="menuImg" src="init/index_img/menu_black.png" onclick="openMenu(this);">
            </div>
            <div class="menu">
                <img id="menuExtension" title="메뉴를 확장합니다." src="init/index_img/extension_black.png" onclick="extendMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
            <div class="bigMenu">
                <img id="menuCompression" title="메뉴를 축소합니다." src="init/index_img/compression_black.png" onclick="compressMenu(this);">
                <div class="profile">
                </div>
                <ul class="tree">
                </ul>
                <p class="copyrights">
                    © 2022. 여행 초짜. All rights reserved.
                </p>
            </div>
        </div>

        <script>
            detectScroll();
            pushFunc();
            detectSize();
        </script>
    </body>
</html>